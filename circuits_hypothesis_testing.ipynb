{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpi/fs00/home/jim.maar/miniconda3/envs/othello-env/lib/python3.11/site-packages/accelerate/utils/imports.py:274: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpretability\n",
      "interpretability\n",
      "focus states: (50, 60, 8, 8)\n",
      "focus_valid_moves (50, 60, 64)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "chapter_dir = r\"./\" if chapter in os.listdir() else os.getcwd().split(chapter)[0]\n",
    "sys.path.append(chapter_dir + f\"{chapter}/exercises\")\n",
    "\n",
    "import os\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import sys\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import einops\n",
    "from ipywidgets import interact\n",
    "import plotly.express as px\n",
    "from ipywidgets import interact\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import random\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float, Int, Bool, Shaped, jaxtyped\n",
    "from typing import List, Union, Optional, Tuple, Callable, Dict\n",
    "import typeguard\n",
    "from functools import partial\n",
    "# from torcheval.metrics.functional import multiclass_f1_score\n",
    "from sklearn.metrics import f1_score as multiclass_f1_score\n",
    "import copy\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "# exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "# section_dir = exercises_dir / \"part6_othellogpt\"\n",
    "# section_dir = \"interpretability\"\n",
    "# if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow\n",
    "from neel_plotly import scatter, line\n",
    "from generate_patches import generate_patch\n",
    "from pprint import pprint\n",
    "from utils import plot_game\n",
    "from training_utils import get_state_stack_num_flipped\n",
    "from utils import plot_probe_outputs\n",
    "from utils import seq_to_state_stack\n",
    "# import part6_othellogpt.tests as tests\n",
    "\n",
    "t.manual_seed(42)\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 8,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = 8,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 61,\n",
    "    n_ctx = 59,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\",\n",
    "    device=device,\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "\n",
    "sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"synthetic_model.pth\")\n",
    "# champion_ship_sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"championship_model.pth\")\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "# An example input\n",
    "sample_input = t.tensor([[\n",
    "    20, 19, 18, 10,  2,  1, 27,  3, 41, 42, 34, 12,  4, 40, 11, 29, 43, 13, 48, 56,\n",
    "    33, 39, 22, 44, 24,  5, 46,  6, 32, 36, 51, 58, 52, 60, 21, 53, 26, 31, 37,  9,\n",
    "    25, 38, 23, 50, 45, 17, 47, 28, 35, 30, 54, 16, 59, 49, 57, 14, 15, 55, 7\n",
    "]]).to(device)\n",
    "\n",
    "# The argmax of the output (ie the most likely next move from each position)\n",
    "sample_output = t.tensor([[\n",
    "    21, 41, 40, 34, 40, 41,  3, 11, 21, 43, 40, 21, 28, 50, 33, 50, 33,  5, 33,  5,\n",
    "    52, 46, 14, 46, 14, 47, 38, 57, 36, 50, 38, 15, 28, 26, 28, 59, 50, 28, 14, 28,\n",
    "    28, 28, 28, 45, 28, 35, 15, 14, 30, 59, 49, 59, 15, 15, 14, 15,  8,  7,  8\n",
    "]]).to(device)\n",
    "\n",
    "assert (model(sample_input).argmax(dim=-1) == sample_output.to(device)).all()\n",
    "\n",
    "# os.chdir(section_dir)\n",
    "section_dir = Path.cwd()\n",
    "sys.path.append(str(section_dir))\n",
    "print(section_dir.name)\n",
    "\n",
    "OTHELLO_ROOT = (section_dir / \"othello_world\").resolve()\n",
    "OTHELLO_MECHINT_ROOT = (OTHELLO_ROOT / \"mechanistic_interpretability\").resolve()\n",
    "\n",
    "# if not OTHELLO_ROOT.exists():\n",
    "#     !git clone https://github.com/likenneth/othello_world\n",
    "\n",
    "sys.path.append(str(OTHELLO_MECHINT_ROOT))\n",
    "\n",
    "from mech_interp_othello_utils import (\n",
    "    plot_board,\n",
    "    plot_single_board,\n",
    "    plot_board_log_probs,\n",
    "    to_string,\n",
    "    to_int,\n",
    "    int_to_label,\n",
    "    string_to_label,\n",
    "    OthelloBoardState\n",
    ")\n",
    "\n",
    "# Load board data as ints (i.e. 0 to 60)\n",
    "board_seqs_int = t.tensor(np.load(OTHELLO_MECHINT_ROOT / \"board_seqs_int_small.npy\"), dtype=t.long)\n",
    "# Load board data as \"strings\" (i.e. 0 to 63 with middle squares skipped out)\n",
    "board_seqs_string = t.tensor(np.load(OTHELLO_MECHINT_ROOT / \"board_seqs_string_small.npy\"), dtype=t.long)\n",
    "\n",
    "assert all([middle_sq not in board_seqs_string for middle_sq in [27, 28, 35, 36]])\n",
    "assert board_seqs_int.max() == 60\n",
    "\n",
    "num_games, length_of_game = board_seqs_int.shape\n",
    "\n",
    "# Define possible indices (excluding the four center squares)\n",
    "stoi_indices = [i for i in range(64) if i not in [27, 28, 35, 36]]\n",
    "\n",
    "# Define our rows, and the function that converts an index into a (row, column) label, e.g. `E2`\n",
    "alpha = \"ABCDEFGH\"\n",
    "\n",
    "def to_board_label(i):\n",
    "    return f\"{alpha[i//8]}{i%8}\"\n",
    "\n",
    "# Get our list of board labels\n",
    "board_labels = list(map(to_board_label, stoi_indices))\n",
    "full_board_labels = list(map(to_board_label, range(64)))\n",
    "\n",
    "def plot_square_as_board(state, diverging_scale=True, **kwargs):\n",
    "    \"\"\"Takes a square input (8 by 8) and plot it as a board. Can do a stack of boards via facet_col=0\"\"\"\n",
    "    kwargs = {\n",
    "        \"y\": [i for i in alpha],\n",
    "        \"x\": [str(i) for i in range(8)],\n",
    "        \"color_continuous_scale\": \"RdBu\" if diverging_scale else \"Blues\",\n",
    "        \"color_continuous_midpoint\": 0. if diverging_scale else None,\n",
    "        \"aspect\": \"equal\",\n",
    "        **kwargs\n",
    "    }\n",
    "    imshow(state, **kwargs)\n",
    "\n",
    "start = 30000\n",
    "num_games = 50\n",
    "focus_games_int = board_seqs_int[start : start + num_games]\n",
    "focus_games_string = board_seqs_string[start: start + num_games]\n",
    "\n",
    "focus_logits, focus_cache = model.run_with_cache(focus_games_int[:, :-1].to(device))\n",
    "focus_logits.shape\n",
    "\n",
    "def one_hot(list_of_ints, num_classes=64):\n",
    "    out = t.zeros((num_classes,), dtype=t.float32)\n",
    "    out[list_of_ints] = 1.\n",
    "    return out\n",
    "\n",
    "focus_states = np.zeros((num_games, 60, 8, 8), dtype=np.float32)\n",
    "focus_valid_moves = t.zeros((num_games, 60, 64), dtype=t.float32)\n",
    "\n",
    "for i in (range(num_games)):\n",
    "    board = OthelloBoardState()\n",
    "    for j in range(60):\n",
    "        board.umpire(focus_games_string[i, j].item())\n",
    "        focus_states[i, j] = board.state\n",
    "        focus_valid_moves[i, j] = one_hot(board.get_valid_moves())\n",
    "\n",
    "print(\"focus states:\", focus_states.shape)\n",
    "print(\"focus_valid_moves\", tuple(focus_valid_moves.shape))\n",
    "\n",
    "# full_linear_probe = t.load(OTHELLO_MECHINT_ROOT / \"main_linear_probe.pth\", map_location=device)\n",
    "\n",
    "linear_probe2 = t.load(\"probes/linear/resid_6_linear.pth\")\n",
    "\n",
    "rows = 8\n",
    "cols = 8\n",
    "options = 3\n",
    "assert linear_probe2.shape == (1, cfg.d_model, rows, cols, options)\n",
    "\n",
    "black_to_play_index = 0\n",
    "white_to_play_index = 1\n",
    "blank_index = 0\n",
    "their_index = 1\n",
    "my_index = 2\n",
    "\n",
    "# Creating values for linear probe (converting the \"black/white to play\" notation into \"me/them to play\")\n",
    "\n",
    "'''LAYER = 6\n",
    "game_index = 0\n",
    "move = 29'''\n",
    "\n",
    "BLANK1 = 0\n",
    "BLACK = 1\n",
    "WHITE = -1\n",
    "\n",
    "# MINE = 0\n",
    "# YOURS = 1\n",
    "# BLANK2 = 2\n",
    "\n",
    "EMPTY = 0\n",
    "YOURS = 1\n",
    "MINE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    }
   ],
   "source": [
    "# Load Probes\n",
    "linear_probes = []\n",
    "flipped_probes = []\n",
    "for layer in range(8):\n",
    "    linear_probe = t.load(f\"probes/linear/resid_{layer}_linear.pth\").to(device)\n",
    "    flipped_probe = t.load(f\"probes/flipped/resid_{layer}_flipped.pth\").to(device)\n",
    "    linear_probes.append(linear_probe)\n",
    "    flipped_probes.append(flipped_probe)\n",
    "print(len(linear_probes), len(flipped_probes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_logits(resid : Float[Tensor, \"batch pos d_model\"], layer : Int, softmax : bool = False) -> Tuple[Float[Tensor, \"model batch seq rows cols options\"], Float[Tensor, \"model batch seq rows cols options\"]]:\n",
    "    \"\"\"\n",
    "    resid: [batch pos d_model]\n",
    "    layer: Int\n",
    "    softmax: bool\n",
    "    output:\n",
    "        Tuple[Float[Tensor, \"model batch seq rows cols options\"], Float[Tensor, \"model batch seq rows cols options\"]]\n",
    "        The first tensor is the logits for the board, the second tensor is the logits for the flipped board\n",
    "    \"\"\"\n",
    "    linear_probe = linear_probes[layer]\n",
    "    flipped_probe = flipped_probes[layer]\n",
    "    board_logits = einops.einsum(resid, linear_probe, 'batch seq d_model, modes d_model rows cols options -> modes batch seq rows cols options')[0]\n",
    "    flipped_logits = einops.einsum(resid, flipped_probe, 'batch seq d_model, modes d_model rows cols options -> modes batch seq rows cols options')[0]\n",
    "    if softmax:\n",
    "        board_logits = F.softmax(board_logits, dim=-1)\n",
    "        flipped_logits = F.softmax(flipped_logits, dim=-1)\n",
    "    return board_logits, flipped_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 47) (3221098436.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 47\u001b[0;36m\u001b[0m\n\u001b[0;31m    num_preds_per_pos = (t.ones((batch_size, seq_len)) * 64).to(device) - einops.reduce((pred_valuesls -> batch pos', reduction='sum')\u001b[0m\n\u001b[0m                                                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 47)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Plan: For Layer 1 to 7. Assume Mine Yours comes from Last Flipped, What accuracy do I get?\n",
    "# result = t.zeros((7, 2), dtype=t.float32) # Layer, (mine vs yours)\n",
    "# TODO: Mine and Yours seperately\n",
    "# TODO: position seperately\n",
    "def test_last_flipped_accuracy(end_pos : Int):\n",
    "    flipped_thresh = 0.9\n",
    "    how_far_back = 15\n",
    "    for layer in range(1, 7):\n",
    "        resid_pre = focus_cache[\"resid_pre\", layer]\n",
    "        resid_post = focus_cache[\"resid_mid\", layer]\n",
    "        # shape: [batch pos rows cols options]\n",
    "        mine_yours_logits, flipped_logits = get_feature_logits(resid_pre, layer-1)\n",
    "        mine_yours_logits_post, flipped_logits_post = get_feature_logits(resid_post, layer)\n",
    "        batch_size, seq_len, _, _, _ = mine_yours_logits_post.shape\n",
    "        last_flipped_values = t.zeros_like(mine_yours_logits_post)\n",
    "        last_flipped_values[:, :, :, :, 0] = 1\n",
    "        # TODO: Not sure if this is correct\n",
    "        for pos1 in range(seq_len):\n",
    "            # MINE\n",
    "            for pos in range(max([pos1 + 1 - how_far_back, 0]), pos1 + 1):\n",
    "                # flipped_diff = flipped_logits[:, pos, :, :, 0] - flipped_logits[:, pos, :, :, 1]\n",
    "                flipped_diff = flipped_logits.softmax(dim=-1)[:, pos, :, :, 0]\n",
    "                was_flipped = flipped_diff > flipped_thresh\n",
    "                # if pos1 % 2 != pos % 2:\n",
    "                # last_flipped_values[:, pos1][was_flipped] = mine_yours_logits[:, pos][was_flipped]\n",
    "                # print(layer, pos1, pos)\n",
    "                '''if layer == 1 and pos1 == 5 and pos == 4:\n",
    "                    print(f\"Debug: {flipped_diff.shape}, {was_flipped.shape}, {last_flipped_values.shape}\")\n",
    "                    print(flipped_diff[1, :, :])\n",
    "                    print(was_flipped[1, :, :])\n",
    "                    print(last_flipped_values[1, pos1, :, :, 1])'''\n",
    "                if pos1 % 2 == pos % 2:\n",
    "                    last_flipped_values[:, pos1][was_flipped] = mine_yours_logits[:, pos][was_flipped]\n",
    "                else:\n",
    "                    mine_yours_logits_copy = mine_yours_logits.clone()\n",
    "                    # print(mine_yours_logits.shape, mine_yours_logits_copy.shape, was_flipped.shape)\n",
    "                    mine_yours_logits_copy[:, :, :, 1], mine_yours_logits_copy[:, :, :, 2] = mine_yours_logits[:, :, :, 2], mine_yours_logits[:, :, :, 1]\n",
    "                    last_flipped_values[:, pos1][was_flipped] = mine_yours_logits_copy[:, pos][was_flipped]\n",
    "        # Calculate Accuracy\n",
    "        # TODO: Dises Mess richtig machen,\n",
    "        # TODO: Prediction für flipped von diesem Move auch rein machen (Einfach for loop erhöhen. und aus for loop die 2 raus nehmen)\n",
    "        # TODO: Danach das Ding in viele nützliche Funktionen aufteilen\n",
    "        pred_values = last_flipped_values.argmax(dim=-1)\n",
    "        real_values = mine_yours_logits_post.argmax(dim=-1)\n",
    "        # real_values shape : [batch pos, rows, cols]()\n",
    "        # get number of zero_preds\n",
    "        num_preds_per_pos = (t.ones((batch_size, seq_len)) * 64).to(device) - einops.reduce((pred_valuesls -> batch pos', reduction='sum')\n",
    "        num_positions_with_preds = (num_preds_per_pos > 0).sum()\n",
    "        num_preds_per_pos[num_preds_per_pos == 0] = 1\n",
    "        pred_values[pred_values == 0] = -1\n",
    "        acc_correct_per_pos = einops.reduce((pred_values == real_values).float(), 'batch pos rows cols -> batch pos', reduction='sum') / num_preds_per_pos\n",
    "        acc = acc_correct_per_pos.mean()\n",
    "        print(f\"Layer: {layer}, Accuracy: {acc.item()}\")\n",
    "\n",
    "test_last_flipped_accuracy(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 0, Pos: 15, Tile: E4\n",
      "Game: 0, Pos: 15, Tile: F4\n",
      "Game: 0, Pos: 22, Tile: D3\n",
      "Game: 0, Pos: 23, Tile: D5\n",
      "Game: 0, Pos: 24, Tile: E4\n",
      "Layer: 1, Accuracy: 0.934215784072876\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# TODO: Print out the Mistakes, so I can visualize them\n",
    "# TODO: Print out the Tiles where the mistake is\n",
    "# TODO: Import shit to visualize\n",
    "# TODO: Visualize\n",
    "# TODO: Different Layers: Different Threshholds, mabe different positions ???\n",
    "# TODO: Add the Flipping Circuit\n",
    "# TODO: For the Future I can add evaluation that distinguishes Tiles that got changed from the previous layer and Tiles that did not\n",
    "# TODO: Add hard eval (everything that's not Empty)\n",
    "\n",
    "@dataclass\n",
    "class Parameters():\n",
    "    flipped_thresh : Float = 0.2\n",
    "    how_far_back : Int = 15\n",
    "    start_pos : Int = 10\n",
    "    end_pos : Int = 20\n",
    "    layers : Int = 7\n",
    "    evaluation_module : str = \"attn_out\"\n",
    "    include_current_pos : bool = False\n",
    "    easy_eval: bool = True\n",
    "    easy_eval_thresh = 0.8\n",
    "\n",
    "def last_flipped_circuit(\n",
    "        params : Parameters,\n",
    "        mine_yours_logits: Float[Tensor, \"batch pos rows cols options\"],\n",
    "        flipped_logits : Float[Tensor, \"batch pos rows cols options\"],\n",
    "        predicted : Bool[Tensor, \"batch pos rows cols\"],\n",
    "    ) -> Tuple[Float[Tensor, \"batch pos rows cols options\"], Bool[Tensor, \"batch pos rows cols\"]]:\n",
    "    last_flipped_values = t.zeros_like(mine_yours_logits)\n",
    "    last_flipped_values[:, :, :, :, 0] = 1\n",
    "    for pos1 in range(params.start_pos, params.end_pos):\n",
    "        if params.include_current_pos:\n",
    "            local_end_pos = pos1 + 1\n",
    "        else:\n",
    "            local_end_pos = pos1\n",
    "        for pos in range(max([local_end_pos - params.how_far_back, 0]), local_end_pos):\n",
    "            flipped_diff = flipped_logits.softmax(dim=-1)[:, pos, :, :, 0]\n",
    "            was_flipped = flipped_diff > params.flipped_thresh\n",
    "            if pos1 % 2 == pos % 2:\n",
    "                last_flipped_values[:, pos1][was_flipped] = mine_yours_logits[:, pos][was_flipped]\n",
    "            else:\n",
    "                mine_yours_logits_copy = mine_yours_logits.clone()\n",
    "                mine_yours_logits_copy[:, pos, :, :, 1], mine_yours_logits_copy[:, pos, :, :, 2] = mine_yours_logits[:, pos, :, :, 2], mine_yours_logits[:, pos, :, :, 1]\n",
    "                last_flipped_values[:, pos1][was_flipped] = mine_yours_logits_copy[:, pos][was_flipped]\n",
    "            predicted[:, pos1] |= was_flipped\n",
    "    return last_flipped_values, predicted\n",
    "\n",
    "def flipping_circuit(\n",
    "        params : Parameters,\n",
    "        mine_yours_logits: Float[Tensor, \"batch pos rows cols options\"],\n",
    "        flipped_logits : Float[Tensor, \"batch pos rows cols options\"],\n",
    "        predicted: Bool[Tensor, \"batch pos rows cols\"],\n",
    "    ) -> Tuple[Float[Tensor, \"batch pos rows cols options\"], Bool[Tensor, \"batch pos rows cols\"]]:\n",
    "    # TODO: Add the history ...\n",
    "    # TODO: Add Canges to the Flipping Logits\n",
    "    # Get the last played Tile(s) (as input or calculate themselves. I think as input is nice)\n",
    "    # Go over each position\n",
    "    #   Use a YOURS Threshold (in params)\n",
    "    #   create list of tiles (row, col) that should be flipped\n",
    "    #   flip the tiles\n",
    "    #       Option1: Add fixxed amount to the MINE logit\n",
    "    #       Option2: Swap the values for MINE and YOURS? (I think this makes sense but isn't very general)\n",
    "    #           Swap using a weight term (write 0.5 of MINE to YOURS ...) then add a fixxed bias to both logits? (Use different weight and bias for mine and yours)\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_predictions(params : Parameters, resid_pre: Float[Tensor, \"batch pos d_model\"], layer : Int) -> Float[Tensor, \"batch pos rows cols options\"]:\n",
    "    # batch pos rows cols option\n",
    "    # TODO: Maybe remove the inputs and outputs and just use function side effects ...\n",
    "    batch_size, seq_len, _= resid_pre.shape\n",
    "    rows, cols = 8, 8\n",
    "    predicted : Bool[Tensor, \"batch pos rows cols\"] = t.zeros((batch_size, seq_len, rows, cols), dtype=t.bool).to(device)\n",
    "    mine_yours_logits, flipped_logits = get_feature_logits(resid_pre, layer-1)\n",
    "    mine_yours_logits_pred, predicted = last_flipped_circuit(params, mine_yours_logits, flipped_logits, predicted)\n",
    "    return mine_yours_logits_pred, predicted\n",
    "\n",
    "def print_first_mistakes(pred : Float[Tensor, \"batch pos rows cols\"], real : Float[Tensor, \"batch pos rows cols\"], mask : Float[Tensor, \"batch pos rows cols\"], num_mistakes : Int):\n",
    "    batch_size, seq_len, rows, cols = pred.shape\n",
    "    mistakes = ((pred != real) & mask)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    if not mistakes[i, j, row, col]:\n",
    "                        continue\n",
    "                    tile_label = string_to_label(row*8 + col)\n",
    "                    print(f\"Game: {i}, Pos: {j}, Tile: {tile_label}\")\n",
    "                    num_mistakes -= 1\n",
    "                    if num_mistakes == 0:\n",
    "                        return\n",
    "\n",
    "def get_evaluation_mask(params : Parameters, predicted : Float[Tensor, \"batch pos rows cols\"], real_values : Float[Tensor, \"batch pos rows cols options\"]):\n",
    "    batch_size, seq_len, rows, cols = predicted.shape\n",
    "    # create bool tensor of shape [batch_size, seq_len, rows, cols] True\n",
    "    mask = t.ones((batch_size, seq_len, rows, cols), dtype=t.bool).to(device)\n",
    "    # Mask pred values that are 0\n",
    "    # mask = mask & (pred_values.argmax(dim=-1) != 0)\n",
    "    mask = mask & predicted\n",
    "    mask = mask & (real_values.argmax(dim=-1) != 0)\n",
    "    # If easy eval, mask real values that are below 0.7\n",
    "    if params.easy_eval:\n",
    "        real_values = real_values.softmax(dim=-1)\n",
    "        mask = mask & ((real_values[:, :, :, :, 2] > params.easy_eval_thresh) | (real_values[:, :, :, :, 1] > params.easy_eval_thresh))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_accuracy(params : Parameters,\n",
    "        pred_values : Float[Tensor, \"batch pos rows cols options\"],\n",
    "        resid_post : Float[Tensor, \"batch pos d_model\"],\n",
    "        predicted: Bool[Tensor, \"batch pos rows cols\"],\n",
    "        layer : Int,\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "    # TODO: This funciton is a mess\n",
    "    #   - Create a Mask for what should be evaluated and what not, with a function, then use that mask to calculate the accuracy\n",
    "    batch_size, seq_len, _, _, _ = pred_values.shape\n",
    "    real_values , _ = get_feature_logits(resid_post, layer)\n",
    "    evaluation_mask = get_evaluation_mask(params, predicted, real_values)\n",
    "    pred_values = pred_values.argmax(dim=-1)\n",
    "    real_values = real_values.argmax(dim=-1)\n",
    "    print_first_mistakes(pred_values, real_values, evaluation_mask, 5)\n",
    "    num_labels = evaluation_mask.sum()\n",
    "    num_correct = (pred_values[evaluation_mask] == real_values[evaluation_mask]).float().sum()\n",
    "    acc = num_correct / num_labels\n",
    "    '''num_preds_per_pos = (t.ones((batch_size, seq_len)) * 64).to(device) - einops.reduce((pred_values == 0), 'batch pos rows cols -> batch pos', reduction='sum')\n",
    "    num_positions_with_preds = (num_preds_per_pos > 0).sum(dim=-1)\n",
    "    num_preds_per_pos[num_preds_per_pos == 0] = 1\n",
    "    pred_values[pred_values == 0] = -1\n",
    "    print(pred_values.shape, real_values.shape, num_preds_per_pos.shape)\n",
    "    acc_correct_per_pos = einops.reduce((pred_values == real_values).float(), 'batch pos rows cols -> batch pos', reduction='sum') / num_preds_per_pos\n",
    "    acc = (acc_correct_per_pos.sum(dim=-1)/num_positions_with_preds).mean()'''\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_last_flipped_accuracy(params : Parameters):\n",
    "    for layer in range(1, params.layers):\n",
    "        resid_pre = focus_cache[\"resid_pre\", layer]\n",
    "        resid_post = focus_cache[params.evaluation_module, layer]\n",
    "        # shape: [batch pos rows cols options]\n",
    "        mine_yours_logits_pred, predicted = get_predictions(params, resid_pre, layer)\n",
    "        acc = get_accuracy(params, mine_yours_logits_pred, resid_post, predicted, layer)\n",
    "        print(f\"Layer: {layer}, Accuracy: {acc.item()}\")\n",
    "\n",
    "params = Parameters()\n",
    "params.layers = 2\n",
    "params.start_pos = 0\n",
    "params.end_pos = 30\n",
    "params.easy_eval = True\n",
    "params.easy_eval_thresh = 0.7\n",
    "params.evaluation_module = \"resid_post\"\n",
    "params.include_current_pos = True\n",
    "\n",
    "test_last_flipped_accuracy(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import VisualzeBoardArguments\n",
    "from utils import visualize_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_args = VisualzeBoardArguments()\n",
    "vis_args.start_pos = 0\n",
    "vis_args.end_pos = 25\n",
    "vis_args.layers = 6\n",
    "vis_args.include_attn_only = False\n",
    "vis_args.include_mlp_only = False\n",
    "# vis_args.static_image = True\n",
    "\n",
    "clean_input_str = focus_games_string[0, :59]\n",
    "\n",
    "# visualize_game(clean_input_str, vis_args, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
