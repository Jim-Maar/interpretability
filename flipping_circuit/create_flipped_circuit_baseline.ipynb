{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpi/fs00/home/jim.maar/miniconda3/envs/othello-env/lib/python3.11/site-packages/accelerate/utils/imports.py:274: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_seqs_int_train = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_int_train.pth\",\n",
    "    )\n",
    ")\n",
    "board_seqs_int_test = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_int_valid.pth\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "START_VALID = 0\n",
    "NUM_GAMES_VALID = 10000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "PRED_LOGITS = 0.5 # 0.1 orde 1 oder 10 wäre auch interessant (also für diff machts keinen unterschied aber für final schon)\n",
    "\n",
    "def bundle_fake_cache(fake_cache):\n",
    "    bundled_fake_cache = {}\n",
    "    key_names = list(set([\".\".join(key.split(\".\")[2:]) for key in fake_cache]))\n",
    "    for key_name in key_names:\n",
    "        act_name_results = {act_name : result for act_name, result in fake_cache.items() if key_name in act_name}\n",
    "        stacked_result : Float[Tensor, \"batch layer pos neurons\"] = t.stack(list(act_name_results.values()), dim=1)\n",
    "        bundled_fake_cache[key_name] = stacked_result\n",
    "    return bundled_fake_cache\n",
    "\n",
    "# %%\n",
    "W_out = model.W_out.detach()\n",
    "b_out = model.b_out.detach()\n",
    "\n",
    "def get_masks(flipped_final_real, flipped_final_pred):\n",
    "    mask = t.zeros_like(flipped_final_real).to(dtype=t.int)\n",
    "    only_real_mask = t.zeros_like(flipped_final_real).to(dtype=t.int)\n",
    "    only_pred_mask = t.zeros_like(flipped_final_real).to(dtype=t.int)\n",
    "    mask[:, 0] = flipped_final_real[:, 0]\n",
    "    for layer in range(1, 8):\n",
    "        change_real = (flipped_final_real[:, layer - 1] != flipped_final_real[:, layer])\n",
    "        change_pred = (flipped_final_real[:, layer - 1] != flipped_final_pred[:, layer]) # This is correct!\n",
    "        mask[:, layer] = (change_real | change_pred).to(dtype=t.int)\n",
    "        only_real_mask[:, layer] = change_real.to(dtype=t.int)\n",
    "        only_pred_mask[:, layer] = change_pred.to(dtype=t.int)\n",
    "    return mask, only_real_mask, only_pred_mask\n",
    "\n",
    "def evaluate_rules(bundled_fake_cache_valid):\n",
    "    results_dict = {\n",
    "        \"avg_neuron_count\" : t.zeros(8, 59).to(device),\n",
    "        \"abs_mean_diff_flipped\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"abs_mean_diff_not_flipped\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"TP_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"FP_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"TN_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"FN_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"TP_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        # \"FP_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        \"TN_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "        # \"FN_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    }\n",
    "    total_number_of_neurons = t.zeros(8, 59).to(device)\n",
    "    total_number_of_predictions = t.zeros(8, 59).to(device)\n",
    "    mask_sum = t.zeros(8, 59, 8, 8, device=device)\n",
    "\n",
    "    probe = probes[\"flipped\"]\n",
    "    probe_lists = {}\n",
    "    for layer in range(8):\n",
    "        probe_lists[layer] = []\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                probe_lists[layer].append(probe[layer, :, row, col, FLIPPED])\n",
    "        \n",
    "    for batch in tqdm(range(START_VALID, START_VALID + NUM_GAMES_VALID, BATCH_SIZE)):\n",
    "        mlp_post_real : Float[Tensor, \"batch layer pos neurons\"] = bundled_fake_cache_valid[\"mlp.hook_post\"][batch:batch+BATCH_SIZE].to(device)\n",
    "        resid_pre_real : Float[Tensor, \"batch layer pos d_model\"] = bundled_fake_cache_valid[\"hook_resid_pre\"][batch:batch+BATCH_SIZE].to(device)\n",
    "        attn_out_real : Float[Tensor, \"batch layer pos d_model\"] = bundled_fake_cache_valid[\"hook_attn_out\"][batch:batch+BATCH_SIZE].to(device)\n",
    "\n",
    "        mlp_out_real = einops.einsum(mlp_post_real, W_out, \"batch layer pos neurons, layer neurons d_model -> batch layer pos d_model\")\n",
    "        flipped_logits_real = einops.einsum(mlp_out_real + attn_out_real, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_real = flipped_logits_real.argmax(dim=-1)\n",
    "\n",
    "        # mlp_out_pred = einops.einsum(mlp_post_pred, W_out, \"batch layer pos neurons, layer neurons d_model -> batch layer pos d_model\")\n",
    "        mlp_out_pred = t.ones_like(mlp_out_real).to(device) * PRED_LOGITS\n",
    "        flipped_logits_pred = einops.einsum(mlp_out_pred + attn_out_real, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_pred = flipped_logits_pred.argmax(dim=-1)\n",
    "\n",
    "        resid_post_real = resid_pre_real + mlp_out_real + attn_out_real + einops.repeat(b_out, \"layer d_model -> layer pos d_model\", pos=59)\n",
    "        final_logits_real = einops.einsum(resid_post_real, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_final_real = (final_logits_real[:, :, :, :, :, 0] > final_logits_real[:, :, :, :, :, 1]).to(t.int)\n",
    "        resid_post_pred = resid_pre_real + mlp_out_pred + attn_out_real + einops.repeat(b_out, \"layer d_model -> layer pos d_model\", pos=59)\n",
    "        final_logits_pred = einops.einsum(resid_post_pred, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_final_pred = (final_logits_pred[:, :, :, :, :, 0] > final_logits_pred[:, :, :, :, :, 1]).to(t.int)\n",
    "        mask, only_real_mask, only_pred_mask = get_masks(flipped_final_real, flipped_final_pred)\n",
    "        mask_sum += mask.sum(dim=0)\n",
    "\n",
    "        abs_diff_flipped = (flipped_logits_real[:, :, :, :, :, FLIPPED] - flipped_logits_pred[:, :, :, :, :, FLIPPED]).abs().sum(dim=0)\n",
    "        abs_diff_not_flipped = (flipped_logits_real[:, :, :, :, :, NOT_FLIPPED] - flipped_logits_pred[:, :, :, :, :, NOT_FLIPPED]).abs().sum(dim=0)\n",
    "        TP = ((flipped_real == FLIPPED) & (flipped_pred == FLIPPED) & mask).sum(dim=0).float()\n",
    "        FP = ((flipped_real == NOT_FLIPPED) & (flipped_pred == FLIPPED) & mask).sum(dim=0).float()\n",
    "        TN = ((flipped_real == NOT_FLIPPED) & (flipped_pred == NOT_FLIPPED) & mask).sum(dim=0).float()\n",
    "        FN = ((flipped_real == FLIPPED) & (flipped_pred == NOT_FLIPPED) & mask).sum(dim=0).float()\n",
    "        results_dict[\"abs_mean_diff_flipped\"] += abs_diff_flipped\n",
    "        results_dict[\"abs_mean_diff_not_flipped\"] += abs_diff_not_flipped\n",
    "        results_dict[\"TP_diff\"] += TP\n",
    "        results_dict[\"FP_diff\"] += FP\n",
    "        results_dict[\"TN_diff\"] += TN\n",
    "        results_dict[\"FN_diff\"] += FN\n",
    "\n",
    "        flipped_change_real = (flipped_real == FLIPPED) & only_real_mask\n",
    "        not_flipped_change_real = (flipped_real == NOT_FLIPPED) & only_real_mask\n",
    "        flipped_change_pred = (flipped_pred == FLIPPED) & only_pred_mask\n",
    "        not_flipped_change_pred = (flipped_pred == NOT_FLIPPED) & only_pred_mask\n",
    "        TP_final = (flipped_change_real & flipped_change_pred).sum(dim=0).float()\n",
    "        FP_final = (not_flipped_change_real & flipped_change_pred).sum(dim=0).float()\n",
    "        TN_final = (not_flipped_change_real & not_flipped_change_pred).sum(dim=0).float()\n",
    "        FN_final = (flipped_change_real & not_flipped_change_pred).sum(dim=0).float()\n",
    "        # DICLAIMER: False Positive and False Negative where not done write, but I can get accuracy using mask_sum ...\n",
    "        results_dict[\"TP_final\"] += TP_final\n",
    "        # results_dict[\"FP_final\"] += FP_final\n",
    "        results_dict[\"TN_final\"] += TN_final\n",
    "        # results_dict[\"FN_final\"] += FN_final\n",
    "    results_dict[\"abs_mean_diff_flipped\"] /= mask_sum\n",
    "    results_dict[\"abs_mean_diff_not_flipped\"] /= mask_sum\n",
    "    results_dict[\"avg_neuron_count\"] = total_number_of_neurons / total_number_of_predictions\n",
    "    return results_dict, mask_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting activations ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:58<00:41,  2.05s/it]"
     ]
    }
   ],
   "source": [
    "act_names = [utils.get_act_name(\"mlp_post\", layer) for layer in range(8)]\n",
    "act_names += [utils.get_act_name(\"attn_out\", layer) for layer in range(8)]\n",
    "act_names += [utils.get_act_name(\"resid_pre\", layer) for layer in range(8)]\n",
    "act_names += [utils.get_act_name(\"resid_post\", layer) for layer in range(8)]\n",
    "act_names += [f\"blocks.{layer}.ln1.hook_normalized\" for layer in range(8)]\n",
    "act_names += [f\"blocks.{layer}.ln2.hook_normalized\" for layer in range(8)]\n",
    "fake_cache_valid = get_activation(board_seqs_int_test, act_names, start=START_VALID, num_games=NUM_GAMES_VALID)\n",
    "bundled_fake_cache_valid = bundle_fake_cache(fake_cache_valid)\n",
    "results_dict, mask_sum = evaluate_rules(bundled_fake_cache_valid)\n",
    "directory = \"flipping_circuit_results\"\n",
    "run_name = f\"baseline_{PRED_LOGITS}\"\n",
    "save_path = f\"{directory}/{run_name}\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "t.save(results_dict, f\"{save_path}/flipped_circuit_results_dict.pth\")\n",
    "t.save(mask_sum, f\"{save_path}/flipped_circuit_mask_sum.pth\")\n",
    "print(\"Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
