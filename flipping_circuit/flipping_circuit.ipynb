{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prove the Flipping Circuit ...\n",
    "- Darauf achten\n",
    "    - Maximal Modular machen immer kleine Funktionen mit High Cohesion und Low Coupling\n",
    "    - für alle Tensoren Typing verwenden.\n",
    "    - Möglichst oft kleine Testfunktionenen schreiben...\n",
    "- Das ist ein bisschen ein Gamble. Ich böller das einfach heute fertig und wenn es klappt ist Insane! Ansonsten reevaluiere ich meine Situation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpi/fs00/home/jim.maar/miniconda3/envs/othello-env/lib/python3.11/site-packages/accelerate/utils/imports.py:274: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "focus_logits, focus_cache = get_focus_logits_and_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Load board data as \"strings\" (i.e. 0 to 63 with middle squares skipped out)\\nboard_seqs_string = t.load(\\n    os.path.join(\\n        section_dir,\\n        \"data/board_seqs_string_train.pth\",\\n    )\\n)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Datasets ...\n",
    "board_seqs_int_train = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_int_train.pth\",\n",
    "    )\n",
    ")\n",
    "board_seqs_int_test = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_int_valid.pth\",\n",
    "    )\n",
    ")\n",
    "'''# Load board data as \"strings\" (i.e. 0 to 63 with middle squares skipped out)\n",
    "board_seqs_string = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_string_train.pth\",\n",
    "    )\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "if not DEBUG:\n",
    "    START_TRAIN = 0\n",
    "    BATCH_SIZE = 500\n",
    "    NUM_GAMES_TRAIN = 20000 # with 60 GB RAM I can do 20.000 (I could allocate more though haha)\n",
    "    START_VALID = 0\n",
    "    NUM_GAMES_VALID = 10000\n",
    "    NUM_RULES = None\n",
    "    MLP_BATCH_SIZE = 1000\n",
    "else:\n",
    "    START_TRAIN = 0\n",
    "    BATCH_SIZE = 100\n",
    "    NUM_GAMES_TRAIN = 500\n",
    "    START_VALID = 0\n",
    "    NUM_GAMES_VALID = 200\n",
    "    NUM_RULES = 4144\n",
    "    MLP_BATCH_SIZE = 200\n",
    "\n",
    "def get_activation(board_seqs_int, act_names, num_games=1000, start=0, games = []):\n",
    "    # TODO: If this takes to long or something, Make a filter step!\n",
    "    inference_size = 200\n",
    "    if len(games) > 0:\n",
    "        num_games = len(games)\n",
    "    iterate = range(start, start+num_games, inference_size)\n",
    "    if num_games > 1000:\n",
    "        iterate = tqdm(iterate, total=num_games//inference_size)\n",
    "    print(\"Getting activations ...\")\n",
    "    act_name_results = {act_name : [] for act_name in act_names}\n",
    "    for batch in iterate:\n",
    "        input_games = list(range(batch, min(batch + inference_size, batch + num_games)))\n",
    "        if len(games) > 0:\n",
    "            input_games = [games[i] for i in input_games]\n",
    "        with t.inference_mode():\n",
    "            _, cache = model.run_with_cache(\n",
    "                board_seqs_int[input_games, :-1].to(device),\n",
    "                return_type=None,\n",
    "                names_filter=lambda name: name in act_names\n",
    "                # names_filter=lambda name: name == f\"blocks.{layer}.hook_resid_mid\" or name == f\"blocks.{layer}.mlp.hook_post\"\n",
    "                # names_filter=lambda name: name == f\"blocks.{layer}.hook_resid_pre\" or name == f\"blocks.{layer}.mlp.hook_post\"\n",
    "            )\n",
    "        for act_name in act_names:\n",
    "            act_name_results[act_name] += [cache[act_name].detach().cpu()]\n",
    "    for act_name in act_names:\n",
    "        act_name_results[act_name] = t.cat(act_name_results[act_name], dim=0)\n",
    "        act_name_results[act_name] = act_name_results[act_name][:num_games]\n",
    "    return act_name_results\n",
    "\n",
    "probes = {}\n",
    "probe_names = [\"linear\", \"flipped\", \"placed\"]\n",
    "for probe_name in probe_names:\n",
    "    probe = []\n",
    "    for layer in range(8):\n",
    "        probe_in_layer = get_probe(layer, probe_name, \"post\")[0].detach()\n",
    "        probe.append(probe_in_layer)\n",
    "    probe : Float[Tensor, \"layer d_model row col options\"]= t.stack(probe, dim=0)\n",
    "    probes[probe_name] = probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Rule for each Game/Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Rules ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4144\n"
     ]
    }
   ],
   "source": [
    "def get_string_from_rule(rule):\n",
    "    return \" OR \".join([f\"({' AND '.join(conjunction)})\" for conjunction in rule])\n",
    "\n",
    "def get_features_in_line_rules_function(lines):\n",
    "    def get_rules():\n",
    "        rules = {}\n",
    "        # line_length = len(lines[0])\n",
    "        # assert all([len(line) == line_length for line in lines])\n",
    "        # assert line_length > 1\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                for row_delta in [-1, 0, 1]:\n",
    "                    for col_delta in [-1, 0, 1]:\n",
    "                        if row_delta == 0 and col_delta == 0:\n",
    "                            continue\n",
    "                        rule = []\n",
    "                        for line, middle_point in lines:\n",
    "                            line_length = len(line)\n",
    "                            line_length_front = line_length - middle_point - 1\n",
    "                            line_length_back = middle_point\n",
    "                            row_start = row - line_length_back * row_delta\n",
    "                            col_start = col - line_length_back * col_delta\n",
    "                            row_end = row + line_length_front * row_delta\n",
    "                            col_end = col + line_length_front * col_delta\n",
    "                            if row_start < 0 or row_start >= 8 or col_start < 0 or col_start >= 8:\n",
    "                                continue\n",
    "                            if row_end < 0 or row_end >= 8 or col_end < 0 or col_end >= 8:\n",
    "                                continue\n",
    "                            conjunction = []\n",
    "                            for i in range(line_length):\n",
    "                                row_new = row + (i - middle_point) * row_delta\n",
    "                                col_new = col + (i - middle_point) * col_delta\n",
    "                                label = tuple_to_label((row_new, col_new))\n",
    "                                features = line[i] # Den Shit zuende machen und testen ....\n",
    "                                for feature in features:\n",
    "                                    conjunction.append(f\"{label} {feature}\")\n",
    "                            rule.append(conjunction)\n",
    "                        if len(rule) == 0:\n",
    "                            continue\n",
    "                        rules[get_string_from_rule(rule)] = rule\n",
    "        return rules\n",
    "    return get_rules\n",
    "\n",
    "def get_all_rules(rule_template_list):\n",
    "    all_rules = {}\n",
    "    for lines in rule_template_list:\n",
    "        get_rules_function = get_features_in_line_rules_function(lines)\n",
    "        rules = get_rules_function()\n",
    "        all_rules.update(rules)\n",
    "    return all_rules\n",
    "\n",
    "line_rules_dict = {\n",
    "    \"flipping_test\" : [([[\"flipped\"], [\"placed\"]], 0)], # Die Reihenfolge ist sehr relevant\n",
    "    \"flipping\" : [([[\"flipped\"], [\"placed\"]], 0), # Die Reihenfolge ist sehr relevant\n",
    "                  ([[\"flipped\"], [\"flipped\"], [\"placed\"]], 0), # I like this because it's very simple!\n",
    "                  ([[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 0),\n",
    "                  ([[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 0),\n",
    "                  ([[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 0),\n",
    "                  ([[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 0),\n",
    "                  ([[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 0)],\n",
    "}\n",
    "\n",
    "# PROBLEM: Am ende not_empty hinzufügen macht alles kaputt im Moment, ich müsste dazu machen, dass die Line sich um das letzte Flipped drehen soll litereally\n",
    "\n",
    "flipping_extra_list = [\n",
    "    [([[\"not_empty\"] ,[\"not_empty\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"not_empty\"] ,[\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"not_empty\"] ,[\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"not_empty\"] ,[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"not_empty\"] ,[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"not_empty\"] ,[\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"mine\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"mine\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"mine\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"mine\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"mine\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"not_empty\"] ,[\"mine\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"not_empty\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"not_empty\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"not_empty\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"not_empty\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"not_empty\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"not_empty\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"mine\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"mine\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"mine\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"mine\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"mine\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "    [([[\"yours\"], [\"mine\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"flipped\"], [\"placed\"]], 1)],\n",
    "]\n",
    "\n",
    "# Also mit so vielen Regeln wird das scheiß lange daruern\n",
    "# Komprimieren zu 4 Regeln? würde gehen ... wäre aber wahrscheinlich schlechter ...\n",
    "# Ich könnte eine Funktion machen die Effizient alle Regeln findet ... Ich weiß garnicht wie einfach das ist, ich muss ja mit vektorisierten Funktionen competen (Das ist schlechte Idee)\n",
    "# Das sind insgesamt so 7000 Regeln maybe \n",
    "# => Ich probiers einfach. Worst case dauerts halt\n",
    "\n",
    "\n",
    "# Ich bin mir grade nciht sicher: davor schienen ja mit der alten Regel ganz okaye Ergebnisse zu bekommen und jede Regel benötigt mehr Zeit\n",
    "# lostlostlostlsotssdltostosotstlotlso\n",
    "# Eigneltich müsste ich halt gucken welche Regenl actually diese Neurons gut beschreiben ... Meine Fresse\n",
    "# Okay oKay . da ich wirklich merke ich habe keinen Bock ahhhhhhhh \n",
    "# Oha nur 4000 Regeln\n",
    "\n",
    "rules = get_all_rules(flipping_extra_list)\n",
    "# print(list(rules.keys())[1000:1200])\n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probe_results(resid : Float[Tensor, \"batch layer pos d_model\"]) -> Dict[str, Float[Tensor, \"batch layer pos row col\"]]:\n",
    "    probe_results = {}\n",
    "    for probe_name in probe_names:\n",
    "        probe = probes[probe_name]\n",
    "        probe_result = einops.einsum(resid, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        probe_result = probe_result.argmax(dim=-1)\n",
    "        probe_results[probe_name] = probe_result\n",
    "    return probe_results\n",
    "\n",
    "def get_probe_name_and_option_from_feature(feature):\n",
    "    if feature == \"flipped\":\n",
    "        return \"flipped\", FLIPPED\n",
    "    elif feature == \"placed\":\n",
    "        return \"placed\", PLACED\n",
    "    elif feature == \"yours\":\n",
    "        return \"linear\", YOURS\n",
    "    elif feature == \"mine\":\n",
    "        return \"linear\", MINE\n",
    "    elif feature == \"empty\":\n",
    "        return \"linear\", EMPTY\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature: {feature}\")\n",
    "\n",
    "def get_games_positions_layers_that_follow_rule(probe_results, rule):\n",
    "    rule_bool = t.zeros(BATCH_SIZE, 8, 59, device=device, dtype=t.bool)\n",
    "    for conjunction in rule:\n",
    "        conjunction_bool = t.ones(BATCH_SIZE, 8, 59, device=device, dtype=t.bool)\n",
    "        for literal in conjunction:\n",
    "            label, feature = literal.split(\" \") # Look if I can make Not_Feature possible. Otherwise change the rules ... Not_Empty doesen't work!\n",
    "            not_feature = False\n",
    "            if feature[:4] == \"not_\":\n",
    "                feature = feature[4:]\n",
    "                not_feature = True\n",
    "            row, col = label_to_tuple(label)\n",
    "            probe_name, option = get_probe_name_and_option_from_feature(feature)\n",
    "            if not_feature:\n",
    "                literal_bool = probe_results[probe_name][:, :, :, row, col] != option\n",
    "            else:\n",
    "                literal_bool = probe_results[probe_name][:, :, :, row, col] == option\n",
    "            conjunction_bool &= literal_bool\n",
    "        rule_bool |= conjunction_bool\n",
    "    return rule_bool\n",
    "\n",
    "# TODO: For each rule, layer and position I need the games where the rule is active, Think about what the format should be for the next steps ...\n",
    "# TODO: Make this Modular so that I can also apply it to board_seqs_int_test\n",
    "# TODO: Mhh this takes a shitload of time ...\n",
    "# NO: for each game, layer and position I need one Rule that is active, What about no Rule ... I guess I could add just placed or something\n",
    "# Take out games with no rule ...\n",
    "def get_fake_cache(board_seqs_int, num_games, start):\n",
    "    act_names = [f\"blocks.{layer}.ln2.hook_normalized\" for layer in range(8)]\n",
    "    act_names += [utils.get_act_name(\"mlp_post\", layer) for layer in range(8)]\n",
    "    fake_cache = get_activation(board_seqs_int, act_names, num_games, start)\n",
    "    return fake_cache\n",
    "\n",
    "def get_games_for_rule_layer(rules, fake_cache, start, num_games):\n",
    "    games_for_rule_layer = {}\n",
    "    for rule_idx, rule_str in enumerate(rules):\n",
    "        for layer in range(8):\n",
    "            games_for_rule_layer[(rule_idx, layer)] = []\n",
    "\n",
    "    for rule_idx, rule_str in tqdm(enumerate(rules), total=len(rules)):\n",
    "        rule = rules[rule_str]\n",
    "        for batch in range(start, start + num_games, BATCH_SIZE):\n",
    "            act_name_results = {act_name : result[batch:batch+BATCH_SIZE].to(device) for act_name, result in fake_cache.items() if \"ln2\" in act_name}\n",
    "            resid : Float[Tensor, \"batch layer pos d_model\"] = t.stack(list(act_name_results.values()), dim=1)\n",
    "            probe_result = get_probe_results(resid)\n",
    "            # print(rule_idx, batch)\n",
    "            rule_bool : Float[Tensor, \"batch layer pos\"] = get_games_positions_layers_that_follow_rule(probe_result, rule)\n",
    "            for layer in range(8): # This actually takes most of the time ...\n",
    "                for pos in range(59):\n",
    "                    games_for_rule_layer_pos = (rule_bool[:, layer, pos].nonzero().flatten() + start).tolist()\n",
    "                    games_for_rule_layer[(rule_idx, layer)] += [(game, pos) for game in games_for_rule_layer_pos]\n",
    "    return games_for_rule_layer\n",
    "\n",
    "# Get avg Neuron Acts for each position and layer \n",
    "def get_avg_mlp_over_pos(num_games = 1000):\n",
    "    act_names = [utils.get_act_name(\"mlp_post\", layer) for layer in range(8)]\n",
    "    fake_cache = get_activation(board_seqs_int_train, act_names, num_games=num_games, start=NUM_GAMES_TRAIN)\n",
    "    avg_mlp_post = t.stack([fake_cache[act_name].to(device) for act_name in act_names])\n",
    "    avg_mlp_post = avg_mlp_post.mean(dim=1)\n",
    "    avg_mlp_post_over_pos = avg_mlp_post.mean(dim=1)\n",
    "    return avg_mlp_post_over_pos, avg_mlp_post\n",
    "\n",
    "# TODO: For each rule, layer and pos: compute the top neurons\n",
    "# TODO: First I need to calculate all the mlp_post activations beforehand ...\n",
    "# TODO: Then I need to check that the rules are actually working correctly ...\n",
    "neuron_acts_diff_dict = {}\n",
    "neuron_acts_dict = {}\n",
    "# num_games_for_rule_layer= {}\n",
    "def get_neuron_acts_diff_dict(rules, avg_mlp_post_over_pos, games_for_rule_layer, fake_cache):\n",
    "    for rule_idx, rule_str in tqdm(enumerate(rules), total=len(rules)):\n",
    "        rule = rules[rule_str]\n",
    "        for layer in range(8):\n",
    "            act_name = utils.get_act_name(\"mlp_post\", layer)\n",
    "            act_names = [act_name]\n",
    "            games_and_positions = games_for_rule_layer[(rule_idx, layer)]\n",
    "            if len(games_and_positions) == 0:\n",
    "                continue\n",
    "            index_tensor = t.Tensor(games_and_positions).to(dtype=t.int)\n",
    "            # print(rule_idx, layer, pos, len(games))\n",
    "            avg_mlp_post_layer = fake_cache[act_name].to(device)[index_tensor[:, 0], index_tensor[:, 1]].mean(dim=0)\n",
    "            neuron_acts_diff = avg_mlp_post_layer - avg_mlp_post_over_pos[layer]\n",
    "            neuron_acts_diff_dict[(rule_idx, layer)] = neuron_acts_diff\n",
    "            # num_games_for_rule_layer[(rule_idx, layer)] = len(games_and_positions)\n",
    "            neuron_acts_dict[(rule_idx, layer)] = avg_mlp_post_layer\n",
    "            # if len(games_and_positions) > 5 and len(games_and_positions) < 500 and layer <= 4:\n",
    "            #     print(rule_idx, layer, len(games_and_positions), neuron_acts_diff.abs().mean().item())\n",
    "    return neuron_acts_diff_dict, neuron_acts_dict\n",
    "\n",
    "# TODO: Turn this into a function that I can easily swap out ...\n",
    "def get_top_neurons_and_activations_per_rule_layer(neuron_acts_diff_dict, neuron_acts_dict, num_neurons=30, neuron_mean_activation_difference_threshold=0.17):\n",
    "    top_neurons_and_activations_per_rule_layer = {}\n",
    "    for rule, layer in neuron_acts_diff_dict:\n",
    "        neuron_acts_diff = neuron_acts_diff_dict[(rule, layer)]\n",
    "        neuron_acts = neuron_acts_dict[(rule, layer)]\n",
    "        # Get top neurons based on activation differences\n",
    "        top_neurons = neuron_acts_diff.abs().topk(num_neurons, largest=True)\n",
    "        neuron_indices = top_neurons.indices\n",
    "        # Get corresponding activations\n",
    "        selected_neuron_acts = neuron_acts[neuron_indices]\n",
    "        # Filter based on threshold\n",
    "        mask = selected_neuron_acts >= neuron_mean_activation_difference_threshold\n",
    "        filtered_indices = neuron_indices[mask]\n",
    "        filtered_acts = selected_neuron_acts[mask]    \n",
    "        # Only store if there are neurons that meet the criteria\n",
    "        if len(filtered_indices) > 0:\n",
    "            top_neurons_and_activations_per_rule_layer[(rule, layer)] = (filtered_indices, filtered_acts)\n",
    "    return top_neurons_and_activations_per_rule_layer\n",
    "\n",
    "def get_rules_for_game_layer_pos(games_for_rule_layer):\n",
    "    rules_for_game_layer_pos = {}\n",
    "    for rule_idx, layer in games_for_rule_layer:\n",
    "        # Qickfix\n",
    "        # if debug and rule_idx > 10:\n",
    "        #     continue\n",
    "        games_and_positions = games_for_rule_layer[(rule_idx, layer)]\n",
    "        for game, pos in games_and_positions:\n",
    "            if (game, layer, pos) not in rules_for_game_layer_pos:\n",
    "                rules_for_game_layer_pos[(game, layer, pos)] = []\n",
    "            rules_for_game_layer_pos[(game, layer, pos)].append(rule_idx)\n",
    "    return rules_for_game_layer_pos\n",
    "\n",
    "def get_neurons_for_game_layer_pos(rules_for_game_layer_pos, top_neurons_and_activations_per_rule_layer):\n",
    "    neuron_indices_for_game_layer_pos = {}\n",
    "    neuron_activations_for_game_layer_pos = {}\n",
    "    for game, layer, pos in rules_for_game_layer_pos:\n",
    "        rules = rules_for_game_layer_pos[(game, layer, pos)]\n",
    "        neuron_indices_all_rules = []\n",
    "        neuron_activations_all_rules = []\n",
    "        for rule in rules:\n",
    "            if (rule, layer) not in top_neurons_and_activations_per_rule_layer:\n",
    "                continue\n",
    "            neuron_indices, neuron_activations = top_neurons_and_activations_per_rule_layer[(rule, layer)]\n",
    "            neuron_indices_all_rules += neuron_indices.tolist()\n",
    "            neuron_activations_all_rules += neuron_activations.tolist()\n",
    "        neuron_to_activations_dict = {}\n",
    "        for neuron_idx, neuron_act in zip(neuron_indices_all_rules, neuron_activations_all_rules):\n",
    "            if neuron_idx not in neuron_to_activations_dict:\n",
    "                neuron_to_activations_dict[neuron_idx] = []\n",
    "            neuron_to_activations_dict[neuron_idx].append(neuron_act)\n",
    "        for neuron_idx in neuron_to_activations_dict:\n",
    "            neuron_to_activations_dict[neuron_idx] = max(neuron_to_activations_dict[neuron_idx])\n",
    "        if not neuron_to_activations_dict:\n",
    "            continue\n",
    "        neuron_indices_for_game_layer_pos[(game, layer, pos)] = list(neuron_to_activations_dict.keys())\n",
    "        neuron_activations_for_game_layer_pos[(game, layer, pos)] = list(neuron_to_activations_dict.values())\n",
    "    return neuron_indices_for_game_layer_pos, neuron_activations_for_game_layer_pos\n",
    "\n",
    "def bundle_fake_cache(fake_cache):\n",
    "    bundled_fake_cache = {}\n",
    "    key_names = list(set([\".\".join(key.split(\".\")[2:]) for key in fake_cache]))\n",
    "    for key_name in key_names:\n",
    "        act_name_results = {act_name : result for act_name, result in fake_cache.items() if key_name in act_name}\n",
    "        stacked_result : Float[Tensor, \"batch layer pos neurons\"] = t.stack(list(act_name_results.values()), dim=1)\n",
    "        bundled_fake_cache[key_name] = stacked_result\n",
    "    return bundled_fake_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dinge die noch flasch sein könnten: Ich habe inputs vergessen und die Funktion nimmt die dann von den globalen ...\n",
    "# TODO Encapsulate this in a function ...\n",
    "# Die Funktion hat kack Cohesion\n",
    "def run_all(rules, num_neurons=30, neruon_mean_activation_difference_threshold = 0.17):\n",
    "    #  line_rules_name = \"flipping_test\"\n",
    "    # line_rule_function = get_features_in_line_rules_function(line_rules_dict[line_rules_name])\n",
    "    # print(\"Getting Line Rules ...\")\n",
    "    # rules2 = line_rule_function()\n",
    "\n",
    "    print(\"Getting Fake Cache ...\")\n",
    "    fake_cache = get_fake_cache(board_seqs_int_train, NUM_GAMES_TRAIN, START_TRAIN)\n",
    "\n",
    "    print(\"Getting Games for Rule Layer ...\")\n",
    "    games_for_rule_layer = get_games_for_rule_layer(rules, fake_cache, start=START_TRAIN, num_games=NUM_GAMES_TRAIN)\n",
    "\n",
    "    print(\"Getting Avg MLP Post Over Pos ...\")\n",
    "    avg_mlp_post_over_pos, avg_mlp_post = get_avg_mlp_over_pos(MLP_BATCH_SIZE)\n",
    "\n",
    "    print(\"Getting Neuron Acts Diff Dict ...\")\n",
    "    neuron_acts_diff_dict, neuron_acts_dict = get_neuron_acts_diff_dict(rules, avg_mlp_post_over_pos, games_for_rule_layer, fake_cache)\n",
    "\n",
    "    print(\"Getting Top Neurons and Activations Per Rule Layer ...\")\n",
    "    top_neurons_and_activations_per_rule_layer = get_top_neurons_and_activations_per_rule_layer(neuron_acts_diff_dict, neuron_acts_dict, num_neurons, neruon_mean_activation_difference_threshold)\n",
    "    return fake_cache, games_for_rule_layer, avg_mlp_post_over_pos, avg_mlp_post, neuron_acts_diff_dict, neuron_acts_dict, top_neurons_and_activations_per_rule_layer\n",
    "\n",
    "def run_all_valid(rules, top_neurons_and_activations_per_rule_layer):\n",
    "    print(\"Getting Rules for Game Layer Pos ...\")\n",
    "    # Eigentlich müsste ich das hier auf dem Test Set machen ...\n",
    "    act_names = [utils.get_act_name(\"mlp_post\", layer) for layer in range(8)]\n",
    "    act_names += [utils.get_act_name(\"attn_out\", layer) for layer in range(8)]\n",
    "    act_names += [utils.get_act_name(\"resid_pre\", layer) for layer in range(8)]\n",
    "    act_names += [f\"blocks.{layer}.ln1.hook_normalized\" for layer in range(8)]\n",
    "    act_names += [f\"blocks.{layer}.ln2.hook_normalized\" for layer in range(8)]\n",
    "    fake_cache_valid = get_activation(board_seqs_int_test, act_names, start=START_VALID, num_games=NUM_GAMES_VALID)\n",
    "    # fake_cache_valid = get_fake_cache(board_seqs_int_test, NUM_GAMES_VALID, START_VALID)\n",
    "    games_for_rule_layer_valid = get_games_for_rule_layer(rules, fake_cache_valid, start=START_VALID, num_games=NUM_GAMES_VALID)\n",
    "    rules_for_game_layer_pos_valid = get_rules_for_game_layer_pos(games_for_rule_layer_valid)\n",
    "    neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid = get_neurons_for_game_layer_pos(rules_for_game_layer_pos_valid, top_neurons_and_activations_per_rule_layer)\n",
    "    return fake_cache_valid, neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = model.W_out.detach()\n",
    "b_out = model.b_out.detach()\n",
    "\n",
    "def get_masks(flipped_final_real, flipped_final_pred):\n",
    "    mask = t.zeros_like(flipped_final_real).to(dtype=t.int)\n",
    "    only_real_mask = t.zeros_like(flipped_final_real).to(dtype=t.int)\n",
    "    only_pred_mask = t.zeros_like(flipped_final_real).to(dtype=t.int)\n",
    "    mask[:, 0] = flipped_final_real[:, 0]\n",
    "    for layer in range(1, 8):\n",
    "        change_real = (flipped_final_real[:, layer - 1] != flipped_final_real[:, layer])\n",
    "        change_pred = (flipped_final_real[:, layer - 1] != flipped_final_pred[:, layer]) # This is correct!\n",
    "        mask[:, layer] = (change_real | change_pred).to(dtype=t.int)\n",
    "        only_real_mask[:, layer] = change_real.to(dtype=t.int)\n",
    "        only_pred_mask[:, layer] = change_pred.to(dtype=t.int)\n",
    "    return mask, only_real_mask, only_pred_mask\n",
    "\n",
    "def orthogonalize_vector_to_group(a, B, normalize=True):\n",
    "    \"\"\"Orthogonalizes vector a against a list of vectors B without in-place modification using PyTorch\"\"\"\n",
    "    orthogonal_a = a.clone()  # Create a copy of a to avoid in-place modification\n",
    "    B_prev = []\n",
    "    for b in B:\n",
    "        if not all([b @ b_prev < 1e-6 for b_prev in B_prev]):\n",
    "            b = orthogonalize_vector_to_group(b, B_prev)\n",
    "        # Project orthogonal_a onto b\n",
    "        projection = einops.repeat(einops.einsum(a, b, \"... d_model, d_model -> ...\") / t.dot(b, b), \"... -> ... d_model\", d_model = b.shape[0]) * b\n",
    "        # Update orthogonal_a by subtracting the projection\n",
    "        orthogonal_a = orthogonal_a - projection\n",
    "        B_prev += [b]\n",
    "    \n",
    "    # Normalize the resulting vector orthogonal_a\n",
    "    if normalize:\n",
    "        orthogonal_a = orthogonal_a / t.norm(orthogonal_a)\n",
    "    \n",
    "    return orthogonal_a\n",
    "\n",
    "def orthogonalize_vectors(vectors, normalize=True):\n",
    "    new_vectors = []\n",
    "    for vector in vectors:\n",
    "        vector = orthogonalize_vector_to_group(vector, new_vectors, normalize=normalize)\n",
    "        new_vectors += [vector]\n",
    "    return new_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_beginning(num_neurons):\n",
    "    rules = get_all_rules(flipping_extra_list)\n",
    "    if DEBUG:\n",
    "        keys = list(rules.keys())\n",
    "        random.shuffle(keys)\n",
    "        rules = {key: rules[key] for key in keys[:NUM_RULES]}\n",
    "    # Neuron_Acts_Diff_Dict: Contains Mean Neuron Activation Difference for each Rule and Layer\n",
    "    fake_cache, games_for_rule_layer, avg_mlp_post_over_pos, avg_mlp_post, neuron_acts_diff_dict, neuron_acts_dict, top_neurons_and_activations_per_rule_layer = run_all(rules, num_neurons)\n",
    "\n",
    "    # top_neurons_and_activations_per_rule_layer = get_top_neurons_and_activations_per_rule_layer(neuron_acts_diff_dict, neuron_acts_dict, 2048)\n",
    "    fake_cache_valid, neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid = run_all_valid(rules, top_neurons_and_activations_per_rule_layer)\n",
    "\n",
    "    act_names = [utils.get_act_name(\"mlp_post\", layer) for layer in range(8)]\n",
    "    act_names += [utils.get_act_name(\"attn_out\", layer) for layer in range(8)]\n",
    "    act_names += [utils.get_act_name(\"resid_pre\", layer) for layer in range(8)]\n",
    "    act_names += [utils.get_act_name(\"resid_post\", layer) for layer in range(8)]\n",
    "    act_names += [f\"blocks.{layer}.ln1.hook_normalized\" for layer in range(8)]\n",
    "    act_names += [f\"blocks.{layer}.ln2.hook_normalized\" for layer in range(8)]\n",
    "    fake_cache_valid = get_activation(board_seqs_int_test, act_names, start=START_VALID, num_games=NUM_GAMES_VALID)\n",
    "\n",
    "    bundled_fake_cache_valid = bundle_fake_cache(fake_cache_valid)\n",
    "    return bundled_fake_cache_valid, neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid, avg_mlp_post, neuron_acts_diff_dict\n",
    "\n",
    "def evaluate_rules(bundled_fake_cache_valid, neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid, avg_mlp_post, use_real_attention, use_real_neuron_acts):\n",
    "    results_dict = {\n",
    "    \"avg_neuron_count\" : t.zeros(8, 59).to(device),\n",
    "    \"abs_mean_diff_flipped\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"abs_mean_diff_not_flipped\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"TP_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"FP_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"TN_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"FN_diff\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"TP_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    # \"FP_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    \"TN_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    # \"FN_final\" : t.zeros(8, 59, 8, 8).to(device),\n",
    "    }\n",
    "    total_number_of_neurons = t.zeros(8, 59).to(device)\n",
    "    total_number_of_predictions = t.zeros(8, 59).to(device)\n",
    "    mask_sum = t.zeros(8, 59, 8, 8, device=device)\n",
    "\n",
    "    probe = probes[\"flipped\"]\n",
    "    probe_lists = {}\n",
    "    for layer in range(8):\n",
    "        probe_lists[layer] = []\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                probe_lists[layer].append(probe[layer, :, row, col, FLIPPED])\n",
    "        \n",
    "    for batch in range(START_VALID, START_VALID + NUM_GAMES_VALID, BATCH_SIZE):\n",
    "        mlp_post_real : Float[Tensor, \"batch layer pos neurons\"] = bundled_fake_cache_valid[\"mlp.hook_post\"][batch:batch+BATCH_SIZE].to(device)\n",
    "        resid_pre_real : Float[Tensor, \"batch layer pos d_model\"] = bundled_fake_cache_valid[\"hook_resid_pre\"][batch:batch+BATCH_SIZE].to(device)\n",
    "        if use_real_attention:\n",
    "            attn_out_real : Float[Tensor, \"batch layer pos d_model\"] = bundled_fake_cache_valid[\"hook_attn_out\"][batch:batch+BATCH_SIZE].to(device)\n",
    "        else:\n",
    "            resid_layernorm : Float[Tensor, \"batch layer pos d_model\"] = bundled_fake_cache_valid[\"ln1.hook_normalized\"][batch:batch+BATCH_SIZE].to(device)\n",
    "            for layer in range(8):\n",
    "                resid_layernorm[:, layer] = orthogonalize_vector_to_group(resid_layernorm[:, layer], probe_lists[layer])\n",
    "                # ...\n",
    "            \n",
    "        # resid_post_real : Float[Tensor, \"batch layer pos d_model\"] = bundled_fake_cache_valid[\"hook_resid_post\"][batch:batch+BATCH_SIZE].to(device)\n",
    "        num_games = mlp_post_real.shape[0]\n",
    "        mlp_post_pred = einops.repeat(avg_mlp_post, \"layer pos neurons -> batch layer pos neurons\", batch=num_games).clone()\n",
    "        for game in range(num_games):\n",
    "            for layer in range(8):\n",
    "                for pos in range(59):\n",
    "                    if (game, layer, pos) not in neuron_indices_for_game_layer_pos_valid:\n",
    "                        continue\n",
    "                    neuron_indices = neuron_indices_for_game_layer_pos_valid[(game, layer, pos)]\n",
    "                    neuron_acts = t.Tensor(neuron_activations_for_game_layer_pos_valid[(game, layer, pos)]).to(device)\n",
    "                    if use_real_neuron_acts:\n",
    "                        neuron_acts = mlp_post_real[game, layer, pos, neuron_indices]\n",
    "                    mlp_post_pred[game, layer, pos, neuron_indices] = neuron_acts\n",
    "                    total_number_of_neurons[layer, pos] += len(neuron_indices)\n",
    "                    total_number_of_predictions[layer, pos] += 1\n",
    "\n",
    "        mlp_out_real = einops.einsum(mlp_post_real, W_out, \"batch layer pos neurons, layer neurons d_model -> batch layer pos d_model\")\n",
    "        flipped_logits_real = einops.einsum(mlp_out_real + attn_out_real, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_real = flipped_logits_real.argmax(dim=-1)\n",
    "\n",
    "        mlp_out_pred = einops.einsum(mlp_post_pred, W_out, \"batch layer pos neurons, layer neurons d_model -> batch layer pos d_model\")\n",
    "        flipped_logits_pred = einops.einsum(mlp_out_pred + attn_out_real, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_pred = flipped_logits_pred.argmax(dim=-1)\n",
    "\n",
    "        resid_post_real = resid_pre_real + mlp_out_real + attn_out_real + einops.repeat(b_out, \"layer d_model -> layer pos d_model\", pos=59)\n",
    "        final_logits_real = einops.einsum(resid_post_real, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_final_real = (final_logits_real[:, :, :, :, :, 0] > final_logits_real[:, :, :, :, :, 1]).to(t.int)\n",
    "        resid_post_pred = resid_pre_real + mlp_out_pred + attn_out_real + einops.repeat(b_out, \"layer d_model -> layer pos d_model\", pos=59)\n",
    "        final_logits_pred = einops.einsum(resid_post_pred, probe, \"batch layer pos d_model, layer d_model row col options -> batch layer pos row col options\")\n",
    "        flipped_final_pred = (final_logits_pred[:, :, :, :, :, 0] > final_logits_pred[:, :, :, :, :, 1]).to(t.int)\n",
    "        mask, only_real_mask, only_pred_mask = get_masks(flipped_final_real, flipped_final_pred)\n",
    "        mask_sum += mask.sum(dim=0)\n",
    "\n",
    "        abs_diff_flipped = (flipped_logits_real[:, :, :, :, :, FLIPPED] - flipped_logits_pred[:, :, :, :, :, FLIPPED]).abs().sum(dim=0)\n",
    "        abs_diff_not_flipped = (flipped_logits_real[:, :, :, :, :, NOT_FLIPPED] - flipped_logits_pred[:, :, :, :, :, NOT_FLIPPED]).abs().sum(dim=0)\n",
    "        TP = ((flipped_real == FLIPPED) & (flipped_pred == FLIPPED) & mask).sum(dim=0).float()\n",
    "        FP = ((flipped_real == NOT_FLIPPED) & (flipped_pred == FLIPPED) & mask).sum(dim=0).float()\n",
    "        TN = ((flipped_real == NOT_FLIPPED) & (flipped_pred == NOT_FLIPPED) & mask).sum(dim=0).float()\n",
    "        FN = ((flipped_real == FLIPPED) & (flipped_pred == NOT_FLIPPED) & mask).sum(dim=0).float()\n",
    "        results_dict[\"abs_mean_diff_flipped\"] += abs_diff_flipped\n",
    "        results_dict[\"abs_mean_diff_not_flipped\"] += abs_diff_not_flipped\n",
    "        results_dict[\"TP_diff\"] += TP\n",
    "        results_dict[\"FP_diff\"] += FP\n",
    "        results_dict[\"TN_diff\"] += TN\n",
    "        results_dict[\"FN_diff\"] += FN\n",
    "\n",
    "        flipped_change_real = (flipped_real == FLIPPED) & only_real_mask\n",
    "        not_flipped_change_real = (flipped_real == NOT_FLIPPED) & only_real_mask\n",
    "        flipped_change_pred = (flipped_pred == FLIPPED) & only_pred_mask\n",
    "        not_flipped_change_pred = (flipped_pred == NOT_FLIPPED) & only_pred_mask\n",
    "        TP_final = (flipped_change_real & flipped_change_pred).sum(dim=0).float()\n",
    "        FP_final = (not_flipped_change_real & flipped_change_pred).sum(dim=0).float()\n",
    "        TN_final = (not_flipped_change_real & not_flipped_change_pred).sum(dim=0).float()\n",
    "        FN_final = (flipped_change_real & not_flipped_change_pred).sum(dim=0).float()\n",
    "        # DICLAIMER: False Positive and False Negative where not done write, but I can get accuracy using mask_sum ...\n",
    "        results_dict[\"TP_final\"] += TP_final\n",
    "        # results_dict[\"FP_final\"] += FP_final\n",
    "        results_dict[\"TN_final\"] += TN_final\n",
    "        # results_dict[\"FN_final\"] += FN_final\n",
    "    results_dict[\"abs_mean_diff_flipped\"] /= mask_sum\n",
    "    results_dict[\"abs_mean_diff_not_flipped\"] /= mask_sum\n",
    "    results_dict[\"avg_neuron_count\"] = total_number_of_neurons / total_number_of_predictions\n",
    "    return results_dict, mask_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 20 Neurons\n",
      "Getting Fake Cache ...\n",
      "Getting activations ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Games for Rule Layer ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeabf3179f2e4411a2ddbee9dd184953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Avg MLP Post Over Pos ...\n",
      "Getting activations ...\n",
      "Getting Neuron Acts Diff Dict ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d88b648ac9f4e9281438314ebe603b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Top Neurons and Activations Per Rule Layer ...\n",
      "Getting Rules for Game Layer Pos ...\n",
      "Getting activations ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9261b30e2740828ea2ca9454d236fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting activations ...\n",
      "Running with real_attention: True\n",
      "Running with real_neuron_acts: True\n",
      "Saved!\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "for num_neurons in [20, 2048]:\n",
    "    print(f\"Running with {num_neurons} Neurons\")\n",
    "    bundled_fake_cache_valid, neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid, avg_mlp_post, neuron_acts_diff_dict = run_beginning(num_neurons)\n",
    "    for use_real_attention in [True]:\n",
    "        print(f\"Running with real_attention: {use_real_attention}\")\n",
    "        for use_real_neuron_acts in [True, False]:\n",
    "            print(f\"Running with real_neuron_acts: {use_real_neuron_acts}\")\n",
    "            results_dict, mask_sum = evaluate_rules(bundled_fake_cache_valid, neuron_indices_for_game_layer_pos_valid, neuron_activations_for_game_layer_pos_valid, avg_mlp_post, use_real_attention, use_real_neuron_acts)\n",
    "            directory = \"flipping_circuit_results\"\n",
    "            run_name = f\"num_neurons_{num_neurons}_real_attention_{use_real_attention}_real_neuron_acts_{use_real_neuron_acts}\"\n",
    "            save_path = f\"{directory}/{run_name}\"\n",
    "            # os.makedirs(save_path)\n",
    "            t.save(results_dict, f\"{save_path}/flipped_circuit_results_dict.pth\")\n",
    "            t.save(mask_sum, f\"{save_path}/flipped_circuit_mask_sum.pth\")\n",
    "            t.save(neuron_acts_diff_dict, f\"{save_path}/flipped_circuit_neuron_acts_diff_dict.pth\")\n",
    "            print(\"Saved!\")\n",
    "            if DEBUG:\n",
    "                break\n",
    "        if DEBUG:\n",
    "            break\n",
    "    if DEBUG:\n",
    "        break\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   nan,    nan, 0.0000, 0.3333, 0.4000, 0.0000, 0.0000,    nan],\n",
       "        [0.0000, 0.1000, 0.1667, 0.2273, 0.2000, 0.0952, 0.4444, 0.0000],\n",
       "        [0.0000, 0.3478, 0.1579, 0.1702, 0.2439, 0.3333, 0.1579, 1.0000],\n",
       "        [0.0000, 0.1333, 0.1591, 0.2778, 0.2807, 0.2667, 0.1200, 0.0000],\n",
       "        [0.6667, 0.3000, 0.3095, 0.2338, 0.2692, 0.2889, 0.1935, 0.3333],\n",
       "        [0.2500, 0.2667, 0.2632, 0.2000, 0.2889, 0.2069, 0.3000, 0.2500],\n",
       "        [0.0000, 0.3333, 0.2727, 0.1053, 0.3200, 0.2500, 0.2000, 0.5000],\n",
       "        [   nan, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,    nan]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"flipping_circuit_results/num_neurons_20_real_attention_True_real_neuron_acts_False\"\n",
    "results_dict = t.load(f\"{save_path}/flipped_circuit_results_dict.pth\")\n",
    "mask_sum = t.load(f\"{save_path}/flipped_circuit_mask_sum.pth\")\n",
    "\n",
    "def get_accuracy(results_dict, diff_or_final=\"diff\"):\n",
    "    # Diff views positive logits as a positive sample, even when the approximation did not predict a change ...\n",
    "    # Final views positive logits as a positive sample, only when the approximation predicted a change ...\n",
    "    tp_str = f\"TP_{diff_or_final}\"\n",
    "    fp_str = f\"FP_{diff_or_final}\"\n",
    "    tn_str = f\"TN_{diff_or_final}\"\n",
    "    fn_str = f\"FN_{diff_or_final}\"\n",
    "    tp = results_dict[tp_str]\n",
    "    # fp = results_dict[fp_str]\n",
    "    tn = results_dict[tn_str]\n",
    "    # fn = results_dict[fn_str]\n",
    "    tp = tp.sum(dim=1)\n",
    "    tn = tn.sum(dim=1)\n",
    "    mask_sum_new = mask_sum.sum(dim=1)\n",
    "    acc = (tp + tn) / mask_sum_new\n",
    "    # acc = (results_dict[tp] + results_dict[tn]) / (results_dict[tp] + results_dict[tn] + results_dict[fp] + results_dict[fn])\n",
    "    return acc\n",
    "\n",
    "# Eval sieht aus wie: Erste Layer decent, sonst kacke ... (naja ...), Layer 3 seems better again. idk lets see (It's good enough that I can put it is the thesis)\n",
    "acc = get_accuracy(results_dict, \"final\")\n",
    "acc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_sum.sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tensor([[   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan],\n",
    "        [   nan, 0.5000, 1.0000, 0.0000, 0.0000, 0.5000, 1.0000,    nan],\n",
    "        [   nan,    nan, 0.7778, 0.8333, 0.8333, 0.8571, 1.0000,    nan],\n",
    "        [   nan, 1.0000, 0.8929, 0.8409, 0.8070, 0.5789, 0.5000,    nan],\n",
    "        [   nan,    nan, 0.8824, 0.8372, 0.7619, 0.8846, 1.0000,    nan],\n",
    "        [   nan,    nan, 1.0000, 0.7500, 0.8696, 0.8750,    nan,    nan],\n",
    "        [   nan,    nan, 0.5000, 0.8333, 0.0000, 1.0000, 1.0000,    nan],\n",
    "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan]],\n",
    "       device='cuda:0')\n",
    "'''\n",
    "\n",
    "# First: turn games_for_rule_layer into rules for game, layer, pos (10)\n",
    "\n",
    "# Iterate over games like above\n",
    "# MLP_post [games, layer, pos, neurons]\n",
    "# Neuron_indices [games, layer, pos, neurons] (I need to get that ...) (15) , Rest: (10)\n",
    "# Neuron_acts [games, layer, pos, neurons]\n",
    "# then the Rest is Chill\n",
    "# Evaluate the my mlp approximation\n",
    "# Copy beginning from above ...\n",
    "# Get top neurons, change out ...\n",
    "# Get the Logits and Take the Difference\n",
    "# Output: Real Logits, Pred Logits, Logit Diff Pred, Logit Diff Real: [Game, Layer, Pos, Tile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game = 23\\nlayer = 6\\npos = 51\\nneuron_indices = neuron_indices_for_game_layer_pos_valid[game, layer, pos]\\nneuron_activations = neuron_activations_for_game_layer_pos_valid[game, layer, pos]\\nact_name = utils.get_act_name(\"mlp_post\", layer)\\nact_names = [act_name]\\nfake_cache_mlp_post = get_activation(board_seqs_int_test, act_names, num_games=1, start=0, games=[game])\\nmlp_post = fake_cache_mlp_post[act_name].to(device)[0, pos]\\nprint(neuron_indices)\\nprint([round(act, 4) for act in neuron_activations])\\nprint([round(act.item(), 4) for act in list(mlp_post[neuron_indices])])'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for some games, which rules are active, and do the neurons make sense ...\n",
    "# It probably would be good to test this ...\n",
    "# print(neuron_indices_for_game_layer_pos[60, 2, 21])\n",
    "# print(neuron_activations_for_game_layer_pos[60, 2, 21])\n",
    "\n",
    "# Look at Neuron Activations, which ones look interesting\n",
    "# Get MLP post from the game / position and make scatter plot ...\n",
    "# list(neuron_activations_for_game_layer_pos_valid.items())[200:220]\n",
    "\n",
    "# 95, 1, 56\n",
    "# 18, 1, 57\n",
    "# 23, 6, 51\n",
    "\n",
    "# Das sieht schonmal promissing aus !!!\n",
    "'''game = 23\n",
    "layer = 6\n",
    "pos = 51\n",
    "neuron_indices = neuron_indices_for_game_layer_pos_valid[game, layer, pos]\n",
    "neuron_activations = neuron_activations_for_game_layer_pos_valid[game, layer, pos]\n",
    "act_name = utils.get_act_name(\"mlp_post\", layer)\n",
    "act_names = [act_name]\n",
    "fake_cache_mlp_post = get_activation(board_seqs_int_test, act_names, num_games=1, start=0, games=[game])\n",
    "mlp_post = fake_cache_mlp_post[act_name].to(device)[0, pos]\n",
    "print(neuron_indices)\n",
    "print([round(act, 4) for act in neuron_activations])\n",
    "print([round(act.item(), 4) for act in list(mlp_post[neuron_indices])])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Neuron Acts Diff",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999,
          1000,
          1001,
          1002,
          1003,
          1004,
          1005,
          1006,
          1007,
          1008,
          1009,
          1010,
          1011,
          1012,
          1013,
          1014,
          1015,
          1016,
          1017,
          1018,
          1019,
          1020,
          1021,
          1022,
          1023,
          1024,
          1025,
          1026,
          1027,
          1028,
          1029,
          1030,
          1031,
          1032,
          1033,
          1034,
          1035,
          1036,
          1037,
          1038,
          1039,
          1040,
          1041,
          1042,
          1043,
          1044,
          1045,
          1046,
          1047,
          1048,
          1049,
          1050,
          1051,
          1052,
          1053,
          1054,
          1055,
          1056,
          1057,
          1058,
          1059,
          1060,
          1061,
          1062,
          1063,
          1064,
          1065,
          1066,
          1067,
          1068,
          1069,
          1070,
          1071,
          1072,
          1073,
          1074,
          1075,
          1076,
          1077,
          1078,
          1079,
          1080,
          1081,
          1082,
          1083,
          1084,
          1085,
          1086,
          1087,
          1088,
          1089,
          1090,
          1091,
          1092,
          1093,
          1094,
          1095,
          1096,
          1097,
          1098,
          1099,
          1100,
          1101,
          1102,
          1103,
          1104,
          1105,
          1106,
          1107,
          1108,
          1109,
          1110,
          1111,
          1112,
          1113,
          1114,
          1115,
          1116,
          1117,
          1118,
          1119,
          1120,
          1121,
          1122,
          1123,
          1124,
          1125,
          1126,
          1127,
          1128,
          1129,
          1130,
          1131,
          1132,
          1133,
          1134,
          1135,
          1136,
          1137,
          1138,
          1139,
          1140,
          1141,
          1142,
          1143,
          1144,
          1145,
          1146,
          1147,
          1148,
          1149,
          1150,
          1151,
          1152,
          1153,
          1154,
          1155,
          1156,
          1157,
          1158,
          1159,
          1160,
          1161,
          1162,
          1163,
          1164,
          1165,
          1166,
          1167,
          1168,
          1169,
          1170,
          1171,
          1172,
          1173,
          1174,
          1175,
          1176,
          1177,
          1178,
          1179,
          1180,
          1181,
          1182,
          1183,
          1184,
          1185,
          1186,
          1187,
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194,
          1195,
          1196,
          1197,
          1198,
          1199,
          1200,
          1201,
          1202,
          1203,
          1204,
          1205,
          1206,
          1207,
          1208,
          1209,
          1210,
          1211,
          1212,
          1213,
          1214,
          1215,
          1216,
          1217,
          1218,
          1219,
          1220,
          1221,
          1222,
          1223,
          1224,
          1225,
          1226,
          1227,
          1228,
          1229,
          1230,
          1231,
          1232,
          1233,
          1234,
          1235,
          1236,
          1237,
          1238,
          1239,
          1240,
          1241,
          1242,
          1243,
          1244,
          1245,
          1246,
          1247,
          1248,
          1249,
          1250,
          1251,
          1252,
          1253,
          1254,
          1255,
          1256,
          1257,
          1258,
          1259,
          1260,
          1261,
          1262,
          1263,
          1264,
          1265,
          1266,
          1267,
          1268,
          1269,
          1270,
          1271,
          1272,
          1273,
          1274,
          1275,
          1276,
          1277,
          1278,
          1279,
          1280,
          1281,
          1282,
          1283,
          1284,
          1285,
          1286,
          1287,
          1288,
          1289,
          1290,
          1291,
          1292,
          1293,
          1294,
          1295,
          1296,
          1297,
          1298,
          1299,
          1300,
          1301,
          1302,
          1303,
          1304,
          1305,
          1306,
          1307,
          1308,
          1309,
          1310,
          1311,
          1312,
          1313,
          1314,
          1315,
          1316,
          1317,
          1318,
          1319,
          1320,
          1321,
          1322,
          1323,
          1324,
          1325,
          1326,
          1327,
          1328,
          1329,
          1330,
          1331,
          1332,
          1333,
          1334,
          1335,
          1336,
          1337,
          1338,
          1339,
          1340,
          1341,
          1342,
          1343,
          1344,
          1345,
          1346,
          1347,
          1348,
          1349,
          1350,
          1351,
          1352,
          1353,
          1354,
          1355,
          1356,
          1357,
          1358,
          1359,
          1360,
          1361,
          1362,
          1363,
          1364,
          1365,
          1366,
          1367,
          1368,
          1369,
          1370,
          1371,
          1372,
          1373,
          1374,
          1375,
          1376,
          1377,
          1378,
          1379,
          1380,
          1381,
          1382,
          1383,
          1384,
          1385,
          1386,
          1387,
          1388,
          1389,
          1390,
          1391,
          1392,
          1393,
          1394,
          1395,
          1396,
          1397,
          1398,
          1399,
          1400,
          1401,
          1402,
          1403,
          1404,
          1405,
          1406,
          1407,
          1408,
          1409,
          1410,
          1411,
          1412,
          1413,
          1414,
          1415,
          1416,
          1417,
          1418,
          1419,
          1420,
          1421,
          1422,
          1423,
          1424,
          1425,
          1426,
          1427,
          1428,
          1429,
          1430,
          1431,
          1432,
          1433,
          1434,
          1435,
          1436,
          1437,
          1438,
          1439,
          1440,
          1441,
          1442,
          1443,
          1444,
          1445,
          1446,
          1447,
          1448,
          1449,
          1450,
          1451,
          1452,
          1453,
          1454,
          1455,
          1456,
          1457,
          1458,
          1459,
          1460,
          1461,
          1462,
          1463,
          1464,
          1465,
          1466,
          1467,
          1468,
          1469,
          1470,
          1471,
          1472,
          1473,
          1474,
          1475,
          1476,
          1477,
          1478,
          1479,
          1480,
          1481,
          1482,
          1483,
          1484,
          1485,
          1486,
          1487,
          1488,
          1489,
          1490,
          1491,
          1492,
          1493,
          1494,
          1495,
          1496,
          1497,
          1498,
          1499,
          1500,
          1501,
          1502,
          1503,
          1504,
          1505,
          1506,
          1507,
          1508,
          1509,
          1510,
          1511,
          1512,
          1513,
          1514,
          1515,
          1516,
          1517,
          1518,
          1519,
          1520,
          1521,
          1522,
          1523,
          1524,
          1525,
          1526,
          1527,
          1528,
          1529,
          1530,
          1531,
          1532,
          1533,
          1534,
          1535,
          1536,
          1537,
          1538,
          1539,
          1540,
          1541,
          1542,
          1543,
          1544,
          1545,
          1546,
          1547,
          1548,
          1549,
          1550,
          1551,
          1552,
          1553,
          1554,
          1555,
          1556,
          1557,
          1558,
          1559,
          1560,
          1561,
          1562,
          1563,
          1564,
          1565,
          1566,
          1567,
          1568,
          1569,
          1570,
          1571,
          1572,
          1573,
          1574,
          1575,
          1576,
          1577,
          1578,
          1579,
          1580,
          1581,
          1582,
          1583,
          1584,
          1585,
          1586,
          1587,
          1588,
          1589,
          1590,
          1591,
          1592,
          1593,
          1594,
          1595,
          1596,
          1597,
          1598,
          1599,
          1600,
          1601,
          1602,
          1603,
          1604,
          1605,
          1606,
          1607,
          1608,
          1609,
          1610,
          1611,
          1612,
          1613,
          1614,
          1615,
          1616,
          1617,
          1618,
          1619,
          1620,
          1621,
          1622,
          1623,
          1624,
          1625,
          1626,
          1627,
          1628,
          1629,
          1630,
          1631,
          1632,
          1633,
          1634,
          1635,
          1636,
          1637,
          1638,
          1639,
          1640,
          1641,
          1642,
          1643,
          1644,
          1645,
          1646,
          1647,
          1648,
          1649,
          1650,
          1651,
          1652,
          1653,
          1654,
          1655,
          1656,
          1657,
          1658,
          1659,
          1660,
          1661,
          1662,
          1663,
          1664,
          1665,
          1666,
          1667,
          1668,
          1669,
          1670,
          1671,
          1672,
          1673,
          1674,
          1675,
          1676,
          1677,
          1678,
          1679,
          1680,
          1681,
          1682,
          1683,
          1684,
          1685,
          1686,
          1687,
          1688,
          1689,
          1690,
          1691,
          1692,
          1693,
          1694,
          1695,
          1696,
          1697,
          1698,
          1699,
          1700,
          1701,
          1702,
          1703,
          1704,
          1705,
          1706,
          1707,
          1708,
          1709,
          1710,
          1711,
          1712,
          1713,
          1714,
          1715,
          1716,
          1717,
          1718,
          1719,
          1720,
          1721,
          1722,
          1723,
          1724,
          1725,
          1726,
          1727,
          1728,
          1729,
          1730,
          1731,
          1732,
          1733,
          1734,
          1735,
          1736,
          1737,
          1738,
          1739,
          1740,
          1741,
          1742,
          1743,
          1744,
          1745,
          1746,
          1747,
          1748,
          1749,
          1750,
          1751,
          1752,
          1753,
          1754,
          1755,
          1756,
          1757,
          1758,
          1759,
          1760,
          1761,
          1762,
          1763,
          1764,
          1765,
          1766,
          1767,
          1768,
          1769,
          1770,
          1771,
          1772,
          1773,
          1774,
          1775,
          1776,
          1777,
          1778,
          1779,
          1780,
          1781,
          1782,
          1783,
          1784,
          1785,
          1786,
          1787,
          1788,
          1789,
          1790,
          1791,
          1792,
          1793,
          1794,
          1795,
          1796,
          1797,
          1798,
          1799,
          1800,
          1801,
          1802,
          1803,
          1804,
          1805,
          1806,
          1807,
          1808,
          1809,
          1810,
          1811,
          1812,
          1813,
          1814,
          1815,
          1816,
          1817,
          1818,
          1819,
          1820,
          1821,
          1822,
          1823,
          1824,
          1825,
          1826,
          1827,
          1828,
          1829,
          1830,
          1831,
          1832,
          1833,
          1834,
          1835,
          1836,
          1837,
          1838,
          1839,
          1840,
          1841,
          1842,
          1843,
          1844,
          1845,
          1846,
          1847,
          1848,
          1849,
          1850,
          1851,
          1852,
          1853,
          1854,
          1855,
          1856,
          1857,
          1858,
          1859,
          1860,
          1861,
          1862,
          1863,
          1864,
          1865,
          1866,
          1867,
          1868,
          1869,
          1870,
          1871,
          1872,
          1873,
          1874,
          1875,
          1876,
          1877,
          1878,
          1879,
          1880,
          1881,
          1882,
          1883,
          1884,
          1885,
          1886,
          1887,
          1888,
          1889,
          1890,
          1891,
          1892,
          1893,
          1894,
          1895,
          1896,
          1897,
          1898,
          1899,
          1900,
          1901,
          1902,
          1903,
          1904,
          1905,
          1906,
          1907,
          1908,
          1909,
          1910,
          1911,
          1912,
          1913,
          1914,
          1915,
          1916,
          1917,
          1918,
          1919,
          1920,
          1921,
          1922,
          1923,
          1924,
          1925,
          1926,
          1927,
          1928,
          1929,
          1930,
          1931,
          1932,
          1933,
          1934,
          1935,
          1936,
          1937,
          1938,
          1939,
          1940,
          1941,
          1942,
          1943,
          1944,
          1945,
          1946,
          1947,
          1948,
          1949,
          1950,
          1951,
          1952,
          1953,
          1954,
          1955,
          1956,
          1957,
          1958,
          1959,
          1960,
          1961,
          1962,
          1963,
          1964,
          1965,
          1966,
          1967,
          1968,
          1969,
          1970,
          1971,
          1972,
          1973,
          1974,
          1975,
          1976,
          1977,
          1978,
          1979,
          1980,
          1981,
          1982,
          1983,
          1984,
          1985,
          1986,
          1987,
          1988,
          1989,
          1990,
          1991,
          1992,
          1993,
          1994,
          1995,
          1996,
          1997,
          1998,
          1999,
          2000,
          2001,
          2002,
          2003,
          2004,
          2005,
          2006,
          2007,
          2008,
          2009,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024,
          2025,
          2026,
          2027,
          2028,
          2029,
          2030,
          2031,
          2032,
          2033,
          2034,
          2035,
          2036,
          2037,
          2038,
          2039,
          2040,
          2041,
          2042,
          2043,
          2044,
          2045,
          2046,
          2047
         ],
         "y": [
          0.23904127,
          -0.003937669,
          -0.07113647,
          0.04050564,
          0.010270197,
          -0.003932534,
          -0.025408715,
          0.002437733,
          0.012589611,
          -0.010243667,
          -0.009006806,
          -0.003095992,
          0.01755943,
          -0.055059463,
          -0.0033099442,
          -0.03481122,
          -0.07139016,
          -0.00022098667,
          -0.013858018,
          -0.010015165,
          -0.000712052,
          -0.035798736,
          0.0074862307,
          -0.06361291,
          0.019286208,
          0.00089163845,
          -0.012939345,
          -0.03485505,
          0.010045586,
          0.005335049,
          -0.06259747,
          -0.061692525,
          -0.011937551,
          -0.012056638,
          -0.03488012,
          -0.033731297,
          0.007312397,
          -0.0018927202,
          -0.07386497,
          -0.0058184722,
          0.011821035,
          -0.014477804,
          0.005239407,
          0.65670776,
          -0.018699097,
          0.004467258,
          -0.006854592,
          -0.0026648948,
          -0.013967328,
          -0.021091696,
          0.025827672,
          -0.079351395,
          -0.000119778095,
          -0.0060952264,
          -0.0075097904,
          0.0055400576,
          0.026182622,
          0.0071275067,
          0.0004910561,
          -0.0056141056,
          0.12020172,
          0.27825254,
          -0.05952994,
          0.008613275,
          -0.0035854578,
          -0.027967058,
          -0.14894104,
          0.014319772,
          -0.035394974,
          0.003248897,
          0.004342679,
          -0.01189585,
          -0.007851381,
          0.0006958917,
          -0.0062983446,
          0.003140755,
          -0.0012248566,
          -0.043597884,
          -0.07167922,
          0.00063454127,
          -0.078926876,
          0.024106387,
          0.00091997697,
          -0.039232187,
          0.009876251,
          -0.0024695396,
          -0.00024036807,
          -0.014337456,
          -0.08458595,
          0.013078555,
          -0.013429912,
          -0.00012680888,
          -0.012480475,
          0.025027838,
          -0.0019689193,
          -0.0129859485,
          -0.0021778783,
          0.011610797,
          -0.0071236053,
          -0.012713172,
          -0.024468858,
          0.0002979834,
          -0.01976977,
          -0.02778571,
          -0.09968795,
          0.006551999,
          -0.17197736,
          0.04094197,
          0.018853363,
          -0.0021358253,
          -0.18779436,
          -0.002905551,
          -0.0070919394,
          -0.039176516,
          -0.026976844,
          0.008993881,
          0.018838992,
          0.0034034103,
          0.021112999,
          -0.010249776,
          0.00673216,
          -0.025188379,
          -0.038057216,
          -0.10653566,
          0.055954285,
          -0.0021093013,
          0.05655742,
          -0.03689682,
          -0.0060156025,
          -0.008828246,
          0.0008372739,
          0.0054672877,
          -0.047729813,
          -0.0109349415,
          -0.0055136317,
          -0.002175503,
          -0.0026994976,
          -0.009169058,
          -0.009106744,
          -0.0012368942,
          -0.024686538,
          -0.0011247492,
          0.108548746,
          0.0028656144,
          0.0016485192,
          -0.0010110755,
          0.020975351,
          -0.026675068,
          0.0019332953,
          0.017909026,
          -0.019843161,
          -0.0037874915,
          -0.005786502,
          -0.1799227,
          -0.022044169,
          0.021064293,
          -0.0020980127,
          -0.026092038,
          -0.058033656,
          -0.0011956207,
          -0.06127025,
          -0.018550597,
          -0.0021837046,
          0.13997534,
          0.005697403,
          -0.017170753,
          -0.025150914,
          -0.03912647,
          -0.0027580783,
          -0.10243114,
          0.03975276,
          0.092016146,
          -0.00093407556,
          0.0010046464,
          0.0009049482,
          -0.0017926237,
          0.0665181,
          -0.00069872895,
          0.007797288,
          -0.004273843,
          -0.007124638,
          0.0028003259,
          -0.0020287433,
          -0.0005735196,
          0.10626964,
          -0.021440381,
          -0.0002750894,
          -0.01919305,
          0.0014840495,
          -0.000075216405,
          0.0026727612,
          -0.022040825,
          -0.0009210184,
          -0.019622091,
          0.0027598133,
          -0.063941985,
          0.0073038777,
          0.00019845972,
          0.0001892503,
          -0.0001921351,
          0.050981652,
          -0.121995404,
          0.15240814,
          -0.01066497,
          -0.06958221,
          -0.037547052,
          0.08480026,
          0.0011115931,
          -0.041328084,
          0.0021935091,
          0.011146872,
          0.079444796,
          -0.013995378,
          -0.044258945,
          -0.044427823,
          -0.0073818825,
          -0.04880825,
          0.009955596,
          -0.0014281499,
          0.020595152,
          -0.011210419,
          0.0032263482,
          0.14916722,
          0.0014029406,
          0.043436866,
          -0.002191464,
          -0.03224488,
          0.008234318,
          -0.035157844,
          -0.0065499777,
          0.0037116092,
          -0.00526217,
          -0.0070982,
          0.0077520628,
          -0.006005871,
          -0.0025211787,
          -0.08576974,
          -0.012857601,
          -0.01691878,
          -0.008417804,
          0.1839446,
          0.0025293417,
          -0.052519627,
          -0.012420067,
          0.043808058,
          -0.037224818,
          -0.026419122,
          -0.066118054,
          -0.013237599,
          0.2145746,
          -0.07270325,
          -0.0885146,
          -0.001636669,
          -0.004101995,
          -0.048998963,
          -0.0009835148,
          -0.016627528,
          0.029827336,
          -0.0010704983,
          0.022076868,
          0.04758252,
          -0.012929596,
          0.038005054,
          -0.010485647,
          0.0031597712,
          -0.038731046,
          0.071196884,
          0.13276666,
          0.50692874,
          -0.017105887,
          -0.0049804337,
          0.001162732,
          -0.013629196,
          -0.03489896,
          -0.10301554,
          0.009096131,
          -0.00005429797,
          0.0058505535,
          0.006016968,
          -0.13223757,
          -0.0016150102,
          0.0015905899,
          0.0026731328,
          -0.0011128122,
          0.01971058,
          0.10174497,
          -0.0011893474,
          -0.04442169,
          -0.0035948325,
          -0.020086236,
          0.40211442,
          -0.063382596,
          0.0009769211,
          0.009893268,
          0.03434809,
          -0.0726884,
          -0.036410213,
          -0.0030457675,
          0.029227143,
          0.2855043,
          -0.0036840746,
          -0.014353946,
          0.0017862767,
          -0.026768308,
          -0.026076661,
          0.068409935,
          -0.27843732,
          -0.0016251802,
          -0.0016412191,
          -0.06405472,
          -0.0061527146,
          -0.03675636,
          -0.07710271,
          -0.045852028,
          -0.015938219,
          -0.0045024604,
          0.010873146,
          -0.13236201,
          0.041821346,
          -0.025233455,
          0.0007601548,
          -0.054956388,
          0.00043444382,
          0.009964401,
          0.02054568,
          -0.047109973,
          0.00093189196,
          0.002779834,
          -0.018580092,
          0.07030518,
          -0.037358932,
          -0.046258558,
          -0.002968621,
          -0.07192994,
          -0.0055641066,
          -0.0017460827,
          0.00031161588,
          0.0023286082,
          0.0003389772,
          0.023555785,
          0.011549978,
          0.008239575,
          0.028601851,
          0.06733801,
          -0.004700176,
          -0.0506642,
          0.05780086,
          0.016580034,
          -0.03339179,
          -0.0015236395,
          -0.008841975,
          0.007920088,
          -0.030454352,
          0.08728382,
          -0.017395342,
          -0.014852464,
          0.09622346,
          0.12369601,
          0.001237018,
          -0.0013207453,
          -0.021164186,
          -0.06274975,
          0.018587517,
          0.066359445,
          -0.0066751717,
          0.0015743091,
          0.0028517414,
          -0.0018015709,
          0.02361587,
          0.0050265826,
          -0.0049892943,
          -0.0009242154,
          -0.0036775097,
          0.00037630182,
          -0.00010102452,
          -0.021017293,
          -0.00562191,
          -0.004126099,
          -0.023110665,
          0.005298648,
          -0.07496888,
          -0.025210364,
          0.028961284,
          -0.0933475,
          -0.0010986333,
          0.004960024,
          -0.0031070462,
          -0.008422499,
          0.09285017,
          0.0014578425,
          -0.04848363,
          0.012510498,
          -0.04416409,
          -0.011272688,
          -0.01382434,
          0.0018885084,
          -0.0074659362,
          -0.011084378,
          0.0039478936,
          -0.0095017105,
          0.000052906573,
          -0.004383661,
          -0.0000804693,
          0.0052717645,
          0.0032984852,
          -0.0010004975,
          -0.071945444,
          -0.000846602,
          0.001925095,
          0.180902,
          -0.025476329,
          0.0049931454,
          0.10955427,
          0.0028048456,
          -0.07354983,
          -0.08963188,
          -0.106678426,
          0.07728663,
          -0.0038676644,
          -0.018476829,
          -0.014411118,
          -0.08285737,
          0.008086693,
          -0.04513064,
          0.076344274,
          0.073274806,
          0.0026996965,
          0.0048009227,
          -0.0422294,
          0.0024587247,
          -0.055074193,
          0.11836333,
          -0.010097805,
          -0.00032602903,
          -0.046474554,
          -0.006145861,
          0.11355259,
          0.0006850746,
          0.0054432494,
          -0.00046007335,
          0.004934486,
          -0.0051364056,
          -0.012226637,
          -0.008092551,
          0.0030657505,
          -0.001465545,
          0.08295776,
          -0.034001775,
          0.034082778,
          0.00076491386,
          0.0022192847,
          0.0010440925,
          -0.001231127,
          -0.003041952,
          -0.020852134,
          -0.023155544,
          0.010047572,
          -0.004812776,
          0.0031718612,
          -0.01119263,
          -0.0017984209,
          -0.00305146,
          0.014395498,
          0.06038912,
          -0.017604416,
          0.0013950281,
          -0.013648085,
          -0.011855017,
          -0.042269062,
          -0.011482323,
          -0.04710447,
          0.021775883,
          0.054097254,
          -0.07992315,
          -0.007734294,
          -0.0037337197,
          0.022760797,
          0.009018743,
          -0.000104003586,
          -0.00066024903,
          -0.0012237502,
          0.0029755756,
          -0.05858305,
          0.0020364616,
          0.044161912,
          0.020484157,
          0.0138760675,
          -0.02651864,
          0.0043737777,
          -0.0005230759,
          -0.045424465,
          -0.0008411594,
          0.001034975,
          0.0043321606,
          -0.008491996,
          0.002725936,
          -0.05924472,
          -0.0046001934,
          0.0036160543,
          -0.009245601,
          0.031102054,
          -0.048945006,
          -0.08291258,
          -0.005408304,
          0.0035397352,
          0.16547309,
          0.00090489164,
          0.022379762,
          0.0019782512,
          0.0061144046,
          -0.011459628,
          -0.004443574,
          -0.0806901,
          0.13444676,
          -0.0036353567,
          -0.034937046,
          -0.0006671436,
          0.07706271,
          0.0008225478,
          -0.008665363,
          -0.010035424,
          -0.09551088,
          -0.023927245,
          0.011561951,
          -0.0014855969,
          -0.09918233,
          0.011570811,
          0.00033934414,
          0.049152598,
          -0.01044862,
          0.004481013,
          -0.023824295,
          -0.02818872,
          -0.0047282004,
          -0.021454824,
          -0.029231768,
          0.07333186,
          -0.005436535,
          -0.00048709597,
          0.0010090619,
          -0.049782347,
          0.035034433,
          -0.012411509,
          -0.0020652856,
          -0.02727427,
          -0.022541005,
          -0.020395366,
          -0.04082775,
          -0.067762084,
          -0.00479854,
          0.0021645583,
          -0.0058438154,
          -0.06687948,
          0.014508437,
          -0.0033609774,
          0.070395544,
          -0.014940421,
          -0.0017542569,
          -0.008570514,
          -0.082679555,
          -0.14070885,
          0.0012934823,
          -0.05132951,
          -0.018003576,
          -0.072742514,
          -0.051187247,
          -0.021805072,
          -0.0031601675,
          0.00059032254,
          -0.020113338,
          -0.037131015,
          -0.009906862,
          -0.018802445,
          0.001921989,
          0.0015395153,
          -0.03361398,
          0.06289886,
          -0.016377043,
          -0.038527057,
          -0.013836378,
          0.006523282,
          -0.042849,
          -0.036495782,
          -0.0014477968,
          0.007699089,
          -0.013305968,
          -0.10268713,
          -0.009814715,
          -0.026848227,
          0.0026279166,
          -0.023056794,
          0.009676421,
          -0.015777413,
          -0.10903461,
          -0.008836567,
          -0.03475918,
          0.48744747,
          -0.016654205,
          -0.0073726065,
          -0.01743516,
          -0.011580873,
          0.025862433,
          -0.00390861,
          -0.10851527,
          -0.0044369083,
          -0.05535277,
          0.010143416,
          0.009221138,
          0.0010300325,
          0.009392612,
          -0.00072278595,
          -0.052627273,
          0.0007906845,
          -0.014015846,
          -0.00635314,
          -0.011966455,
          -0.01249269,
          0.0021890444,
          -0.087713204,
          -0.022013899,
          -0.015239382,
          0.0036213952,
          0.0012013861,
          -0.0759575,
          -0.037602205,
          0.003665837,
          0.0017871759,
          -0.019145597,
          -0.0031383857,
          -0.030571423,
          -0.032579947,
          -0.0010558343,
          -0.008966679,
          -0.0058635697,
          0.0016198071,
          -0.0438062,
          -0.0036946698,
          -0.09233479,
          -0.0030544726,
          -0.018674495,
          0.016122852,
          0.0032039285,
          -0.0038533867,
          0.00048413768,
          0.0036934153,
          -0.023837872,
          -0.062372558,
          -0.009128682,
          -0.03150884,
          0.04430243,
          0.04019553,
          -0.02471228,
          -0.040907018,
          -0.003581956,
          0.022950038,
          -0.004343828,
          -0.010623807,
          -0.06857255,
          -0.08230611,
          -0.053893644,
          -0.03611149,
          0.26842833,
          -0.0929311,
          -0.008674667,
          -0.021451823,
          -0.030511431,
          0.0051237815,
          -0.0038684243,
          -0.07004326,
          -0.004757558,
          0.018781293,
          -0.028879948,
          -0.04306148,
          0.13429976,
          0.0029643134,
          0.0046088663,
          -0.0045767426,
          -0.00016032206,
          0.0115192775,
          -0.007694248,
          -0.02811183,
          0.001965059,
          -0.06724636,
          0.0043929694,
          0.020827029,
          0.006035776,
          -0.056862205,
          -0.007974691,
          0.0037445128,
          -0.058175012,
          -0.07935793,
          0.005549937,
          -0.04051403,
          -0.005949654,
          -0.05948105,
          0.11942497,
          -0.051619202,
          0.047635585,
          0.021431442,
          0.0402825,
          -0.014023668,
          -0.038800348,
          -0.0038695615,
          0.017144162,
          -0.023850866,
          -0.10338829,
          -0.027510293,
          -0.0003024186,
          0.01766245,
          0.018095471,
          -0.0073639816,
          0.033816427,
          -0.0057968227,
          -0.009804075,
          -0.019220924,
          -0.04583102,
          -0.054841198,
          0.1700336,
          0.10274821,
          -0.04760516,
          0.024815809,
          -0.0010578106,
          0.0038881658,
          -0.004323506,
          0.0098344665,
          -0.103791595,
          -0.00296087,
          -0.0011632028,
          -0.037530027,
          0.005935193,
          0.0032525472,
          -0.039874718,
          -0.012842914,
          -0.028834626,
          -0.007828973,
          -0.022626081,
          -0.00084286556,
          0.009419598,
          0.010642797,
          -0.035917938,
          0.0072951238,
          0.013336023,
          -0.0003717239,
          0.03232602,
          -0.0771094,
          0.03156668,
          -0.005251085,
          0.00046850368,
          -0.004521141,
          0.010418617,
          -0.03596287,
          -0.092094645,
          -0.015065681,
          -0.051370766,
          -0.0903935,
          -0.020683823,
          -0.039599754,
          -0.020698823,
          -0.00027999096,
          0.010519601,
          -0.05950546,
          -0.012188015,
          0.6779035,
          -0.0188812,
          0.011538379,
          -0.034625776,
          0.5084427,
          0.0012602732,
          -0.0019600913,
          0.000694206,
          -0.007448027,
          0.023982076,
          0.0029721744,
          -0.02467854,
          -0.0015133307,
          -0.036841955,
          -0.0045860372,
          -0.008280851,
          -0.005944173,
          -0.015288033,
          -0.0027668583,
          -0.000043155625,
          -0.027708508,
          -0.008059275,
          -0.0032955091,
          -0.0632921,
          0.001118388,
          -0.051491983,
          -0.048706613,
          -0.0023541364,
          0.00007021241,
          0.001882365,
          0.00759295,
          0.22449924,
          -0.005209055,
          -0.030142844,
          -0.02771168,
          -0.38225812,
          -0.065233424,
          -0.03734303,
          -0.0073604872,
          0.013378069,
          0.038246173,
          -0.010639152,
          0.04769218,
          -0.0012208885,
          0.0064096972,
          -0.03528302,
          0.017525174,
          0.04289034,
          -0.03829342,
          -0.0056072166,
          -0.00651963,
          -0.005914239,
          -0.008171711,
          -0.031202223,
          -0.013988409,
          -0.028906088,
          -0.052549187,
          -0.17142773,
          0.015632609,
          -0.0033119356,
          0.0036157148,
          -0.0053728,
          -0.0009271573,
          -0.020442877,
          -0.13749923,
          0.008746945,
          -0.044177175,
          -0.0046969303,
          -0.028922252,
          0.048573475,
          0.0012869583,
          -0.067915685,
          0.06204638,
          -0.01757286,
          0.0020001533,
          -0.04506211,
          -0.027944492,
          -0.011662163,
          0.02323699,
          0.0031394735,
          0.054091036,
          0.2501672,
          -0.0044905907,
          -0.0071591064,
          -0.050160892,
          -0.005702734,
          0.0016365154,
          -0.003029964,
          0.003205791,
          -0.000083016,
          0.04011355,
          0.0039331703,
          -0.016398147,
          0.05506753,
          -0.00013684761,
          0.017053079,
          -0.006252315,
          0.5275786,
          -0.0013564449,
          -0.02476209,
          -0.03337674,
          -0.023323316,
          -0.06393881,
          -0.013471272,
          0.0041035195,
          0.00031280704,
          -0.06316817,
          0.01233888,
          -0.0028485917,
          -0.018502899,
          0.0021732561,
          -0.0070594065,
          -0.03465216,
          0.006344014,
          0.0046205074,
          -0.00080824795,
          -0.0018200763,
          -0.0060068746,
          0.0063086674,
          -0.01608073,
          0.0035920774,
          -0.034045525,
          -0.016559286,
          -0.0812421,
          -0.0074518956,
          0.0030561395,
          -0.0007189908,
          0.016030326,
          -0.05051787,
          0.001061561,
          -0.033397846,
          -0.01840375,
          -0.05404754,
          0.45847452,
          -0.00034904247,
          -0.002723962,
          0.024521811,
          -0.0041303416,
          -0.04118319,
          -0.012984276,
          -0.038722426,
          -0.059801605,
          -0.08322747,
          -0.01038833,
          -0.01355562,
          0.00077258144,
          -0.00023995386,
          -0.01384123,
          0.013976309,
          -0.014271151,
          -0.044900004,
          -0.00037420727,
          -0.006774537,
          -0.00042711524,
          0.0038952164,
          -0.006198857,
          -0.12872502,
          -0.015521816,
          -0.0034078425,
          -0.040188517,
          -0.006130824,
          -0.023476567,
          -0.05541756,
          -0.03548879,
          -0.06371491,
          -0.001602296,
          0.030939382,
          -0.004250314,
          -0.0067314506,
          -0.07068165,
          0.09329884,
          -0.0127783585,
          0.0014371164,
          0.0066899136,
          -0.05905872,
          0.076423705,
          -0.00080195395,
          -0.097538486,
          -0.007403313,
          -0.054188304,
          -0.007436093,
          0.0142815,
          -0.0029234265,
          -0.044603463,
          -0.056101173,
          -0.020017985,
          -0.0013425313,
          -0.015343289,
          0.012277242,
          -0.034329675,
          0.0007031858,
          -0.006920312,
          0.0009366735,
          0.0044701584,
          -0.0011346936,
          0.035182178,
          -0.001956678,
          -0.030222833,
          -0.06688546,
          0.021410225,
          -0.0007210076,
          -0.03723825,
          -0.013941405,
          0.021974396,
          -0.058288753,
          -0.004462734,
          0.00048580393,
          -0.0040406627,
          0.0021705509,
          0.10551343,
          -0.0013855612,
          -0.06441579,
          -0.0041457335,
          -0.0025419127,
          0.019314874,
          -0.0024374295,
          0.0019787233,
          0.008308652,
          -0.0014144661,
          0.0043893624,
          -0.009888481,
          -0.024833584,
          0.05759319,
          0.003338322,
          -0.0024422,
          -0.03127921,
          -0.003485648,
          0.004804641,
          -0.006738631,
          -0.011703676,
          0.027771061,
          -0.004785182,
          0.01954517,
          -0.0010536298,
          0.017454345,
          -0.004745725,
          -0.05247178,
          0.002689695,
          -0.0022316615,
          -0.038207367,
          0.061491393,
          -0.0052034683,
          -0.044900592,
          0.022208005,
          -0.004025214,
          0.073290735,
          -0.015072131,
          0.119681194,
          0.009085692,
          -0.052519344,
          -0.0068303198,
          0.0020829448,
          -0.057320967,
          -0.00139899,
          0.0042362455,
          -0.020913716,
          0.02031351,
          0.01708683,
          -0.05663482,
          -0.10741479,
          -0.025351629,
          -0.03483964,
          -0.08122732,
          -0.050691646,
          -0.019116934,
          -0.029170133,
          -0.10358915,
          -0.091970466,
          -0.008115377,
          -0.007027641,
          -0.013165688,
          -0.015328787,
          -0.0076346463,
          -0.059746224,
          -0.0050985385,
          -0.1553978,
          0.0015318431,
          -0.01609525,
          -0.0051463055,
          -0.006916393,
          0.025775982,
          -0.021359567,
          -0.008392595,
          0.0023412812,
          -0.011147874,
          -0.045729212,
          -0.0050488645,
          -0.007826149,
          -0.0027406812,
          -0.01820876,
          -0.002976587,
          -0.00890508,
          -0.062031783,
          -0.016039018,
          -0.003236888,
          0.0022747675,
          -0.0038452367,
          0.0033387877,
          0.013443695,
          -0.022954732,
          -0.049369693,
          0.038272366,
          -0.00086005405,
          -0.008741131,
          0.0018961208,
          0.0025889017,
          -0.04475794,
          0.038872086,
          0.03720503,
          -0.000036135316,
          -0.0055767447,
          -0.03823573,
          -0.0020892061,
          -0.015614744,
          0.00053523853,
          0.0024129413,
          -0.039277475,
          -0.03293582,
          0.017606188,
          -0.02270294,
          0.001054418,
          -0.014233876,
          -0.022036817,
          -0.0077031273,
          0.0005872641,
          -0.04464055,
          0.005836364,
          -0.0047503803,
          -0.0344772,
          -0.0071088113,
          -0.063029885,
          -0.011000418,
          -0.12373678,
          -0.028457977,
          -0.007356122,
          -0.0031478887,
          -0.014488956,
          -0.031807415,
          -0.009197675,
          0.01887171,
          0.0024531556,
          -0.06959434,
          -0.00237275,
          -0.026667133,
          -0.04799751,
          0.00088426704,
          0.006486114,
          0.004470151,
          -0.0007119761,
          0.021654438,
          0.0018881806,
          0.027891077,
          0.0035148878,
          -0.039206304,
          -0.005763745,
          -0.0038207835,
          -0.023612212,
          0.08420865,
          0.0024508624,
          -0.045526136,
          0.00642242,
          0.008330796,
          -0.0042311437,
          0.20917492,
          0.015576302,
          -0.0036146156,
          0.029364977,
          -0.036030307,
          -0.00006261538,
          -0.17708696,
          0.0012356052,
          0.0053470284,
          -0.034339216,
          -0.015730303,
          -0.009380351,
          -0.08267513,
          -0.037017412,
          -0.033069454,
          -0.077080116,
          -0.036053963,
          -0.0038969202,
          -0.011444547,
          0.09029046,
          0.0027710623,
          0.2464513,
          0.020284653,
          0.0008614375,
          -0.009624121,
          -0.023464764,
          -0.0035588183,
          -0.05402453,
          -0.025353143,
          -0.00041358848,
          -0.033059873,
          -0.07219252,
          -0.0063911034,
          -0.03160529,
          0.009539319,
          -0.01847157,
          -0.014910474,
          -0.0011682278,
          -0.007786859,
          0.09344057,
          -0.049789123,
          0.012290967,
          -0.079609096,
          0.04546015,
          -0.14438547,
          -0.041067444,
          0.20614207,
          0.0141771715,
          -0.03001238,
          0.0025738515,
          0.015016522,
          -0.03783466,
          -0.0004891525,
          -0.0059746634,
          -0.06470113,
          -0.00015190756,
          0.08613673,
          0.001231648,
          0.013946645,
          0.000831784,
          0.010583244,
          -0.0024283594,
          -0.10144673,
          -0.10804151,
          -0.047775324,
          0.0013518948,
          0.04077516,
          -0.00271477,
          -0.009679118,
          -0.011315497,
          0.38479736,
          -0.0015218798,
          -0.074162036,
          -0.021833453,
          -0.06802209,
          -0.034108244,
          0.19955274,
          0.013624525,
          0.018507779,
          -0.030444842,
          0.0011060331,
          -0.019509025,
          0.0003394303,
          -0.06767132,
          0.0019492488,
          0.013290744,
          0.00070584565,
          -0.029904835,
          0.00052629644,
          -0.012673033,
          0.0018544829,
          0.049878128,
          0.006785805,
          0.11623536,
          -0.038326524,
          -0.0029989704,
          -0.0040904125,
          -0.0946604,
          -0.011912134,
          0.13377413,
          -0.011091063,
          -0.07605472,
          0.0031089957,
          -0.00018788548,
          -0.027242705,
          -0.10253242,
          -0.049726482,
          -0.020230971,
          -0.001766853,
          -0.0096951425,
          -0.061868563,
          0.004824862,
          -0.05983634,
          -0.004760555,
          0.0027169343,
          0.0008636224,
          -0.03197228,
          0.013556898,
          -0.06759749,
          -0.00140894,
          -0.05031157,
          0.035246156,
          -0.0008996548,
          -0.02615681,
          0.011824263,
          0.0008543475,
          -0.06664632,
          0.007165026,
          0.002419345,
          0.03325538,
          0.0026117843,
          0.008712221,
          0.012722142,
          -0.054391477,
          -0.0018080724,
          -0.017193373,
          0.0064822724,
          -0.012676887,
          0.028446287,
          0.021115765,
          0.0081479885,
          0.003547328,
          0.046946105,
          0.017003298,
          -0.25701013,
          -0.036803927,
          -0.020206839,
          -0.03909393,
          -0.0033063404,
          -0.057399668,
          -0.016984498,
          -0.06300871,
          -0.02766459,
          -0.047452126,
          -0.09473395,
          -0.020100564,
          -0.0366866,
          -0.0067002615,
          -0.019941228,
          -0.11410207,
          -0.009415597,
          -0.009507202,
          -0.0389062,
          0.0025686864,
          -0.0025780573,
          0.06064443,
          -0.07769068,
          0.2453089,
          -0.0020995056,
          0.0022326373,
          0.01136096,
          -0.013695501,
          0.005480864,
          0.006300647,
          -0.17606315,
          0.013218721,
          0.06179379,
          -0.013792209,
          -0.013517854,
          -0.0008066227,
          0.0005983766,
          0.412692,
          -0.06273978,
          -0.058779173,
          0.0049744104,
          -0.03258946,
          -0.001691875,
          -0.00014984049,
          -0.05357475,
          -0.00455569,
          -0.028976519,
          0.010654565,
          2.132732,
          -0.0041039437,
          -0.020673651,
          -0.052650295,
          -0.076647855,
          0.096765324,
          0.004584532,
          -0.0065647047,
          -0.026209146,
          0.034412637,
          -0.004798761,
          -0.0061787404,
          0.027376156,
          0.005230199,
          0.054110937,
          -0.0023630229,
          0.0022901697,
          -0.00409664,
          -0.00054647785,
          -0.011757971,
          0.048014186,
          -0.027763296,
          0.06872758,
          0.0056324946,
          -0.05524524,
          0.019333933,
          0.21899274,
          -0.0021692994,
          -0.009310855,
          -0.006429393,
          0.00034312997,
          0.0032851323,
          0.002614707,
          0.0010557175,
          -0.06987858,
          -0.015799817,
          -0.04715957,
          0.021018349,
          -0.07084873,
          -0.042235553,
          -0.028734982,
          0.02281607,
          -0.075186506,
          -0.010435876,
          -0.0052494537,
          0.007408198,
          -0.14735425,
          -0.04164415,
          0.16526087,
          0.022955358,
          -0.05657178,
          -0.00083347596,
          -0.04549827,
          0.00006783893,
          0.0067317216,
          -0.010784535,
          0.0020953268,
          -0.014696728,
          0.0031523374,
          -0.027387347,
          0.027322482,
          0.038170524,
          -0.018202167,
          0.054034915,
          -0.006685743,
          0.0071197953,
          0.005984515,
          0.022618534,
          -0.0011496877,
          0.00006807549,
          -0.08113194,
          -0.008490936,
          -0.011454584,
          -0.0010749935,
          -0.04574528,
          0.0013936812,
          -0.0032944926,
          0.023701316,
          -0.00266656,
          -0.06780577,
          -0.010041297,
          -0.0013976297,
          0.07509798,
          0.22297794,
          -0.076913945,
          -0.034884736,
          0.010769913,
          -0.00014859997,
          -0.007848686,
          0.0074403286,
          -0.011722349,
          0.060571786,
          0.0028390298,
          0.037487615,
          -0.007786149,
          -0.06905134,
          -0.033992954,
          -0.043767698,
          -0.026549784,
          -0.00735357,
          -0.006997848,
          0.1487856,
          0.018057333,
          0.013673173,
          0.013038819,
          0.09417884,
          -0.0020569125,
          -0.0034641032,
          -0.03508841,
          0.059874944,
          0.036899813,
          0.002626204,
          -0.025215898,
          -0.0021701595,
          -0.041725874,
          0.009160245,
          -0.016320828,
          0.0070088822,
          -0.023834433,
          -0.0055726245,
          0.01871818,
          -0.074386194,
          0.004425168,
          -0.010185491,
          -0.034128353,
          -0.0069048135,
          0.0045683263,
          0.009698535,
          -0.000031435862,
          -0.05260165,
          0.022292838,
          -0.041543253,
          0.025414512,
          -0.011904724,
          0.0038016066,
          0.0022024387,
          -0.0018429738,
          0.0014975462,
          -0.0457954,
          -0.005097121,
          -0.013818309,
          0.067637004,
          0.0234048,
          0.00032004341,
          0.07890962,
          -0.000290579,
          -0.0026291278,
          0.056663178,
          -0.038352676,
          0.21577555,
          0.052523963,
          0.0006862879,
          -0.026884481,
          -0.0016028557,
          0.031420268,
          0.004164759,
          -0.01213768,
          -0.12249741,
          -0.027331587,
          0.001706264,
          -0.04856256,
          0.0024818794,
          -0.002220058,
          -0.0073597315,
          -0.07279181,
          -0.0043243533,
          0.04031106,
          -0.012333274,
          0.010705635,
          -0.17356709,
          -0.0019256063,
          -0.06134767,
          -0.005071583,
          -0.0041616117,
          -0.0032903648,
          0.08743015,
          0.001756859,
          -0.083704054,
          -0.0005665561,
          -0.030911341,
          0.0054847794,
          -0.08565679,
          -0.047504716,
          -0.003925018,
          0.16713673,
          -0.016365163,
          -0.06341094,
          -0.07555765,
          -0.007812757,
          0.006949246,
          -0.0037174108,
          -0.021676471,
          -0.047065318,
          0.0006777467,
          0.008575693,
          -0.019800764,
          -0.01481983,
          -0.039166763,
          0.085405156,
          -0.028467923,
          -0.007613674,
          -0.1371263,
          -0.021011464,
          -0.008903392,
          -0.012868412,
          0.000674514,
          -0.13887013,
          -0.00045442767,
          0.16421805,
          0.004850015,
          0.011118278,
          0.0023327444,
          -0.018865073,
          0.0026618517,
          -0.092282176,
          0.0071230307,
          -0.050254695,
          -0.07039791,
          -0.0009794263,
          0.031913847,
          -0.008710086,
          -0.023074202,
          -0.073036164,
          -0.0015852621,
          0.0030150115,
          -0.02123672,
          -0.012443751,
          -0.0054588383,
          0.008431368,
          0.00042581,
          0.0029781498,
          0.0031214785,
          -0.00029656058,
          0.37316042,
          0.008862247,
          -0.011342594,
          -0.056309443,
          -0.010723364,
          -0.01663526,
          -0.0050454945,
          0.013082512,
          -0.0042112954,
          -0.044673957,
          -0.04622397,
          -0.013601842,
          -0.0012384407,
          -0.071443856,
          -0.0010088626,
          0.008961217,
          -0.08393322,
          0.020292379,
          0.059440862,
          -0.028370183,
          0.0014997923,
          -0.035078175,
          0.0005380162,
          -0.076280095,
          -0.03025259,
          -0.04528869,
          0.01391259,
          -0.011320807,
          0.011650458,
          0.060566746,
          0.119442165,
          0.02302336,
          -0.0019393237,
          -0.006663813,
          0.0024416274,
          -0.04016833,
          0.033216022,
          -0.035065234,
          -0.010918785,
          -0.016547203,
          -0.01683118,
          -0.066740654,
          0.01932184,
          0.00016315933,
          -0.021125525,
          -0.010761686,
          0.005982479,
          -0.0008722283,
          -0.0316947,
          -0.0023325088,
          -0.015290603,
          0.11398047,
          -0.03507691,
          0.011151323,
          -0.0007060545,
          -0.0010339292,
          0.028002463,
          -0.0067466768,
          -0.018322697,
          -0.001628275,
          -0.0023343277,
          -0.002306813,
          -0.013526619,
          0.07065824,
          -0.024245031,
          0.003915339,
          -0.03487345,
          0.0022011613,
          0.01900544,
          -0.0044929106,
          0.023877885,
          -0.028956916,
          0.013434723,
          -0.09266515,
          -0.07943118,
          -0.06235183,
          0.004497409,
          0.010398097,
          -0.0010826141,
          -0.04863093,
          0.28876036,
          -0.013532314,
          -0.026715219,
          -0.0057783043,
          -0.006619935,
          -0.083257206,
          0.025781108,
          -0.04929718,
          -0.046497315,
          -0.0042866617,
          0.00016842224,
          -0.074612424,
          0.0064441315,
          -0.0051801195,
          0.0010622852,
          -0.01650218,
          -0.10068485,
          -0.09874011,
          -0.04157687,
          0.023584526,
          -0.01543008,
          -0.15975502,
          0.009103568,
          -0.0013185425,
          -0.006037149,
          -0.044823494,
          -0.092274696,
          -0.05007778,
          -0.004114012,
          0.007295411,
          -0.0024176368,
          0.08060623,
          -0.00020890264,
          0.10606294,
          0.0030351405,
          -0.014846098,
          0.0049278885,
          0.011074584,
          -0.008646145,
          -0.075939886,
          -0.0048097763,
          -0.06830412,
          0.05314022,
          -0.028753445,
          0.012817266,
          0.25810993,
          -0.096558444,
          -0.016188368,
          -0.017637618,
          0.084254414,
          -0.0059134914,
          -0.053907312,
          0.06941706,
          0.002299347,
          -0.084855475,
          -0.004074942,
          0.016667288,
          0.080900595,
          -0.039184064,
          -0.0040140664,
          -0.040946722,
          0.0058989404,
          0.07688538,
          -0.0017200636,
          0.015640965,
          0.011735887,
          -0.0009492333,
          -0.024323113,
          -0.0022016857,
          -0.008184007,
          -0.002000373,
          -0.0121694,
          -0.026498564,
          0.00060324883,
          -0.0075227576,
          -0.021033512,
          -0.0012142146,
          -0.0041815657,
          -0.05303681,
          0.006293043,
          -0.11674412,
          -0.0566476,
          -0.0015965556,
          0.010179867,
          -0.020867078,
          0.06614711,
          -0.026528329,
          -0.026625093,
          0.01580581,
          0.00348674,
          -0.0005730018,
          -0.0015957612,
          -0.022486135,
          -0.025319923,
          -0.045926604,
          0.04097446,
          -0.0022362676,
          -0.07543792,
          -0.002599238,
          0.0015314105,
          0.004109938,
          -0.038939007,
          -0.02262368,
          0.005133889,
          -0.09988272,
          -0.013392119,
          0.015237924,
          0.0038717892,
          -0.0015269443,
          -0.101874106,
          0.013909988,
          -0.04285482,
          0.036984686,
          0.0133988,
          0.031775624,
          -0.050555896,
          -0.010533099,
          0.005491702,
          -0.065383166,
          -0.025561567,
          0.0059748758,
          -0.02314431,
          -0.0041536726,
          0.0022176432,
          -0.0027600676,
          0.08940158,
          -0.043604463,
          -0.011777891,
          -0.00016720325,
          -0.0033527575,
          -0.0039328076,
          0.0020551458,
          0.003167946,
          0.0030203653,
          -0.026177436,
          -0.0010751671,
          0.19228989,
          0.00066520297,
          -0.006578963,
          0.00007059891,
          -0.056767445,
          0.00841127,
          0.00782527,
          0.0891615,
          -0.017871035,
          -0.03327214,
          -0.12159831,
          -0.0041367337,
          -0.0810146,
          -0.015525995,
          -0.06861713,
          -0.029864043,
          0.0023299754,
          0.0072216783,
          -0.03028158,
          -0.053960826,
          0.0009700749,
          0.009245223,
          0.03564737,
          0.011525886,
          0.009345425,
          -0.04394458,
          -0.02164498,
          -0.0024575186,
          -0.002554649,
          0.1246298,
          0.0006645913,
          -0.0024225377,
          -0.010801344,
          -0.0074530607,
          0.08660738,
          0.0028796978,
          0.0017528189,
          0.019311499,
          -0.017520696,
          -0.028882235,
          -0.01763634,
          -0.035863504,
          -0.04389747,
          -0.0006845568,
          0.13241965,
          0.001196105,
          0.42009187,
          -0.058855426,
          -0.0054749474,
          -0.108977236,
          0.019210132,
          0.009644914,
          -0.0145034045,
          0.22938089,
          0.014071144,
          0.0047893473,
          -0.0023329593,
          -0.050312415,
          0.027370594,
          -0.14008452,
          0.006562978,
          -0.0013750186,
          -0.04346501,
          -0.07965095,
          -0.0028849738,
          -0.001153131,
          -0.022429058,
          -0.006991219,
          -0.0114111565,
          -0.0030948035,
          -0.019839471,
          0.110696286,
          -0.10959334,
          -0.02558294,
          -0.036472313,
          0.004733486,
          -0.050815254,
          -0.00085860305,
          -0.03927321,
          -0.019842723,
          -0.018159779,
          -0.0053899647,
          0.07402281,
          0.0067304373,
          -0.013616817,
          -0.048148416,
          -0.0020392393,
          0.0029602405,
          -0.042427875,
          -0.007707143,
          -0.04038668,
          -0.027366057,
          0.07988384,
          -0.08025926,
          0.018612802,
          -0.046594888,
          0.001040942,
          -0.032303635,
          0.00055751903,
          -0.01068154,
          -0.025943883,
          -0.028287923,
          0.05500242,
          -0.042688962,
          0.010426125,
          -0.026565956,
          -0.09289156,
          -0.042299896,
          0.012787521,
          0.0013239235,
          -0.010894788,
          -0.028402913,
          -0.002902633,
          0.0005503325,
          0.16239165,
          -0.0030925963,
          0.0024453746,
          -0.07271515,
          -0.03252969,
          -0.0072650625,
          0.0044996687,
          0.043111667,
          -0.02975243,
          0.008533947,
          0.035981156,
          -0.033430494,
          0.068149894,
          0.9437437,
          -0.0041003707,
          -0.028958045,
          -0.0765689,
          0.09923218,
          -0.08535473,
          -0.023407135,
          -0.10705836,
          -0.0009682607,
          -0.007413468,
          -0.021585822,
          -0.024052229,
          0.0019800514,
          0.00084022526,
          -0.027515884,
          -0.06655913,
          -0.011071789,
          -0.012289029,
          -0.043763205,
          0.023749845,
          0.018048957,
          -0.006391904,
          -0.026531912,
          -0.002226354,
          0.009625755,
          0.032235637,
          -0.006846321,
          0.0021393485,
          -0.0013830881,
          -0.0225996,
          -0.00073458534,
          -0.016985588,
          -0.0030024722,
          0.13239905,
          -0.04218699,
          -0.011303432,
          0.003555093,
          0.027083557,
          0.001277009,
          -0.030622728,
          -0.0063988194,
          -0.0062113637,
          -0.071940124,
          -0.014172317,
          -0.0067238715,
          -0.043156415,
          0.0972536,
          -0.008567011,
          -0.00632542,
          -0.024780441,
          0.0067422367,
          -0.0002943538,
          -0.0113729555,
          0.0097677,
          0.0037718054,
          -0.019028928,
          0.0013503567,
          0.17428419,
          -0.028148651,
          1.2860682,
          -0.00446146,
          -0.012341071,
          0.036983192,
          0.0019778106,
          -0.015152533,
          -0.032206275,
          -0.0012948439,
          -0.0038879886,
          0.011020165,
          -0.0049578557,
          -0.0013320851,
          -0.033768892,
          -0.042850778,
          -0.016478322,
          -0.03584567,
          1.0041835,
          0.028514769,
          -0.0049299607,
          0.003480211,
          -0.014661178,
          -0.00038491585,
          -0.02014643,
          -0.019137137,
          -0.0133311525,
          0.014583334,
          -0.0021801337,
          -0.04101389,
          0.007246053,
          0.00048280344,
          0.00094698183,
          -0.05454751,
          -0.13544896,
          0.0046043973,
          -0.08338973,
          0.0024178196,
          0.030928109,
          -0.017127372,
          -0.02975034,
          -0.0052384883,
          -0.05124931,
          -0.0005124605,
          0.08878325,
          -0.021076228,
          -0.036873024,
          -0.0027722516,
          -0.0121959,
          -0.012479793,
          0.21006091,
          0.0056794356,
          -0.005460049,
          0.046483167,
          -0.0073887203,
          0.11636763,
          -0.02111939,
          0.001971329,
          0.0007204645,
          -0.0755321,
          0.06398285,
          -0.050901145,
          -0.00011840928,
          -0.0019120104,
          0.008207586,
          -0.0559702,
          0.33064905,
          -0.0009805709,
          0.00857535,
          0.00019138935,
          -0.035299063,
          0.0900287,
          -0.11296202,
          -0.051842257,
          -0.014684093,
          -0.0065314537,
          -0.008576487,
          -0.0032303713,
          0.024169233,
          -0.015114222,
          0.0778806,
          0.071866974,
          0.001483338,
          -0.10365624,
          -0.043729465,
          -0.007445045,
          0.0014756517,
          0.004575478,
          0.00929812,
          -0.021134531,
          0.07643907,
          -0.009644739,
          0.0209926,
          -0.012349164,
          -0.012770984,
          -0.033835817,
          0.00481924,
          -0.0047704913
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Rule 8, Layer 1"
        },
        "xaxis": {
         "title": {
          "text": "Neuron"
         }
        },
        "yaxis": {
         "title": {
          "text": "Neuron Acts Diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"23b09c02-b466-48cb-987c-68fc7ee7e1f9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"23b09c02-b466-48cb-987c-68fc7ee7e1f9\")) {                    Plotly.newPlot(                        \"23b09c02-b466-48cb-987c-68fc7ee7e1f9\",                        [{\"mode\":\"markers\",\"name\":\"Neuron Acts Diff\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047],\"y\":[0.23904127,-0.003937669,-0.07113647,0.04050564,0.010270197,-0.003932534,-0.025408715,0.002437733,0.012589611,-0.010243667,-0.009006806,-0.003095992,0.01755943,-0.055059463,-0.0033099442,-0.03481122,-0.07139016,-0.00022098667,-0.013858018,-0.010015165,-0.000712052,-0.035798736,0.0074862307,-0.06361291,0.019286208,0.00089163845,-0.012939345,-0.03485505,0.010045586,0.005335049,-0.06259747,-0.061692525,-0.011937551,-0.012056638,-0.03488012,-0.033731297,0.007312397,-0.0018927202,-0.07386497,-0.0058184722,0.011821035,-0.014477804,0.005239407,0.65670776,-0.018699097,0.004467258,-0.006854592,-0.0026648948,-0.013967328,-0.021091696,0.025827672,-0.079351395,-0.000119778095,-0.0060952264,-0.0075097904,0.0055400576,0.026182622,0.0071275067,0.0004910561,-0.0056141056,0.12020172,0.27825254,-0.05952994,0.008613275,-0.0035854578,-0.027967058,-0.14894104,0.014319772,-0.035394974,0.003248897,0.004342679,-0.01189585,-0.007851381,0.0006958917,-0.0062983446,0.003140755,-0.0012248566,-0.043597884,-0.07167922,0.00063454127,-0.078926876,0.024106387,0.00091997697,-0.039232187,0.009876251,-0.0024695396,-0.00024036807,-0.014337456,-0.08458595,0.013078555,-0.013429912,-0.00012680888,-0.012480475,0.025027838,-0.0019689193,-0.0129859485,-0.0021778783,0.011610797,-0.0071236053,-0.012713172,-0.024468858,0.0002979834,-0.01976977,-0.02778571,-0.09968795,0.006551999,-0.17197736,0.04094197,0.018853363,-0.0021358253,-0.18779436,-0.002905551,-0.0070919394,-0.039176516,-0.026976844,0.008993881,0.018838992,0.0034034103,0.021112999,-0.010249776,0.00673216,-0.025188379,-0.038057216,-0.10653566,0.055954285,-0.0021093013,0.05655742,-0.03689682,-0.0060156025,-0.008828246,0.0008372739,0.0054672877,-0.047729813,-0.0109349415,-0.0055136317,-0.002175503,-0.0026994976,-0.009169058,-0.009106744,-0.0012368942,-0.024686538,-0.0011247492,0.108548746,0.0028656144,0.0016485192,-0.0010110755,0.020975351,-0.026675068,0.0019332953,0.017909026,-0.019843161,-0.0037874915,-0.005786502,-0.1799227,-0.022044169,0.021064293,-0.0020980127,-0.026092038,-0.058033656,-0.0011956207,-0.06127025,-0.018550597,-0.0021837046,0.13997534,0.005697403,-0.017170753,-0.025150914,-0.03912647,-0.0027580783,-0.10243114,0.03975276,0.092016146,-0.00093407556,0.0010046464,0.0009049482,-0.0017926237,0.0665181,-0.00069872895,0.007797288,-0.004273843,-0.007124638,0.0028003259,-0.0020287433,-0.0005735196,0.10626964,-0.021440381,-0.0002750894,-0.01919305,0.0014840495,-0.000075216405,0.0026727612,-0.022040825,-0.0009210184,-0.019622091,0.0027598133,-0.063941985,0.0073038777,0.00019845972,0.0001892503,-0.0001921351,0.050981652,-0.121995404,0.15240814,-0.01066497,-0.06958221,-0.037547052,0.08480026,0.0011115931,-0.041328084,0.0021935091,0.011146872,0.079444796,-0.013995378,-0.044258945,-0.044427823,-0.0073818825,-0.04880825,0.009955596,-0.0014281499,0.020595152,-0.011210419,0.0032263482,0.14916722,0.0014029406,0.043436866,-0.002191464,-0.03224488,0.008234318,-0.035157844,-0.0065499777,0.0037116092,-0.00526217,-0.0070982,0.0077520628,-0.006005871,-0.0025211787,-0.08576974,-0.012857601,-0.01691878,-0.008417804,0.1839446,0.0025293417,-0.052519627,-0.012420067,0.043808058,-0.037224818,-0.026419122,-0.066118054,-0.013237599,0.2145746,-0.07270325,-0.0885146,-0.001636669,-0.004101995,-0.048998963,-0.0009835148,-0.016627528,0.029827336,-0.0010704983,0.022076868,0.04758252,-0.012929596,0.038005054,-0.010485647,0.0031597712,-0.038731046,0.071196884,0.13276666,0.50692874,-0.017105887,-0.0049804337,0.001162732,-0.013629196,-0.03489896,-0.10301554,0.009096131,-0.00005429797,0.0058505535,0.006016968,-0.13223757,-0.0016150102,0.0015905899,0.0026731328,-0.0011128122,0.01971058,0.10174497,-0.0011893474,-0.04442169,-0.0035948325,-0.020086236,0.40211442,-0.063382596,0.0009769211,0.009893268,0.03434809,-0.0726884,-0.036410213,-0.0030457675,0.029227143,0.2855043,-0.0036840746,-0.014353946,0.0017862767,-0.026768308,-0.026076661,0.068409935,-0.27843732,-0.0016251802,-0.0016412191,-0.06405472,-0.0061527146,-0.03675636,-0.07710271,-0.045852028,-0.015938219,-0.0045024604,0.010873146,-0.13236201,0.041821346,-0.025233455,0.0007601548,-0.054956388,0.00043444382,0.009964401,0.02054568,-0.047109973,0.00093189196,0.002779834,-0.018580092,0.07030518,-0.037358932,-0.046258558,-0.002968621,-0.07192994,-0.0055641066,-0.0017460827,0.00031161588,0.0023286082,0.0003389772,0.023555785,0.011549978,0.008239575,0.028601851,0.06733801,-0.004700176,-0.0506642,0.05780086,0.016580034,-0.03339179,-0.0015236395,-0.008841975,0.007920088,-0.030454352,0.08728382,-0.017395342,-0.014852464,0.09622346,0.12369601,0.001237018,-0.0013207453,-0.021164186,-0.06274975,0.018587517,0.066359445,-0.0066751717,0.0015743091,0.0028517414,-0.0018015709,0.02361587,0.0050265826,-0.0049892943,-0.0009242154,-0.0036775097,0.00037630182,-0.00010102452,-0.021017293,-0.00562191,-0.004126099,-0.023110665,0.005298648,-0.07496888,-0.025210364,0.028961284,-0.0933475,-0.0010986333,0.004960024,-0.0031070462,-0.008422499,0.09285017,0.0014578425,-0.04848363,0.012510498,-0.04416409,-0.011272688,-0.01382434,0.0018885084,-0.0074659362,-0.011084378,0.0039478936,-0.0095017105,0.000052906573,-0.004383661,-0.0000804693,0.0052717645,0.0032984852,-0.0010004975,-0.071945444,-0.000846602,0.001925095,0.180902,-0.025476329,0.0049931454,0.10955427,0.0028048456,-0.07354983,-0.08963188,-0.106678426,0.07728663,-0.0038676644,-0.018476829,-0.014411118,-0.08285737,0.008086693,-0.04513064,0.076344274,0.073274806,0.0026996965,0.0048009227,-0.0422294,0.0024587247,-0.055074193,0.11836333,-0.010097805,-0.00032602903,-0.046474554,-0.006145861,0.11355259,0.0006850746,0.0054432494,-0.00046007335,0.004934486,-0.0051364056,-0.012226637,-0.008092551,0.0030657505,-0.001465545,0.08295776,-0.034001775,0.034082778,0.00076491386,0.0022192847,0.0010440925,-0.001231127,-0.003041952,-0.020852134,-0.023155544,0.010047572,-0.004812776,0.0031718612,-0.01119263,-0.0017984209,-0.00305146,0.014395498,0.06038912,-0.017604416,0.0013950281,-0.013648085,-0.011855017,-0.042269062,-0.011482323,-0.04710447,0.021775883,0.054097254,-0.07992315,-0.007734294,-0.0037337197,0.022760797,0.009018743,-0.000104003586,-0.00066024903,-0.0012237502,0.0029755756,-0.05858305,0.0020364616,0.044161912,0.020484157,0.0138760675,-0.02651864,0.0043737777,-0.0005230759,-0.045424465,-0.0008411594,0.001034975,0.0043321606,-0.008491996,0.002725936,-0.05924472,-0.0046001934,0.0036160543,-0.009245601,0.031102054,-0.048945006,-0.08291258,-0.005408304,0.0035397352,0.16547309,0.00090489164,0.022379762,0.0019782512,0.0061144046,-0.011459628,-0.004443574,-0.0806901,0.13444676,-0.0036353567,-0.034937046,-0.0006671436,0.07706271,0.0008225478,-0.008665363,-0.010035424,-0.09551088,-0.023927245,0.011561951,-0.0014855969,-0.09918233,0.011570811,0.00033934414,0.049152598,-0.01044862,0.004481013,-0.023824295,-0.02818872,-0.0047282004,-0.021454824,-0.029231768,0.07333186,-0.005436535,-0.00048709597,0.0010090619,-0.049782347,0.035034433,-0.012411509,-0.0020652856,-0.02727427,-0.022541005,-0.020395366,-0.04082775,-0.067762084,-0.00479854,0.0021645583,-0.0058438154,-0.06687948,0.014508437,-0.0033609774,0.070395544,-0.014940421,-0.0017542569,-0.008570514,-0.082679555,-0.14070885,0.0012934823,-0.05132951,-0.018003576,-0.072742514,-0.051187247,-0.021805072,-0.0031601675,0.00059032254,-0.020113338,-0.037131015,-0.009906862,-0.018802445,0.001921989,0.0015395153,-0.03361398,0.06289886,-0.016377043,-0.038527057,-0.013836378,0.006523282,-0.042849,-0.036495782,-0.0014477968,0.007699089,-0.013305968,-0.10268713,-0.009814715,-0.026848227,0.0026279166,-0.023056794,0.009676421,-0.015777413,-0.10903461,-0.008836567,-0.03475918,0.48744747,-0.016654205,-0.0073726065,-0.01743516,-0.011580873,0.025862433,-0.00390861,-0.10851527,-0.0044369083,-0.05535277,0.010143416,0.009221138,0.0010300325,0.009392612,-0.00072278595,-0.052627273,0.0007906845,-0.014015846,-0.00635314,-0.011966455,-0.01249269,0.0021890444,-0.087713204,-0.022013899,-0.015239382,0.0036213952,0.0012013861,-0.0759575,-0.037602205,0.003665837,0.0017871759,-0.019145597,-0.0031383857,-0.030571423,-0.032579947,-0.0010558343,-0.008966679,-0.0058635697,0.0016198071,-0.0438062,-0.0036946698,-0.09233479,-0.0030544726,-0.018674495,0.016122852,0.0032039285,-0.0038533867,0.00048413768,0.0036934153,-0.023837872,-0.062372558,-0.009128682,-0.03150884,0.04430243,0.04019553,-0.02471228,-0.040907018,-0.003581956,0.022950038,-0.004343828,-0.010623807,-0.06857255,-0.08230611,-0.053893644,-0.03611149,0.26842833,-0.0929311,-0.008674667,-0.021451823,-0.030511431,0.0051237815,-0.0038684243,-0.07004326,-0.004757558,0.018781293,-0.028879948,-0.04306148,0.13429976,0.0029643134,0.0046088663,-0.0045767426,-0.00016032206,0.0115192775,-0.007694248,-0.02811183,0.001965059,-0.06724636,0.0043929694,0.020827029,0.006035776,-0.056862205,-0.007974691,0.0037445128,-0.058175012,-0.07935793,0.005549937,-0.04051403,-0.005949654,-0.05948105,0.11942497,-0.051619202,0.047635585,0.021431442,0.0402825,-0.014023668,-0.038800348,-0.0038695615,0.017144162,-0.023850866,-0.10338829,-0.027510293,-0.0003024186,0.01766245,0.018095471,-0.0073639816,0.033816427,-0.0057968227,-0.009804075,-0.019220924,-0.04583102,-0.054841198,0.1700336,0.10274821,-0.04760516,0.024815809,-0.0010578106,0.0038881658,-0.004323506,0.0098344665,-0.103791595,-0.00296087,-0.0011632028,-0.037530027,0.005935193,0.0032525472,-0.039874718,-0.012842914,-0.028834626,-0.007828973,-0.022626081,-0.00084286556,0.009419598,0.010642797,-0.035917938,0.0072951238,0.013336023,-0.0003717239,0.03232602,-0.0771094,0.03156668,-0.005251085,0.00046850368,-0.004521141,0.010418617,-0.03596287,-0.092094645,-0.015065681,-0.051370766,-0.0903935,-0.020683823,-0.039599754,-0.020698823,-0.00027999096,0.010519601,-0.05950546,-0.012188015,0.6779035,-0.0188812,0.011538379,-0.034625776,0.5084427,0.0012602732,-0.0019600913,0.000694206,-0.007448027,0.023982076,0.0029721744,-0.02467854,-0.0015133307,-0.036841955,-0.0045860372,-0.008280851,-0.005944173,-0.015288033,-0.0027668583,-0.000043155625,-0.027708508,-0.008059275,-0.0032955091,-0.0632921,0.001118388,-0.051491983,-0.048706613,-0.0023541364,0.00007021241,0.001882365,0.00759295,0.22449924,-0.005209055,-0.030142844,-0.02771168,-0.38225812,-0.065233424,-0.03734303,-0.0073604872,0.013378069,0.038246173,-0.010639152,0.04769218,-0.0012208885,0.0064096972,-0.03528302,0.017525174,0.04289034,-0.03829342,-0.0056072166,-0.00651963,-0.005914239,-0.008171711,-0.031202223,-0.013988409,-0.028906088,-0.052549187,-0.17142773,0.015632609,-0.0033119356,0.0036157148,-0.0053728,-0.0009271573,-0.020442877,-0.13749923,0.008746945,-0.044177175,-0.0046969303,-0.028922252,0.048573475,0.0012869583,-0.067915685,0.06204638,-0.01757286,0.0020001533,-0.04506211,-0.027944492,-0.011662163,0.02323699,0.0031394735,0.054091036,0.2501672,-0.0044905907,-0.0071591064,-0.050160892,-0.005702734,0.0016365154,-0.003029964,0.003205791,-0.000083016,0.04011355,0.0039331703,-0.016398147,0.05506753,-0.00013684761,0.017053079,-0.006252315,0.5275786,-0.0013564449,-0.02476209,-0.03337674,-0.023323316,-0.06393881,-0.013471272,0.0041035195,0.00031280704,-0.06316817,0.01233888,-0.0028485917,-0.018502899,0.0021732561,-0.0070594065,-0.03465216,0.006344014,0.0046205074,-0.00080824795,-0.0018200763,-0.0060068746,0.0063086674,-0.01608073,0.0035920774,-0.034045525,-0.016559286,-0.0812421,-0.0074518956,0.0030561395,-0.0007189908,0.016030326,-0.05051787,0.001061561,-0.033397846,-0.01840375,-0.05404754,0.45847452,-0.00034904247,-0.002723962,0.024521811,-0.0041303416,-0.04118319,-0.012984276,-0.038722426,-0.059801605,-0.08322747,-0.01038833,-0.01355562,0.00077258144,-0.00023995386,-0.01384123,0.013976309,-0.014271151,-0.044900004,-0.00037420727,-0.006774537,-0.00042711524,0.0038952164,-0.006198857,-0.12872502,-0.015521816,-0.0034078425,-0.040188517,-0.006130824,-0.023476567,-0.05541756,-0.03548879,-0.06371491,-0.001602296,0.030939382,-0.004250314,-0.0067314506,-0.07068165,0.09329884,-0.0127783585,0.0014371164,0.0066899136,-0.05905872,0.076423705,-0.00080195395,-0.097538486,-0.007403313,-0.054188304,-0.007436093,0.0142815,-0.0029234265,-0.044603463,-0.056101173,-0.020017985,-0.0013425313,-0.015343289,0.012277242,-0.034329675,0.0007031858,-0.006920312,0.0009366735,0.0044701584,-0.0011346936,0.035182178,-0.001956678,-0.030222833,-0.06688546,0.021410225,-0.0007210076,-0.03723825,-0.013941405,0.021974396,-0.058288753,-0.004462734,0.00048580393,-0.0040406627,0.0021705509,0.10551343,-0.0013855612,-0.06441579,-0.0041457335,-0.0025419127,0.019314874,-0.0024374295,0.0019787233,0.008308652,-0.0014144661,0.0043893624,-0.009888481,-0.024833584,0.05759319,0.003338322,-0.0024422,-0.03127921,-0.003485648,0.004804641,-0.006738631,-0.011703676,0.027771061,-0.004785182,0.01954517,-0.0010536298,0.017454345,-0.004745725,-0.05247178,0.002689695,-0.0022316615,-0.038207367,0.061491393,-0.0052034683,-0.044900592,0.022208005,-0.004025214,0.073290735,-0.015072131,0.119681194,0.009085692,-0.052519344,-0.0068303198,0.0020829448,-0.057320967,-0.00139899,0.0042362455,-0.020913716,0.02031351,0.01708683,-0.05663482,-0.10741479,-0.025351629,-0.03483964,-0.08122732,-0.050691646,-0.019116934,-0.029170133,-0.10358915,-0.091970466,-0.008115377,-0.007027641,-0.013165688,-0.015328787,-0.0076346463,-0.059746224,-0.0050985385,-0.1553978,0.0015318431,-0.01609525,-0.0051463055,-0.006916393,0.025775982,-0.021359567,-0.008392595,0.0023412812,-0.011147874,-0.045729212,-0.0050488645,-0.007826149,-0.0027406812,-0.01820876,-0.002976587,-0.00890508,-0.062031783,-0.016039018,-0.003236888,0.0022747675,-0.0038452367,0.0033387877,0.013443695,-0.022954732,-0.049369693,0.038272366,-0.00086005405,-0.008741131,0.0018961208,0.0025889017,-0.04475794,0.038872086,0.03720503,-0.000036135316,-0.0055767447,-0.03823573,-0.0020892061,-0.015614744,0.00053523853,0.0024129413,-0.039277475,-0.03293582,0.017606188,-0.02270294,0.001054418,-0.014233876,-0.022036817,-0.0077031273,0.0005872641,-0.04464055,0.005836364,-0.0047503803,-0.0344772,-0.0071088113,-0.063029885,-0.011000418,-0.12373678,-0.028457977,-0.007356122,-0.0031478887,-0.014488956,-0.031807415,-0.009197675,0.01887171,0.0024531556,-0.06959434,-0.00237275,-0.026667133,-0.04799751,0.00088426704,0.006486114,0.004470151,-0.0007119761,0.021654438,0.0018881806,0.027891077,0.0035148878,-0.039206304,-0.005763745,-0.0038207835,-0.023612212,0.08420865,0.0024508624,-0.045526136,0.00642242,0.008330796,-0.0042311437,0.20917492,0.015576302,-0.0036146156,0.029364977,-0.036030307,-0.00006261538,-0.17708696,0.0012356052,0.0053470284,-0.034339216,-0.015730303,-0.009380351,-0.08267513,-0.037017412,-0.033069454,-0.077080116,-0.036053963,-0.0038969202,-0.011444547,0.09029046,0.0027710623,0.2464513,0.020284653,0.0008614375,-0.009624121,-0.023464764,-0.0035588183,-0.05402453,-0.025353143,-0.00041358848,-0.033059873,-0.07219252,-0.0063911034,-0.03160529,0.009539319,-0.01847157,-0.014910474,-0.0011682278,-0.007786859,0.09344057,-0.049789123,0.012290967,-0.079609096,0.04546015,-0.14438547,-0.041067444,0.20614207,0.0141771715,-0.03001238,0.0025738515,0.015016522,-0.03783466,-0.0004891525,-0.0059746634,-0.06470113,-0.00015190756,0.08613673,0.001231648,0.013946645,0.000831784,0.010583244,-0.0024283594,-0.10144673,-0.10804151,-0.047775324,0.0013518948,0.04077516,-0.00271477,-0.009679118,-0.011315497,0.38479736,-0.0015218798,-0.074162036,-0.021833453,-0.06802209,-0.034108244,0.19955274,0.013624525,0.018507779,-0.030444842,0.0011060331,-0.019509025,0.0003394303,-0.06767132,0.0019492488,0.013290744,0.00070584565,-0.029904835,0.00052629644,-0.012673033,0.0018544829,0.049878128,0.006785805,0.11623536,-0.038326524,-0.0029989704,-0.0040904125,-0.0946604,-0.011912134,0.13377413,-0.011091063,-0.07605472,0.0031089957,-0.00018788548,-0.027242705,-0.10253242,-0.049726482,-0.020230971,-0.001766853,-0.0096951425,-0.061868563,0.004824862,-0.05983634,-0.004760555,0.0027169343,0.0008636224,-0.03197228,0.013556898,-0.06759749,-0.00140894,-0.05031157,0.035246156,-0.0008996548,-0.02615681,0.011824263,0.0008543475,-0.06664632,0.007165026,0.002419345,0.03325538,0.0026117843,0.008712221,0.012722142,-0.054391477,-0.0018080724,-0.017193373,0.0064822724,-0.012676887,0.028446287,0.021115765,0.0081479885,0.003547328,0.046946105,0.017003298,-0.25701013,-0.036803927,-0.020206839,-0.03909393,-0.0033063404,-0.057399668,-0.016984498,-0.06300871,-0.02766459,-0.047452126,-0.09473395,-0.020100564,-0.0366866,-0.0067002615,-0.019941228,-0.11410207,-0.009415597,-0.009507202,-0.0389062,0.0025686864,-0.0025780573,0.06064443,-0.07769068,0.2453089,-0.0020995056,0.0022326373,0.01136096,-0.013695501,0.005480864,0.006300647,-0.17606315,0.013218721,0.06179379,-0.013792209,-0.013517854,-0.0008066227,0.0005983766,0.412692,-0.06273978,-0.058779173,0.0049744104,-0.03258946,-0.001691875,-0.00014984049,-0.05357475,-0.00455569,-0.028976519,0.010654565,2.132732,-0.0041039437,-0.020673651,-0.052650295,-0.076647855,0.096765324,0.004584532,-0.0065647047,-0.026209146,0.034412637,-0.004798761,-0.0061787404,0.027376156,0.005230199,0.054110937,-0.0023630229,0.0022901697,-0.00409664,-0.00054647785,-0.011757971,0.048014186,-0.027763296,0.06872758,0.0056324946,-0.05524524,0.019333933,0.21899274,-0.0021692994,-0.009310855,-0.006429393,0.00034312997,0.0032851323,0.002614707,0.0010557175,-0.06987858,-0.015799817,-0.04715957,0.021018349,-0.07084873,-0.042235553,-0.028734982,0.02281607,-0.075186506,-0.010435876,-0.0052494537,0.007408198,-0.14735425,-0.04164415,0.16526087,0.022955358,-0.05657178,-0.00083347596,-0.04549827,0.00006783893,0.0067317216,-0.010784535,0.0020953268,-0.014696728,0.0031523374,-0.027387347,0.027322482,0.038170524,-0.018202167,0.054034915,-0.006685743,0.0071197953,0.005984515,0.022618534,-0.0011496877,0.00006807549,-0.08113194,-0.008490936,-0.011454584,-0.0010749935,-0.04574528,0.0013936812,-0.0032944926,0.023701316,-0.00266656,-0.06780577,-0.010041297,-0.0013976297,0.07509798,0.22297794,-0.076913945,-0.034884736,0.010769913,-0.00014859997,-0.007848686,0.0074403286,-0.011722349,0.060571786,0.0028390298,0.037487615,-0.007786149,-0.06905134,-0.033992954,-0.043767698,-0.026549784,-0.00735357,-0.006997848,0.1487856,0.018057333,0.013673173,0.013038819,0.09417884,-0.0020569125,-0.0034641032,-0.03508841,0.059874944,0.036899813,0.002626204,-0.025215898,-0.0021701595,-0.041725874,0.009160245,-0.016320828,0.0070088822,-0.023834433,-0.0055726245,0.01871818,-0.074386194,0.004425168,-0.010185491,-0.034128353,-0.0069048135,0.0045683263,0.009698535,-0.000031435862,-0.05260165,0.022292838,-0.041543253,0.025414512,-0.011904724,0.0038016066,0.0022024387,-0.0018429738,0.0014975462,-0.0457954,-0.005097121,-0.013818309,0.067637004,0.0234048,0.00032004341,0.07890962,-0.000290579,-0.0026291278,0.056663178,-0.038352676,0.21577555,0.052523963,0.0006862879,-0.026884481,-0.0016028557,0.031420268,0.004164759,-0.01213768,-0.12249741,-0.027331587,0.001706264,-0.04856256,0.0024818794,-0.002220058,-0.0073597315,-0.07279181,-0.0043243533,0.04031106,-0.012333274,0.010705635,-0.17356709,-0.0019256063,-0.06134767,-0.005071583,-0.0041616117,-0.0032903648,0.08743015,0.001756859,-0.083704054,-0.0005665561,-0.030911341,0.0054847794,-0.08565679,-0.047504716,-0.003925018,0.16713673,-0.016365163,-0.06341094,-0.07555765,-0.007812757,0.006949246,-0.0037174108,-0.021676471,-0.047065318,0.0006777467,0.008575693,-0.019800764,-0.01481983,-0.039166763,0.085405156,-0.028467923,-0.007613674,-0.1371263,-0.021011464,-0.008903392,-0.012868412,0.000674514,-0.13887013,-0.00045442767,0.16421805,0.004850015,0.011118278,0.0023327444,-0.018865073,0.0026618517,-0.092282176,0.0071230307,-0.050254695,-0.07039791,-0.0009794263,0.031913847,-0.008710086,-0.023074202,-0.073036164,-0.0015852621,0.0030150115,-0.02123672,-0.012443751,-0.0054588383,0.008431368,0.00042581,0.0029781498,0.0031214785,-0.00029656058,0.37316042,0.008862247,-0.011342594,-0.056309443,-0.010723364,-0.01663526,-0.0050454945,0.013082512,-0.0042112954,-0.044673957,-0.04622397,-0.013601842,-0.0012384407,-0.071443856,-0.0010088626,0.008961217,-0.08393322,0.020292379,0.059440862,-0.028370183,0.0014997923,-0.035078175,0.0005380162,-0.076280095,-0.03025259,-0.04528869,0.01391259,-0.011320807,0.011650458,0.060566746,0.119442165,0.02302336,-0.0019393237,-0.006663813,0.0024416274,-0.04016833,0.033216022,-0.035065234,-0.010918785,-0.016547203,-0.01683118,-0.066740654,0.01932184,0.00016315933,-0.021125525,-0.010761686,0.005982479,-0.0008722283,-0.0316947,-0.0023325088,-0.015290603,0.11398047,-0.03507691,0.011151323,-0.0007060545,-0.0010339292,0.028002463,-0.0067466768,-0.018322697,-0.001628275,-0.0023343277,-0.002306813,-0.013526619,0.07065824,-0.024245031,0.003915339,-0.03487345,0.0022011613,0.01900544,-0.0044929106,0.023877885,-0.028956916,0.013434723,-0.09266515,-0.07943118,-0.06235183,0.004497409,0.010398097,-0.0010826141,-0.04863093,0.28876036,-0.013532314,-0.026715219,-0.0057783043,-0.006619935,-0.083257206,0.025781108,-0.04929718,-0.046497315,-0.0042866617,0.00016842224,-0.074612424,0.0064441315,-0.0051801195,0.0010622852,-0.01650218,-0.10068485,-0.09874011,-0.04157687,0.023584526,-0.01543008,-0.15975502,0.009103568,-0.0013185425,-0.006037149,-0.044823494,-0.092274696,-0.05007778,-0.004114012,0.007295411,-0.0024176368,0.08060623,-0.00020890264,0.10606294,0.0030351405,-0.014846098,0.0049278885,0.011074584,-0.008646145,-0.075939886,-0.0048097763,-0.06830412,0.05314022,-0.028753445,0.012817266,0.25810993,-0.096558444,-0.016188368,-0.017637618,0.084254414,-0.0059134914,-0.053907312,0.06941706,0.002299347,-0.084855475,-0.004074942,0.016667288,0.080900595,-0.039184064,-0.0040140664,-0.040946722,0.0058989404,0.07688538,-0.0017200636,0.015640965,0.011735887,-0.0009492333,-0.024323113,-0.0022016857,-0.008184007,-0.002000373,-0.0121694,-0.026498564,0.00060324883,-0.0075227576,-0.021033512,-0.0012142146,-0.0041815657,-0.05303681,0.006293043,-0.11674412,-0.0566476,-0.0015965556,0.010179867,-0.020867078,0.06614711,-0.026528329,-0.026625093,0.01580581,0.00348674,-0.0005730018,-0.0015957612,-0.022486135,-0.025319923,-0.045926604,0.04097446,-0.0022362676,-0.07543792,-0.002599238,0.0015314105,0.004109938,-0.038939007,-0.02262368,0.005133889,-0.09988272,-0.013392119,0.015237924,0.0038717892,-0.0015269443,-0.101874106,0.013909988,-0.04285482,0.036984686,0.0133988,0.031775624,-0.050555896,-0.010533099,0.005491702,-0.065383166,-0.025561567,0.0059748758,-0.02314431,-0.0041536726,0.0022176432,-0.0027600676,0.08940158,-0.043604463,-0.011777891,-0.00016720325,-0.0033527575,-0.0039328076,0.0020551458,0.003167946,0.0030203653,-0.026177436,-0.0010751671,0.19228989,0.00066520297,-0.006578963,0.00007059891,-0.056767445,0.00841127,0.00782527,0.0891615,-0.017871035,-0.03327214,-0.12159831,-0.0041367337,-0.0810146,-0.015525995,-0.06861713,-0.029864043,0.0023299754,0.0072216783,-0.03028158,-0.053960826,0.0009700749,0.009245223,0.03564737,0.011525886,0.009345425,-0.04394458,-0.02164498,-0.0024575186,-0.002554649,0.1246298,0.0006645913,-0.0024225377,-0.010801344,-0.0074530607,0.08660738,0.0028796978,0.0017528189,0.019311499,-0.017520696,-0.028882235,-0.01763634,-0.035863504,-0.04389747,-0.0006845568,0.13241965,0.001196105,0.42009187,-0.058855426,-0.0054749474,-0.108977236,0.019210132,0.009644914,-0.0145034045,0.22938089,0.014071144,0.0047893473,-0.0023329593,-0.050312415,0.027370594,-0.14008452,0.006562978,-0.0013750186,-0.04346501,-0.07965095,-0.0028849738,-0.001153131,-0.022429058,-0.006991219,-0.0114111565,-0.0030948035,-0.019839471,0.110696286,-0.10959334,-0.02558294,-0.036472313,0.004733486,-0.050815254,-0.00085860305,-0.03927321,-0.019842723,-0.018159779,-0.0053899647,0.07402281,0.0067304373,-0.013616817,-0.048148416,-0.0020392393,0.0029602405,-0.042427875,-0.007707143,-0.04038668,-0.027366057,0.07988384,-0.08025926,0.018612802,-0.046594888,0.001040942,-0.032303635,0.00055751903,-0.01068154,-0.025943883,-0.028287923,0.05500242,-0.042688962,0.010426125,-0.026565956,-0.09289156,-0.042299896,0.012787521,0.0013239235,-0.010894788,-0.028402913,-0.002902633,0.0005503325,0.16239165,-0.0030925963,0.0024453746,-0.07271515,-0.03252969,-0.0072650625,0.0044996687,0.043111667,-0.02975243,0.008533947,0.035981156,-0.033430494,0.068149894,0.9437437,-0.0041003707,-0.028958045,-0.0765689,0.09923218,-0.08535473,-0.023407135,-0.10705836,-0.0009682607,-0.007413468,-0.021585822,-0.024052229,0.0019800514,0.00084022526,-0.027515884,-0.06655913,-0.011071789,-0.012289029,-0.043763205,0.023749845,0.018048957,-0.006391904,-0.026531912,-0.002226354,0.009625755,0.032235637,-0.006846321,0.0021393485,-0.0013830881,-0.0225996,-0.00073458534,-0.016985588,-0.0030024722,0.13239905,-0.04218699,-0.011303432,0.003555093,0.027083557,0.001277009,-0.030622728,-0.0063988194,-0.0062113637,-0.071940124,-0.014172317,-0.0067238715,-0.043156415,0.0972536,-0.008567011,-0.00632542,-0.024780441,0.0067422367,-0.0002943538,-0.0113729555,0.0097677,0.0037718054,-0.019028928,0.0013503567,0.17428419,-0.028148651,1.2860682,-0.00446146,-0.012341071,0.036983192,0.0019778106,-0.015152533,-0.032206275,-0.0012948439,-0.0038879886,0.011020165,-0.0049578557,-0.0013320851,-0.033768892,-0.042850778,-0.016478322,-0.03584567,1.0041835,0.028514769,-0.0049299607,0.003480211,-0.014661178,-0.00038491585,-0.02014643,-0.019137137,-0.0133311525,0.014583334,-0.0021801337,-0.04101389,0.007246053,0.00048280344,0.00094698183,-0.05454751,-0.13544896,0.0046043973,-0.08338973,0.0024178196,0.030928109,-0.017127372,-0.02975034,-0.0052384883,-0.05124931,-0.0005124605,0.08878325,-0.021076228,-0.036873024,-0.0027722516,-0.0121959,-0.012479793,0.21006091,0.0056794356,-0.005460049,0.046483167,-0.0073887203,0.11636763,-0.02111939,0.001971329,0.0007204645,-0.0755321,0.06398285,-0.050901145,-0.00011840928,-0.0019120104,0.008207586,-0.0559702,0.33064905,-0.0009805709,0.00857535,0.00019138935,-0.035299063,0.0900287,-0.11296202,-0.051842257,-0.014684093,-0.0065314537,-0.008576487,-0.0032303713,0.024169233,-0.015114222,0.0778806,0.071866974,0.001483338,-0.10365624,-0.043729465,-0.007445045,0.0014756517,0.004575478,0.00929812,-0.021134531,0.07643907,-0.009644739,0.0209926,-0.012349164,-0.012770984,-0.033835817,0.00481924,-0.0047704913],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Rule 8, Layer 1\"},\"xaxis\":{\"title\":{\"text\":\"Neuron\"}},\"yaxis\":{\"title\":{\"text\":\"Neuron Acts Diff\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('23b09c02-b466-48cb-987c-68fc7ee7e1f9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize neuron_acts_diff_dict using plotly scatter plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_scatter_plot(rule_idx, layer):\n",
    "    neuron_acts_diff = neuron_acts_diff_dict[(rule_idx, layer)]\n",
    "    neuron_acts_diff_np = neuron_acts_diff.cpu().numpy()\n",
    "    x = np.arange(0, 2048)\n",
    "    y = neuron_acts_diff_np\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x = x,\n",
    "        y = y,\n",
    "        mode = \"markers\",\n",
    "        name = \"Neuron Acts Diff\",\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title = f\"Rule {rule_idx}, Layer {layer}\",\n",
    "        xaxis_title = \"Neuron\",\n",
    "        yaxis_title = \"Neuron Acts Diff\",\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "'''\n",
    "3 1 135\n",
    "3 1 30 6 0.03993378207087517\n",
    "3 2 30 6 0.045415375381708145\n",
    "3 3 30 6 0.04130616784095764\n",
    "3 4 30 6 0.057706933468580246\n",
    "8 1 38 6 0.038451485335826874\n",
    "8 1 40 7 0.03534302860498428'''\n",
    "\n",
    "rule_idx = 8\n",
    "layer = 1\n",
    "fig = get_scatter_plot(rule_idx, layer)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'inference_size' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m act_names \u001b[38;5;241m=\u001b[39m [utils\u001b[38;5;241m.\u001b[39mget_act_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp_post\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m fake_cache \u001b[38;5;241m=\u001b[39m \u001b[43mget_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard_seqs_int_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mget_activation\u001b[0;34m(board_seqs_int, act_names, num_games, start, games)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(games) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      8\u001b[0m     num_games \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(games)\n\u001b[0;32m----> 9\u001b[0m iterate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(start, start\u001b[38;5;241m+\u001b[39mnum_games, \u001b[43minference_size\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_games \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m     11\u001b[0m     iterate \u001b[38;5;241m=\u001b[39m tqdm(iterate, total\u001b[38;5;241m=\u001b[39mnum_games\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39minference_size)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'inference_size' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "act_names = [utils.get_act_name(\"mlp_post\", layer) for layer in range(8)]\n",
    "fake_cache = get_activation(board_seqs_int_train, act_names, games = [2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 59, 2048])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_cache['blocks.0.mlp.hook_post'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_bool[:, layer, pos].nonzero().squeeze(-1) + 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
