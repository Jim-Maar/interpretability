{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens sae-dashboard\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>model</th>\n",
       "      <th>saes_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2b-it-res-jb</th>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "      <td>{'blocks.12.hook_resid_post': 'gemma_2b_it_blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-2b-res-jb</th>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'gemma_2b_blocks....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-27b-pt-res</th>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_10/width_131k/average_l0_106': 'layer_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_104': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-att-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-mlp</th>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_119': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-mlp-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-res</th>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/average_l0_105': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-2b-pt-res-canonical</th>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_16k/canonical': 'layer_0/width...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-att</th>\n",
       "      <td>gemma-scope-9b-pt-att</td>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_131k/average_l0_55': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-scope-9b-pt-mlp</th>\n",
       "      <td>gemma-scope-9b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>{'layer_0/width_131k/average_l0_11': 'layer_0/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-attn-out-v5-128k</th>\n",
       "      <td>gpt2-small-attn-out-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'v5_128k_layer_0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-attn-out-v5-32k</th>\n",
       "      <td>gpt2-small-attn-out-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_attn_out': 'v5_32k_layer_0', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-hook-z-kk</th>\n",
       "      <td>gpt2-small-hook-z-kk</td>\n",
       "      <td>ckkissane/attn-saes-gpt2-small-all-layers</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_z': 'gpt2-small_L0_Hcat_z_lr1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-mlp-out-v5-128k</th>\n",
       "      <td>gpt2-small-mlp-out-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'v5_128k_layer_0', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-mlp-out-v5-32k</th>\n",
       "      <td>gpt2-small-mlp-out-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'v5_32k_layer_0', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-mlp-tm</th>\n",
       "      <td>gpt2-small-mlp-tm</td>\n",
       "      <td>tommmcgrath/gpt2-small-mlp-out-saes</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': 'sae_group_gpt2_bloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res-jb</th>\n",
       "      <td>gpt2-small-res-jb</td>\n",
       "      <td>jbloom/GPT2-Small-SAEs-Reformatted</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_pre': 'blocks.0.hook_res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res-jb-feature-splitting</th>\n",
       "      <td>gpt2-small-res-jb-feature-splitting</td>\n",
       "      <td>jbloom/GPT2-Small-Feature-Splitting-Experiment...</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.8.hook_resid_pre_768': 'blocks.8.hook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_sce-ajt</th>\n",
       "      <td>gpt2-small-res_sce-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_sce-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_sce-ajt', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_scefr-ajt</th>\n",
       "      <td>gpt2-small-res_scefr-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_scefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_scefr-ajt',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_scl-ajt</th>\n",
       "      <td>gpt2-small-res_scl-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_scl-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_scl-ajt', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_sle-ajt</th>\n",
       "      <td>gpt2-small-res_sle-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_sle-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_sle-ajt', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_slefr-ajt</th>\n",
       "      <td>gpt2-small-res_slefr-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_slefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_slefr-ajt',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-res_sll-ajt</th>\n",
       "      <td>gpt2-small-res_sll-ajt</td>\n",
       "      <td>neuronpedia/gpt2-small__res_sll-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.2.hook_resid_pre': '2-res_sll-ajt', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-mid-v5-128k</th>\n",
       "      <td>gpt2-small-resid-mid-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_mid': 'v5_128k_layer_0',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-mid-v5-32k</th>\n",
       "      <td>gpt2-small-resid-mid-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_mid': 'v5_32k_layer_0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-post-v5-128k</th>\n",
       "      <td>gpt2-small-resid-post-v5-128k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'v5_128k_layer_0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small-resid-post-v5-32k</th>\n",
       "      <td>gpt2-small-resid-post-v5-32k</td>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>{'blocks.0.hook_resid_post': 'v5_32k_layer_0.p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-res-wg</th>\n",
       "      <td>mistral-7b-res-wg</td>\n",
       "      <td>JoshEngels/Mistral-7B-Residual-Stream-SAEs</td>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>{'blocks.8.hook_resid_pre': 'mistral_7b_layer_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-70m-deduped-att-sm</th>\n",
       "      <td>pythia-70m-deduped-att-sm</td>\n",
       "      <td>ctigges/pythia-70m-deduped__att-sm_processed</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.0.hook_attn_out': '0-att-sm', 'blocks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-70m-deduped-mlp-sm</th>\n",
       "      <td>pythia-70m-deduped-mlp-sm</td>\n",
       "      <td>ctigges/pythia-70m-deduped__mlp-sm_processed</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.0.hook_mlp_out': '0-mlp-sm', 'blocks....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-70m-deduped-res-sm</th>\n",
       "      <td>pythia-70m-deduped-res-sm</td>\n",
       "      <td>ctigges/pythia-70m-deduped__res-sm_processed</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>{'blocks.0.hook_resid_pre': 'e-res-sm', 'block...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 release  \\\n",
       "gemma-2b-it-res-jb                                    gemma-2b-it-res-jb   \n",
       "gemma-2b-res-jb                                          gemma-2b-res-jb   \n",
       "gemma-scope-27b-pt-res                            gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                              gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-att-canonical          gemma-scope-2b-pt-att-canonical   \n",
       "gemma-scope-2b-pt-mlp                              gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-mlp-canonical          gemma-scope-2b-pt-mlp-canonical   \n",
       "gemma-scope-2b-pt-res                              gemma-scope-2b-pt-res   \n",
       "gemma-scope-2b-pt-res-canonical          gemma-scope-2b-pt-res-canonical   \n",
       "gemma-scope-9b-pt-att                              gemma-scope-9b-pt-att   \n",
       "gemma-scope-9b-pt-mlp                              gemma-scope-9b-pt-mlp   \n",
       "gpt2-small-attn-out-v5-128k                  gpt2-small-attn-out-v5-128k   \n",
       "gpt2-small-attn-out-v5-32k                    gpt2-small-attn-out-v5-32k   \n",
       "gpt2-small-hook-z-kk                                gpt2-small-hook-z-kk   \n",
       "gpt2-small-mlp-out-v5-128k                    gpt2-small-mlp-out-v5-128k   \n",
       "gpt2-small-mlp-out-v5-32k                      gpt2-small-mlp-out-v5-32k   \n",
       "gpt2-small-mlp-tm                                      gpt2-small-mlp-tm   \n",
       "gpt2-small-res-jb                                      gpt2-small-res-jb   \n",
       "gpt2-small-res-jb-feature-splitting  gpt2-small-res-jb-feature-splitting   \n",
       "gpt2-small-res_sce-ajt                            gpt2-small-res_sce-ajt   \n",
       "gpt2-small-res_scefr-ajt                        gpt2-small-res_scefr-ajt   \n",
       "gpt2-small-res_scl-ajt                            gpt2-small-res_scl-ajt   \n",
       "gpt2-small-res_sle-ajt                            gpt2-small-res_sle-ajt   \n",
       "gpt2-small-res_slefr-ajt                        gpt2-small-res_slefr-ajt   \n",
       "gpt2-small-res_sll-ajt                            gpt2-small-res_sll-ajt   \n",
       "gpt2-small-resid-mid-v5-128k                gpt2-small-resid-mid-v5-128k   \n",
       "gpt2-small-resid-mid-v5-32k                  gpt2-small-resid-mid-v5-32k   \n",
       "gpt2-small-resid-post-v5-128k              gpt2-small-resid-post-v5-128k   \n",
       "gpt2-small-resid-post-v5-32k                gpt2-small-resid-post-v5-32k   \n",
       "mistral-7b-res-wg                                      mistral-7b-res-wg   \n",
       "pythia-70m-deduped-att-sm                      pythia-70m-deduped-att-sm   \n",
       "pythia-70m-deduped-mlp-sm                      pythia-70m-deduped-mlp-sm   \n",
       "pythia-70m-deduped-res-sm                      pythia-70m-deduped-res-sm   \n",
       "\n",
       "                                                                               repo_id  \\\n",
       "gemma-2b-it-res-jb                             jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "gemma-2b-res-jb                                   jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "gemma-scope-27b-pt-res                                   google/gemma-scope-27b-pt-res   \n",
       "gemma-scope-2b-pt-att                                     google/gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-att-canonical                           google/gemma-scope-2b-pt-att   \n",
       "gemma-scope-2b-pt-mlp                                     google/gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-mlp-canonical                           google/gemma-scope-2b-pt-mlp   \n",
       "gemma-scope-2b-pt-res                                     google/gemma-scope-2b-pt-res   \n",
       "gemma-scope-2b-pt-res-canonical                           google/gemma-scope-2b-pt-res   \n",
       "gemma-scope-9b-pt-att                                     google/gemma-scope-9b-pt-att   \n",
       "gemma-scope-9b-pt-mlp                                     google/gemma-scope-9b-pt-mlp   \n",
       "gpt2-small-attn-out-v5-128k                jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs   \n",
       "gpt2-small-attn-out-v5-32k                  jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs   \n",
       "gpt2-small-hook-z-kk                         ckkissane/attn-saes-gpt2-small-all-layers   \n",
       "gpt2-small-mlp-out-v5-128k                  jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs   \n",
       "gpt2-small-mlp-out-v5-32k                    jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs   \n",
       "gpt2-small-mlp-tm                                  tommmcgrath/gpt2-small-mlp-out-saes   \n",
       "gpt2-small-res-jb                                   jbloom/GPT2-Small-SAEs-Reformatted   \n",
       "gpt2-small-res-jb-feature-splitting  jbloom/GPT2-Small-Feature-Splitting-Experiment...   \n",
       "gpt2-small-res_sce-ajt                             neuronpedia/gpt2-small__res_sce-ajt   \n",
       "gpt2-small-res_scefr-ajt                         neuronpedia/gpt2-small__res_scefr-ajt   \n",
       "gpt2-small-res_scl-ajt                             neuronpedia/gpt2-small__res_scl-ajt   \n",
       "gpt2-small-res_sle-ajt                             neuronpedia/gpt2-small__res_sle-ajt   \n",
       "gpt2-small-res_slefr-ajt                         neuronpedia/gpt2-small__res_slefr-ajt   \n",
       "gpt2-small-res_sll-ajt                             neuronpedia/gpt2-small__res_sll-ajt   \n",
       "gpt2-small-resid-mid-v5-128k              jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs   \n",
       "gpt2-small-resid-mid-v5-32k                jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs   \n",
       "gpt2-small-resid-post-v5-128k            jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs   \n",
       "gpt2-small-resid-post-v5-32k              jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs   \n",
       "mistral-7b-res-wg                           JoshEngels/Mistral-7B-Residual-Stream-SAEs   \n",
       "pythia-70m-deduped-att-sm                 ctigges/pythia-70m-deduped__att-sm_processed   \n",
       "pythia-70m-deduped-mlp-sm                 ctigges/pythia-70m-deduped__mlp-sm_processed   \n",
       "pythia-70m-deduped-res-sm                 ctigges/pythia-70m-deduped__res-sm_processed   \n",
       "\n",
       "                                                  model  \\\n",
       "gemma-2b-it-res-jb                          gemma-2b-it   \n",
       "gemma-2b-res-jb                                gemma-2b   \n",
       "gemma-scope-27b-pt-res                       gemma-2-2b   \n",
       "gemma-scope-2b-pt-att                        gemma-2-2b   \n",
       "gemma-scope-2b-pt-att-canonical              gemma-2-2b   \n",
       "gemma-scope-2b-pt-mlp                        gemma-2-2b   \n",
       "gemma-scope-2b-pt-mlp-canonical              gemma-2-2b   \n",
       "gemma-scope-2b-pt-res                        gemma-2-2b   \n",
       "gemma-scope-2b-pt-res-canonical              gemma-2-2b   \n",
       "gemma-scope-9b-pt-att                        gemma-2-2b   \n",
       "gemma-scope-9b-pt-mlp                        gemma-2-2b   \n",
       "gpt2-small-attn-out-v5-128k                  gpt2-small   \n",
       "gpt2-small-attn-out-v5-32k                   gpt2-small   \n",
       "gpt2-small-hook-z-kk                         gpt2-small   \n",
       "gpt2-small-mlp-out-v5-128k                   gpt2-small   \n",
       "gpt2-small-mlp-out-v5-32k                    gpt2-small   \n",
       "gpt2-small-mlp-tm                            gpt2-small   \n",
       "gpt2-small-res-jb                            gpt2-small   \n",
       "gpt2-small-res-jb-feature-splitting          gpt2-small   \n",
       "gpt2-small-res_sce-ajt                       gpt2-small   \n",
       "gpt2-small-res_scefr-ajt                     gpt2-small   \n",
       "gpt2-small-res_scl-ajt                       gpt2-small   \n",
       "gpt2-small-res_sle-ajt                       gpt2-small   \n",
       "gpt2-small-res_slefr-ajt                     gpt2-small   \n",
       "gpt2-small-res_sll-ajt                       gpt2-small   \n",
       "gpt2-small-resid-mid-v5-128k                 gpt2-small   \n",
       "gpt2-small-resid-mid-v5-32k                  gpt2-small   \n",
       "gpt2-small-resid-post-v5-128k                gpt2-small   \n",
       "gpt2-small-resid-post-v5-32k                 gpt2-small   \n",
       "mistral-7b-res-wg                            mistral-7b   \n",
       "pythia-70m-deduped-att-sm            pythia-70m-deduped   \n",
       "pythia-70m-deduped-mlp-sm            pythia-70m-deduped   \n",
       "pythia-70m-deduped-res-sm            pythia-70m-deduped   \n",
       "\n",
       "                                                                              saes_map  \n",
       "gemma-2b-it-res-jb                   {'blocks.12.hook_resid_post': 'gemma_2b_it_blo...  \n",
       "gemma-2b-res-jb                      {'blocks.0.hook_resid_post': 'gemma_2b_blocks....  \n",
       "gemma-scope-27b-pt-res               {'layer_10/width_131k/average_l0_106': 'layer_...  \n",
       "gemma-scope-2b-pt-att                {'layer_0/width_16k/average_l0_104': 'layer_0/...  \n",
       "gemma-scope-2b-pt-att-canonical      {'layer_0/width_16k/canonical': 'layer_0/width...  \n",
       "gemma-scope-2b-pt-mlp                {'layer_0/width_16k/average_l0_119': 'layer_0/...  \n",
       "gemma-scope-2b-pt-mlp-canonical      {'layer_0/width_16k/canonical': 'layer_0/width...  \n",
       "gemma-scope-2b-pt-res                {'layer_0/width_16k/average_l0_105': 'layer_0/...  \n",
       "gemma-scope-2b-pt-res-canonical      {'layer_0/width_16k/canonical': 'layer_0/width...  \n",
       "gemma-scope-9b-pt-att                {'layer_0/width_131k/average_l0_55': 'layer_0/...  \n",
       "gemma-scope-9b-pt-mlp                {'layer_0/width_131k/average_l0_11': 'layer_0/...  \n",
       "gpt2-small-attn-out-v5-128k          {'blocks.0.hook_attn_out': 'v5_128k_layer_0', ...  \n",
       "gpt2-small-attn-out-v5-32k           {'blocks.0.hook_attn_out': 'v5_32k_layer_0', '...  \n",
       "gpt2-small-hook-z-kk                 {'blocks.0.hook_z': 'gpt2-small_L0_Hcat_z_lr1....  \n",
       "gpt2-small-mlp-out-v5-128k           {'blocks.0.hook_mlp_out': 'v5_128k_layer_0', '...  \n",
       "gpt2-small-mlp-out-v5-32k            {'blocks.0.hook_mlp_out': 'v5_32k_layer_0', 'b...  \n",
       "gpt2-small-mlp-tm                    {'blocks.0.hook_mlp_out': 'sae_group_gpt2_bloc...  \n",
       "gpt2-small-res-jb                    {'blocks.0.hook_resid_pre': 'blocks.0.hook_res...  \n",
       "gpt2-small-res-jb-feature-splitting  {'blocks.8.hook_resid_pre_768': 'blocks.8.hook...  \n",
       "gpt2-small-res_sce-ajt               {'blocks.2.hook_resid_pre': '2-res_sce-ajt', '...  \n",
       "gpt2-small-res_scefr-ajt             {'blocks.2.hook_resid_pre': '2-res_scefr-ajt',...  \n",
       "gpt2-small-res_scl-ajt               {'blocks.2.hook_resid_pre': '2-res_scl-ajt', '...  \n",
       "gpt2-small-res_sle-ajt               {'blocks.2.hook_resid_pre': '2-res_sle-ajt', '...  \n",
       "gpt2-small-res_slefr-ajt             {'blocks.2.hook_resid_pre': '2-res_slefr-ajt',...  \n",
       "gpt2-small-res_sll-ajt               {'blocks.2.hook_resid_pre': '2-res_sll-ajt', '...  \n",
       "gpt2-small-resid-mid-v5-128k         {'blocks.0.hook_resid_mid': 'v5_128k_layer_0',...  \n",
       "gpt2-small-resid-mid-v5-32k          {'blocks.0.hook_resid_mid': 'v5_32k_layer_0', ...  \n",
       "gpt2-small-resid-post-v5-128k        {'blocks.0.hook_resid_post': 'v5_128k_layer_0'...  \n",
       "gpt2-small-resid-post-v5-32k         {'blocks.0.hook_resid_post': 'v5_32k_layer_0.p...  \n",
       "mistral-7b-res-wg                    {'blocks.8.hook_resid_pre': 'mistral_7b_layer_...  \n",
       "pythia-70m-deduped-att-sm            {'blocks.0.hook_attn_out': '0-att-sm', 'blocks...  \n",
       "pythia-70m-deduped-mlp-sm            {'blocks.0.hook_mlp_out': '0-mlp-sm', 'blocks....  \n",
       "pythia-70m-deduped-res-sm            {'blocks.0.hook_resid_pre': 'e-res-sm', 'block...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records({k:v.__dict__ for k,v in get_pretrained_saes_directory().items()}).T\n",
    "df.drop(columns=[\"expected_var_explained\", \"expected_l0\", \"config_overrides\", \"conversion_func\"], inplace=True)\n",
    "df # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
