{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_names = [utils.get_act_name(\"resid_pre\", layer) for layer in range(8)]\n",
    "fake_cache = get_activation(act_names, 10000, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 59, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_cache[utils.get_act_name(\"resid_pre\", 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(589351, device='cuda:0'),\n",
       " tensor(592642, device='cuda:0'),\n",
       " tensor(0.9944, device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make list of all placed tiles, then list of list of all adjecent tiles\n",
    "def get_placed_yours_acc(layer_yours, layer_placed):\n",
    "    resid_layer_yours = fake_cache[utils.get_act_name(\"resid_pre\", layer_yours)].to(device)\n",
    "    resid_layer_placed = fake_cache[utils.get_act_name(\"resid_pre\", layer_placed)].to(device)\n",
    "    placed_probe = get_probe(layer_placed, \"placed\", \"post\")[0].to(device)\n",
    "    linear_probe = get_probe(layer_yours, \"linear\", \"post\")[0].to(device)\n",
    "    placed_probe_result = einops.einsum(resid_layer_placed, placed_probe, 'b p d, d r c o -> b p r c o').argmax(axis=-1)\n",
    "    emb_linear_result = einops.einsum(resid_layer_yours, linear_probe, 'b p d, d r c o -> b p r c o').argmax(axis=-1)\n",
    "    num_correct = ((placed_probe_result == PLACED) * (emb_linear_result == YOURS)).sum()\n",
    "    num_total = (placed_probe_result == PLACED).sum()\n",
    "    return num_correct, num_total, num_correct / num_total\n",
    "\n",
    "num_correct, num_total, acc = get_placed_yours_acc(1, 1)\n",
    "num_correct, num_total, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "ADJACENT = 0\n",
    "NOT_ADJACENT = 1\n",
    "\n",
    "def mark_adjacent_positions(layer_placed):\n",
    "    placed_probe = get_probe(layer_placed, \"placed\", \"post\")[0].to(device)\n",
    "    resid_layer_placed = fake_cache[utils.get_act_name(\"resid_pre\", layer_placed)].to(device)\n",
    "    placed_probe_result = einops.einsum(resid_layer_placed, placed_probe, 'b p d, d r c o -> b p r c o').argmax(axis=-1)\n",
    "    # Create kernel for all 8 directions (including diagonals)\n",
    "    kernel = t.tensor([\n",
    "        [1, 1, 1],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 1]\n",
    "    ], device=placed_probe_result.device).to(t.float32)\n",
    "    \n",
    "    # Reshape kernel for conv2d: (out_channels, in_channels, height, width)\n",
    "    kernel = kernel.view(1, 1, 3, 3)\n",
    "    \n",
    "    # Process each batch and position\n",
    "    batch_size, num_positions, height, width = placed_probe_result.shape\n",
    "    result = t.zeros_like(placed_probe_result)\n",
    "    \n",
    "    # Reshape to combine batch and position dimensions\n",
    "    placed = (placed_probe_result == PLACED).view(-1, 1, height, width).float()\n",
    "    \n",
    "    # Use convolution to mark adjacent positions\n",
    "    # Padding=1 to handle edges correctly\n",
    "    adjacent = F.conv2d(placed, kernel, padding=1) > 0\n",
    "    \n",
    "    # Reshape back to original dimensions and convert to original dtype\n",
    "    result = adjacent.view(batch_size, num_positions, height, width)\n",
    "    \n",
    "    return result\n",
    "\n",
    "placed_adjacent_result = mark_adjacent_positions(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False,  True,  True,  True, False],\n",
       "        [False, False, False, False,  True, False,  True, False],\n",
       "        [False, False, False, False,  True,  True,  True, False],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placed_adjacent_result[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6266, device='cuda:0'),\n",
       " tensor(0.8550, device='cuda:0'),\n",
       " tensor(0.7232, device='cuda:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_flipped_scores(layer_flipped_pred, layer_flipped_correct, placed_adjacent_result):\n",
    "    resid_layer_flipped_pred = fake_cache[utils.get_act_name(\"resid_pre\", layer_flipped_pred)].to(device)\n",
    "    resid_layer_flipped_correct = fake_cache[utils.get_act_name(\"resid_pre\", layer_flipped_correct)].to(device)\n",
    "    flipped_probe_pred = get_probe(layer_flipped_pred, \"flipped\", \"post\")[0].to(device)\n",
    "    flipped_probe_correct = get_probe(layer_flipped_correct, \"flipped\", \"post\")[0].to(device)\n",
    "    flipped_probe_result_pred = einops.einsum(resid_layer_flipped_pred, flipped_probe_pred, 'b p d, d r c o -> b p r c o').argmax(axis=-1)\n",
    "    flipped_probe_result_correct = einops.einsum(resid_layer_flipped_correct, flipped_probe_correct, 'b p d, d r c o -> b p r c o').argmax(axis=-1)\n",
    "    correct = ((flipped_probe_result_correct == FLIPPED) * placed_adjacent_result).to(t.int32)\n",
    "    pred = ((flipped_probe_result_pred == FLIPPED) * placed_adjacent_result).to(t.int32)\n",
    "    tp = (correct * pred).sum()\n",
    "    fp = ((1 - correct) * pred).sum()\n",
    "    fn = (correct * (1 - pred)).sum()\n",
    "    tn = ((1 - correct) * (1 - pred)).sum()\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = 2 * recall * precision / (recall + precision)\n",
    "    return recall, precision, f1\n",
    "\n",
    "recall, precision, f1 = get_flipped_scores(1, 6, placed_adjacent_result)\n",
    "recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Scores:\n",
    "# Embedding Accuracy at Placed : 0.9269\n",
    "# Layer 0 Accuracy at Placed : 0.9986\n",
    "\n",
    "# Flipped Scores:\n",
    "# Recall: 0.6266\n",
    "# Precision: 0.8550\n",
    "# F1: 0.7232"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
