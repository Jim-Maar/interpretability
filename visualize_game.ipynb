{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "chapter_dir = r\"./\" if chapter in os.listdir() else os.getcwd().split(chapter)[0]\n",
    "sys.path.append(chapter_dir + f\"{chapter}/exercises\")\n",
    "\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from ipywidgets import interact\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import random\n",
    "from IPython.display import display\n",
    "from typing import List, Union, Optional, Tuple, Callable, Dict\n",
    "import typeguard\n",
    "from functools import partial\n",
    "# from torcheval.metrics.functional import multiclass_f1_score\n",
    "from sklearn.metrics import f1_score as multiclass_f1_score\n",
    "import dataclasses\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "\n",
    "from plotly_utils import imshow\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Optional, Tuple, Callable, Dict\n",
    "from jaxtyping import Float, Int, Bool, Shaped, jaxtyped\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "# os.chdir(section_dir)\n",
    "section_dir = Path.cwd()\n",
    "assert section_dir.name == \"interpretability\"\n",
    "\n",
    "OTHELLO_ROOT = (section_dir / \"othello_world\").resolve()\n",
    "OTHELLO_MECHINT_ROOT = (OTHELLO_ROOT / \"mechanistic_interpretability\").resolve()\n",
    "\n",
    "sys.path.append(str(OTHELLO_MECHINT_ROOT))\n",
    "from mech_interp_othello_utils import (\n",
    "    OthelloBoardState,\n",
    "    to_int,\n",
    "    to_string,\n",
    "    string_to_label,\n",
    "    str_to_int,\n",
    ")\n",
    "\n",
    "t.manual_seed(42)\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 8,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = 8,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 61,\n",
    "    n_ctx = 59,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\",\n",
    "    device=device,\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "\n",
    "sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"synthetic_model.pth\")\n",
    "# champion_ship_sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"championship_model.pth\")\n",
    "model.load_state_dict(sd)\n",
    "board_seqs_int = t.tensor(np.load(OTHELLO_MECHINT_ROOT / \"board_seqs_int_small.npy\"), dtype=t.long)\n",
    "board_seqs_string_train = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_string_train.pth\",\n",
    "    )\n",
    ")\n",
    "'''board_seqs_string_test = t.load(\n",
    "    os.path.join(\n",
    "        section_dir,\n",
    "        \"data/board_seqs_string_valid.pth\",\n",
    "    )\n",
    ")'''\n",
    "board_seqs_string = t.tensor(np.load(OTHELLO_MECHINT_ROOT / \"board_seqs_string_small.npy\"), dtype=t.long)\n",
    "assert all([middle_sq not in board_seqs_string for middle_sq in [27, 28, 35, 36]])\n",
    "assert board_seqs_int.max() == 60\n",
    "num_games, length_of_game = board_seqs_int.shape\n",
    "# Define possible indices (excluding the four center squares)\n",
    "stoi_indices = [i for i in range(64) if i not in [27, 28, 35, 36]]\n",
    "# Define our rows, and the function that converts an index into a (row, column) label, e.g. `E2`\n",
    "alpha = \"ABCDEFGH\"\n",
    "def to_board_label(i):\n",
    "    return f\"{alpha[i//8]}{i%8}\"\n",
    "# Get our list of board labels\n",
    "board_labels = list(map(to_board_label, stoi_indices))\n",
    "full_board_labels = list(map(to_board_label, range(64)))\n",
    "# start = 30000\n",
    "def get_focus_logits_and_cache():\n",
    "    start = 0\n",
    "    num_games = 50\n",
    "    focus_games_int = board_seqs_int[start : start + num_games]\n",
    "    focus_games_string = board_seqs_string[start: start + num_games]\n",
    "    focus_logits, focus_cache = model.run_with_cache(focus_games_int[:, :-1].to(device))\n",
    "    return focus_logits, focus_cache\n",
    "# focus_logits.shape\n",
    "def one_hot(list_of_ints, num_classes=64):\n",
    "    out = t.zeros((num_classes,), dtype=t.float32)\n",
    "    out[list_of_ints] = 1.\n",
    "    return out\n",
    "focus_states = np.zeros((num_games, 60, 8, 8), dtype=np.float32)\n",
    "focus_valid_moves = t.zeros((num_games, 60, 64), dtype=t.float32)\n",
    "\n",
    "BLANK1 = 0\n",
    "BLACK = 1\n",
    "WHITE = -1\n",
    "\n",
    "EMPTY = 0\n",
    "YOURS = 1\n",
    "MINE = 2\n",
    "\n",
    "FLIPPED = 0\n",
    "NOT_FLIPPED = 1\n",
    "\n",
    "PLACED = 0\n",
    "NOT_PLACED = 1\n",
    "\n",
    "FLIPPED_TOP = 0\n",
    "FLIPPED_TOP_RIGHT = 1\n",
    "FLIPPED_RIGHT = 2\n",
    "FLIPPED_BOTTOM_RIGHT = 3\n",
    "FLIPPED_BOTTOM = 4\n",
    "FLIPPED_BOTTOM_LEFT = 5\n",
    "FLIPPED_LEFT = 6\n",
    "FLIPPED_TOP_LEFT = 7\n",
    "\n",
    "ACCESIBLE = 0\n",
    "NOT_ACCESIBLE = 1\n",
    "\n",
    "LEGAL = 0\n",
    "NOT_LEGAL = 1\n",
    "\n",
    "# Load Model\n",
    "def load_model(device):\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers = 8,\n",
    "        d_model = 512,\n",
    "        d_head = 64,\n",
    "        n_heads = 8,\n",
    "        d_mlp = 2048,\n",
    "        d_vocab = 61,\n",
    "        n_ctx = 59,\n",
    "        act_fn=\"gelu\",\n",
    "        normalization_type=\"LNPre\",\n",
    "        device=device,\n",
    "    )\n",
    "    model = HookedTransformer(cfg)\n",
    "\n",
    "    sd = transformer_lens.utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"synthetic_model.pth\")\n",
    "    # champion_ship_sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"championship_model.pth\")\n",
    "    model.load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "alpha = \"ABCDEFGH\"\n",
    "\n",
    "# Load Probes\n",
    "\n",
    "probes = dict()\n",
    "probe_modules = os.listdir(\"probes\")\n",
    "for probe_module in probe_modules:\n",
    "    probe_types = os.listdir(f\"probes/{probe_module}\")\n",
    "    for probe_type in probe_types:\n",
    "        for layer in range(8):\n",
    "            path = f\"probes/{probe_module}/{probe_type}/resid_{layer}_{probe_type}.pth\"\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            if device.type == \"cpu\":\n",
    "                probe = t.load(path, map_location=device).detach()\n",
    "            else:\n",
    "                probe = t.load(path).to(device).detach()\n",
    "            probes[(probe_module, probe_type, layer)] = probe\n",
    "\n",
    "def get_probe(layer : Int = 5, probe_type : str = \"linear\", probe_module : str = \"post\"):\n",
    "    # assert probe_module in [\"post\", \"mid\"]\n",
    "    # assert probe_type in [\"linear\", \"flipped\"]\n",
    "    return probes[(probe_module, probe_type, layer)]\n",
    "\n",
    "probe_directions = {\n",
    "    \"linear\": {\n",
    "        \"empty\" : EMPTY,\n",
    "        \"yours\" : YOURS,\n",
    "        \"mine\" : MINE, \n",
    "    },\n",
    "    \"flipped\": {\n",
    "        \"flipped\" : FLIPPED,\n",
    "        \"not_flipped\" : NOT_FLIPPED,\n",
    "    },\n",
    "    \"placed\" : {\n",
    "        \"placed\" : PLACED,\n",
    "        \"not_placed\" : NOT_PLACED,\n",
    "    },\n",
    "    \"accesible\" : {\n",
    "        \"accesible\" : ACCESIBLE,\n",
    "        \"not_accesible\" : NOT_ACCESIBLE,\n",
    "    },\n",
    "    \"legal\" : {\n",
    "        \"legal\" : LEGAL,\n",
    "        \"not_legal\" : NOT_LEGAL,\n",
    "    },\n",
    "    \"placed_and_flipped\" : {\n",
    "        \"top\" : FLIPPED_TOP,\n",
    "        \"top_right\" : FLIPPED_TOP_RIGHT,\n",
    "        \"right\" : FLIPPED_RIGHT,\n",
    "        \"bottom_right\" : FLIPPED_BOTTOM_RIGHT,\n",
    "        \"bottom\" : FLIPPED_BOTTOM,\n",
    "        \"bottom_left\" : FLIPPED_BOTTOM_LEFT,\n",
    "        \"left\" : FLIPPED_LEFT,\n",
    "        \"top_left\" : FLIPPED_TOP_LEFT,\n",
    "    },\n",
    "    \"placed_and_flipped_stripe\" : {\n",
    "        \"top\" : FLIPPED_TOP,\n",
    "        \"top_right\" : FLIPPED_TOP_RIGHT,\n",
    "        \"right\" : FLIPPED_RIGHT,\n",
    "        \"bottom_right\" : FLIPPED_BOTTOM_RIGHT,\n",
    "        \"bottom\" : FLIPPED_BOTTOM,\n",
    "        \"bottom_left\" : FLIPPED_BOTTOM_LEFT,\n",
    "        \"left\" : FLIPPED_LEFT,\n",
    "        \"top_left\" : FLIPPED_TOP_LEFT,\n",
    "    }\n",
    "}\n",
    "\n",
    "probe_directions_list = {\n",
    "    k : list(v.keys()) for k, v in probe_directions.items()\n",
    "}\n",
    "\n",
    "short_cuts = {\n",
    "    \"empty\" : \"E\",\n",
    "    \"yours\" : \"Y\",\n",
    "    \"mine\" : \"M\",\n",
    "    \"flipped\" : \"F\",\n",
    "    \"not_flipped\" : \"NF\",\n",
    "    \"placed\" : \"P\",\n",
    "    \"not_placed\" : \"NP\",\n",
    "    \"accesible\" : \"A\",\n",
    "    \"not_accesible\" : \"NA\",\n",
    "    \"legal\" : \"L\",\n",
    "    \"not_legal\" : \"NL\",\n",
    "    \"top\" : \"T\",\n",
    "    \"top_right\" : \"TR\",\n",
    "    \"right\" : \"R\",\n",
    "    \"bottom_right\" : \"BR\",\n",
    "    \"bottom\" : \"B\",\n",
    "    \"bottom_left\" : \"BL\",\n",
    "    \"left\" : \"L\",\n",
    "    \"top_left\" : \"TL\",\n",
    "    \"linear\" : \"L\",\n",
    "    \"placed_and_flipped\" : \"PF\",\n",
    "    \"placed_and_flipped_stripe\" : \"PFS\",\n",
    "}\n",
    "\n",
    "def get_short_cut(name):\n",
    "    return short_cuts[name]\n",
    "\n",
    "def get_probe_names():\n",
    "    return list(probe_directions.keys())\n",
    "\n",
    "def get_direction_str(probe_name, direction_int):\n",
    "    for direction_str in probe_directions[probe_name]:\n",
    "        if probe_directions[probe_name][direction_str] == direction_int:\n",
    "            return direction_str\n",
    "    assert(False)\n",
    "\n",
    "def get_direction_int(directions_str):\n",
    "    directions_str = directions_str.lower()\n",
    "    for probe_name in probe_directions:\n",
    "        if directions_str in probe_directions[probe_name]:\n",
    "            return probe_directions[probe_name][directions_str]\n",
    "    assert(False)\n",
    "\n",
    "def seq_to_state_stack(str_moves):\n",
    "    \"\"\"\n",
    "    Takes a sequence of moves and returns a stack of states for each move with dimensions (num_moves, rows, cols)\n",
    "    -1 white, 0 blank, 1 black\n",
    "    \"\"\"\n",
    "    board = OthelloBoardState()\n",
    "    states = []\n",
    "    for move in str_moves:\n",
    "        board.umpire(move)\n",
    "        states.append(np.copy(board.state))\n",
    "    states = np.stack(states, axis=0)\n",
    "    return states\n",
    "\n",
    "\n",
    "# Ploting Functions\n",
    "\n",
    "def plot_square_as_board(state, diverging_scale=True, **kwargs):\n",
    "    \"\"\"Takes a square input (8 by 8) and plot it as a board. Can do a stack of boards via facet_col=0\"\"\"\n",
    "    kwargs = {\n",
    "        \"y\": [i for i in alpha],\n",
    "        \"x\": [str(i) for i in range(8)],\n",
    "        \"color_continuous_scale\": \"RdBu\" if diverging_scale else \"Blues\",\n",
    "        \"color_continuous_midpoint\": 0. if diverging_scale else None,\n",
    "        \"aspect\": \"equal\",\n",
    "        **kwargs\n",
    "    }\n",
    "    imshow(state, **kwargs)\n",
    "\n",
    "def plot_probe_outputs(focus_cache, linear_probe, layer, game_index, move, **kwargs):\n",
    "    residual_stream = focus_cache[\"resid_post\", layer][game_index, move]\n",
    "    # print(\"residual_stream\", residual_stream.shape)\n",
    "    # probe_out = einops.einsum(residual_stream, linear_probe, \"d_model, d_model row col options -> row col options\")\n",
    "    probe_out = einops.einsum(residual_stream, linear_probe, \"d_model, modes d_model row col options -> modes row col options\")[0]\n",
    "    '''if move % 2 == 0:\n",
    "        probe_out = probe_out[0]\n",
    "    else:\n",
    "        probe_out = probe_out[1]'''\n",
    "    probabilities = probe_out.softmax(dim=-1)\n",
    "    plot_square_as_board(probabilities, facet_col=2, facet_labels=[\"P(EMPTY)\", \"P(YOURS)\", \"P(MINE)\"], **kwargs)\n",
    "\n",
    "def plot_game(games_str, game_index=0, end_move=16):\n",
    "    '''\n",
    "    This shows the game the 0'th move is the first move the display shows the board after the move was made\n",
    "    '''\n",
    "    focus_states = seq_to_state_stack(games_str[game_index])\n",
    "    imshow(\n",
    "        focus_states[:end_move],\n",
    "        facet_col=0,\n",
    "        facet_col_wrap=8,\n",
    "        facet_labels=[f\"Move {i}\" for i in range(0, end_move)],\n",
    "        title=\"First 16 moves of first game\",\n",
    "        color_continuous_scale=\"Greys\",\n",
    "        y = [i for i in alpha],\n",
    "    )\n",
    "\n",
    "def square_to_tuple(square, is_int=False):\n",
    "    if is_int:\n",
    "        square = to_string(square)\n",
    "    row = square // 8\n",
    "    col = square % 8\n",
    "    return (row, col)\n",
    "\n",
    "def to_board_label(i):\n",
    "    return f\"{alpha[i//8]}{i%8}\"\n",
    "\n",
    "def tuple_to_label(t):\n",
    "    row = t[0]\n",
    "    col = t[1]\n",
    "    return f\"{alpha[row]}{col}\"\n",
    "\n",
    "def get_focus_games(model = None, device = \"cpu\"):\n",
    "    # Load board data as ints (i.e. 0 to 60)\n",
    "    board_seqs_int = t.tensor(np.load(OTHELLO_MECHINT_ROOT / \"board_seqs_int_small.npy\"), dtype=t.long)\n",
    "    # Load board data as \"strings\" (i.e. 0 to 63 with middle squares skipped out)\n",
    "    board_seqs_string = t.tensor(np.load(OTHELLO_MECHINT_ROOT / \"board_seqs_string_small.npy\"), dtype=t.long)\n",
    "\n",
    "    assert all([middle_sq not in board_seqs_string for middle_sq in [27, 28, 35, 36]])\n",
    "    assert board_seqs_int.max() == 60\n",
    "\n",
    "    num_games, length_of_game = board_seqs_int.shape\n",
    "    start = 0\n",
    "    num_games = 50\n",
    "    focus_games_int = board_seqs_int[start : start + num_games]\n",
    "    focus_games_string = board_seqs_string[start: start + num_games]\n",
    "\n",
    "    if model is not None:\n",
    "        focus_logits, focus_cache = model.run_with_cache(focus_games_int[:, :-1].to(device))\n",
    "        return focus_games_int, focus_games_string, focus_logits, focus_cache\n",
    "    return focus_games_int, focus_games_string\n",
    "\n",
    "def square_tuple_from_square(square : str):\n",
    "    return (alpha.index(square[0]), int(square[1])) \n",
    "\n",
    "reverse_alpha = [\"H\", \"G\", \"F\", \"E\", \"D\", \"C\", \"B\", \"A\"]\n",
    "\n",
    "def save_plotly_to_html(fig, filename):\n",
    "    TEMPLATE_PATH = \"interactive_plots/template.html\"\n",
    "    assert os.path.exists(TEMPLATE_PATH)\n",
    "    plotly_jinja_data = {\"fig\":fig.to_html(full_html=False)}\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        with open(TEMPLATE_PATH) as template_file:\n",
    "            j2_template = Template(template_file.read())\n",
    "            output_file.write(j2_template.render(plotly_jinja_data))\n",
    "\n",
    "def save_plotly_to_png(fig, filename):\n",
    "    fig.write_image(filename)\n",
    "\n",
    "def save_plotly(fig, name):\n",
    "    save_plotly_to_html(fig, f\"interactive_plots/{name}.html\")\n",
    "    save_plotly_to_png(fig, f\"plots/{name}.png\")\n",
    "\n",
    "def plot_boards_general(x_labels : List[str],\n",
    "                        y_labels : List[str],\n",
    "                        boards : Float[Tensor, \"x y rows cols\"],\n",
    "                        size_of_board : Int = 200,\n",
    "                        margin_t : Int = 100,\n",
    "                        title_text : str = \"\",\n",
    "                        color_scale : str = \"RdBu\",\n",
    "                        color_range  : str = \"symmetric\",\n",
    "                        static_image : bool = False,\n",
    "                        save : bool = False):\n",
    "    # TODO: add attn/mlp only\n",
    "    # TODO: Change Width and Height accordingly\n",
    "    boards = boards.flip(2)\n",
    "    x_len, y_len, rows, cols = boards.shape\n",
    "    subplot_titles = [f\"{y_label}, {x_label}\" for y_label in y_labels for x_label in x_labels]\n",
    "    # subplot_titles = [f\"P: {i}, T: {label_list[i]}, L: {j}\" for i in range(vis_args.start_pos, vis_args.end_pos) for j in range(vis_args.layers)]\n",
    "    width = x_len * size_of_board\n",
    "    height = y_len * size_of_board + margin_t\n",
    "    vertical_spacing = 70 / height\n",
    "    fig = make_subplots(rows=y_len, cols=x_len, subplot_titles=subplot_titles, vertical_spacing=vertical_spacing)\n",
    "    boards_min = boards.min().item()\n",
    "    boards_max = boards.max().item()        \n",
    "    abs_max = max(abs(boards_min), abs(boards_max))\n",
    "    if color_range == \"symmetric\":\n",
    "        begin = -abs_max\n",
    "        end = abs_max\n",
    "    else:\n",
    "        begin = boards_min\n",
    "        end = boards_max\n",
    "    for x in range(x_len):\n",
    "        for y in range(y_len):\n",
    "            heatmap = go.Heatmap(\n",
    "                z=boards[x, y].cpu(),\n",
    "                x=list(range(0, rows)),\n",
    "                y=reverse_alpha,\n",
    "                hoverongaps = False,\n",
    "                zmin=begin,\n",
    "                zmax=end,\n",
    "                colorscale=color_scale,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                heatmap,\n",
    "                row=y + 1,\n",
    "                col=x + 1\n",
    "            )\n",
    "    fig.layout.update(width=width, height=height, margin_t=margin_t, title_text=title_text) \n",
    "    if static_image:\n",
    "        # count the number of images in the last_plot directory\n",
    "        num_images = len(list(Path(\"last_plot\").glob(\"*.png\")))\n",
    "        fig.write_image(f'last_plot/last_plot{num_images+1}.png')\n",
    "    else:\n",
    "        fig.show()\n",
    "    if save:\n",
    "        save_plotly(fig, title_text)\n",
    "\n",
    "def get_color(val : float):\n",
    "    val = min(int(val * 5), 4)\n",
    "    # Define the gradient characters from darkest to lightest\n",
    "    gradient_chars = [\" \", \"░\", \"▒\", \"▓\", \"█\"]\n",
    "    return gradient_chars[val]\n",
    "\n",
    "@dataclass\n",
    "class VisualzeBoardArguments:\n",
    "    include_attn_only = False\n",
    "    include_mlp_only = False\n",
    "    include_pre_resid = False\n",
    "    include_layer_norm = False\n",
    "    include_resid_post = True\n",
    "    start_pos=0\n",
    "    end_pos=59\n",
    "    layer_start=0\n",
    "    layers=8\n",
    "    static_image=False#\n",
    "    size_of_board = 225\n",
    "    margin_t = 100\n",
    "    mode = \"linear\"\n",
    "    horizontal_spacing = 20\n",
    "    margin_l = 100\n",
    "\n",
    "def get_score_from_resid(resid, layer):\n",
    "    # assert probe_name in [\"linear\", \"flipped\"]\n",
    "    linear_probe = get_probe(layer, \"linear\", \"post\")\n",
    "    flipped_probe = get_probe(layer, \"flipped\", \"post\")\n",
    "    assert len(resid.shape) == 2\n",
    "    seq_len, d_model = resid.shape\n",
    "    logits = einops.einsum(resid, linear_probe, 'pos d_model, modes d_model rows cols options -> modes pos rows cols options')[0]\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    flipped_logits = einops.einsum(resid, flipped_probe, 'pos d_model, modes d_model rows cols options -> modes pos rows cols options')[0]\n",
    "    flipped_probs = flipped_logits.softmax(dim=-1)\n",
    "    probs_copy = probs.clone()\n",
    "    # Convert Back to Balck/White\n",
    "    for i in range(0, seq_len, 2):\n",
    "        probs[i, :, :, 1], probs[i, :, :, 2] = probs_copy[i, :, :, 2], probs_copy[i, :, :, 1]\n",
    "    color_score = 0.5 + (probs[:, :, :, 2] - probs[:, :, :, 1])/2\n",
    "    # Flip the color score on the rows dimension\n",
    "    # TODO: Add Flips as Labels...\n",
    "    color_score = color_score.flip(1)\n",
    "    flip_score = flipped_probs[:, :, :, [0]].flip(1).squeeze(dim=-1)\n",
    "    # flip_score = flipped_probs[:, :, :, [0]].squeeze(dim=-1)\n",
    "    return color_score, flip_score\n",
    "\n",
    "def get_boards(input_int : Float[Tensor, \"pos\"], vis_args : VisualzeBoardArguments, model: HookedTransformer):\n",
    "    _, cache = model.run_with_cache(input_int)\n",
    "    boards = []\n",
    "    flip_boards = []\n",
    "    for layer in range(vis_args.layers):\n",
    "        color_scores = []\n",
    "        flip_scores = []\n",
    "        if vis_args.include_resid_post:\n",
    "            resid = cache[\"resid_post\", layer][0].detach()\n",
    "            color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "            color_scores += [color_score]\n",
    "            flip_scores += [flip_score]\n",
    "        if vis_args.include_pre_resid:\n",
    "            resid = cache[\"resid_pre\", layer][0].detach()\n",
    "            color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "            color_scores += [color_score]\n",
    "            flip_scores += [flip_score]\n",
    "        if vis_args.include_attn_only:\n",
    "            resid = cache[\"resid_post\", layer][0].detach() - t.stack([cache[\"mlp_out\", l][0].detach() for l in range(layer, layer + 1)]).sum(dim=0) - cache[\"resid_pre\", layer][0].detach()\n",
    "            color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "            color_scores += [color_score]\n",
    "            flip_scores += [flip_score]\n",
    "        if vis_args.include_mlp_only:\n",
    "            resid = cache[\"resid_post\", layer][0].detach() - t.stack([cache[\"attn_out\", l][0].detach() for l in range(layer, layer + 1)]).sum(dim=0) - cache[\"resid_pre\", layer][0].detach()\n",
    "            color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "            color_scores += [color_score]\n",
    "            flip_scores += [flip_score]\n",
    "        if vis_args.include_layer_norm:\n",
    "            resid = cache[f\"blocks.{layer}.ln1.hook_normalized\"][0].detach()\n",
    "            color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "            color_scores += [color_score]\n",
    "            flip_scores += [flip_score]\n",
    "        if vis_args.include_layer_norm:\n",
    "            resid = cache[f\"blocks.{layer}.ln2.hook_normalized\"][0].detach()\n",
    "            color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "            color_scores += [color_score]\n",
    "            flip_scores += [flip_score]\n",
    "        color_score = t.stack(color_scores, dim=0)\n",
    "        # color_score = color_score.transpose(0, 1)\n",
    "        # color_score = color_score.reshape(-1, 8, 8)\n",
    "        flip_score = t.stack(flip_scores, dim=0)\n",
    "        # flip_score = flip_score.transpose(0, 1)\n",
    "        # flip_score = flip_score.reshape(-1, 8, 8)\n",
    "        # color_score, flip_score = get_score_from_resid(resid, layer)\n",
    "        boards += [color_score]\n",
    "        flip_boards += [flip_score]\n",
    "    boards = t.stack(boards)\n",
    "    flip_boards = t.stack(flip_boards)\n",
    "    return boards, flip_boards\n",
    "\n",
    "def plot_boards(label_list: List[str], boards : Float[Tensor, \"layers mode pos rows cols\"], flip_boards : Float[Tensor, \"layers mode pos rows cols\"], vis_args: VisualzeBoardArguments):\n",
    "    # TODO: add attn/mlp only\n",
    "    # TODO: Change Width and Height accordingly\n",
    "    _, _, _, rows, cols = boards.shape\n",
    "    print(boards.shape)\n",
    "    seq_len = vis_args.end_pos - vis_args.start_pos\n",
    "    modes = []\n",
    "    if vis_args.include_resid_post:\n",
    "        modes += [\"N\"]\n",
    "    if vis_args.include_pre_resid:\n",
    "        modes += [\"P\"]\n",
    "    if vis_args.include_attn_only:\n",
    "        modes += [\"A\"]\n",
    "    if vis_args.include_mlp_only:\n",
    "        modes += [\"M\"]\n",
    "    if vis_args.include_layer_norm:\n",
    "        modes += [\"Attn\"]\n",
    "        modes += [\"MLP\"]\n",
    "    subplot_titles = [f\"P: {i}, T: {label_list[i]}, L: {j}, {mode}\" for i in range(vis_args.start_pos, vis_args.end_pos) for j in range(vis_args.layer_start, vis_args.layers) for mode in modes]\n",
    "    # subplot_titles = [f\"P: {i}, T: {label_list[i]}, L: {j}\" for i in range(vis_args.start_pos, vis_args.end_pos) for j in range(vis_args.layers)]\n",
    "    width = ((vis_args.layers - vis_args.layer_start) * len(modes) * vis_args.size_of_board) * (1 + 2 * vis_args.horizontal_spacing)\n",
    "    height = vis_args.margin_t + seq_len * vis_args.size_of_board\n",
    "    vertical_spacing = 70 / height\n",
    "    horizontal_spacing = vis_args.horizontal_spacing\n",
    "    fig = make_subplots(rows=seq_len, cols=(vis_args.layers - vis_args.layer_start) * len(modes), subplot_titles=subplot_titles, vertical_spacing=vertical_spacing, horizontal_spacing = horizontal_spacing)\n",
    "    for pos_idx, pos in enumerate(range(vis_args.start_pos, vis_args.end_pos)):\n",
    "        for layer_idx, layer in enumerate(range(vis_args.layer_start, vis_args.layers)):\n",
    "            for mode_idx, mode in enumerate(modes):\n",
    "                text_data = [[get_color(flip_boards[layer, mode_idx, pos, i, j]) for j in range(cols)] for i in range(rows)]\n",
    "                if vis_args.mode == \"linear\":\n",
    "                    heatmap = go.Heatmap(\n",
    "                        z=boards[layer, mode_idx, pos].cpu(),\n",
    "                        text=text_data,\n",
    "                        x=list(range(0, rows)),\n",
    "                        y=reverse_alpha,\n",
    "                        hoverongaps = False,\n",
    "                        zmin=0.0,\n",
    "                        zmax=1.0,\n",
    "                        colorscale=\"RdBu\",\n",
    "                        texttemplate=\"%{text}\",\n",
    "                        showscale=False,\n",
    "                        # textfont_color=\"green\",\n",
    "                    )\n",
    "                elif vis_args.mode == \"flipped\":\n",
    "                    heatmap = go.Heatmap(\n",
    "                        z=flip_boards[layer, mode_idx, pos].cpu(),\n",
    "                        x=list(range(0, rows)),\n",
    "                        y=reverse_alpha,\n",
    "                        hoverongaps = False,\n",
    "                        zmin=0.0,\n",
    "                        zmax=1.0,\n",
    "                        colorscale=\"Greens\", # Green color scale\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid Mode\")\n",
    "                fig.add_trace(\n",
    "                    heatmap,\n",
    "                    row=pos_idx + 1,\n",
    "                    col=layer_idx * len(modes) + mode_idx + 1\n",
    "                )\n",
    "    fig.layout.update(width=width, height=height, margin_t=vis_args.margin_t, margin_l=vis_args.margin_l, title_text=f\"Probe Results per Position per Layer\") \n",
    "    if vis_args.static_image:\n",
    "        # count the number of images in the last_plot directory\n",
    "        num_images = len(list(Path(\"last_plot\").glob(\"*.png\")))\n",
    "        fig.write_image(f'last_plot/last_plot{num_images+1}.png')\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "def visualize_game(input_str, vis_args: VisualzeBoardArguments, model: HookedTransformer):\n",
    "    # 1. Get the cache\n",
    "    # 2. Get Board States from the cache using the Pobes\n",
    "    # 3. Plot the Board States\n",
    "    # assert not (vis_args.include_attn_only and vis_args.include_mlp_only)\n",
    "    if len(input_str) > 59:\n",
    "        input_str = input_str[:59]\n",
    "    label_list = string_to_label(input_str)\n",
    "    boards, flip_boards = get_boards(t.Tensor(to_int(input_str)).to(t.int32), vis_args, model)\n",
    "    plot_boards(label_list, boards, flip_boards, vis_args)\n",
    "\n",
    "\n",
    "def label_to_tuple(label):\n",
    "    # return f\"{alpha[label // 8]}{label % 8}\" This but reverse\n",
    "    alhpha_ind = alpha.find(label[0])\n",
    "    return (alhpha_ind, int(label[1]))\n",
    "\n",
    "def label_to_string(label):\n",
    "    tup = label_to_tuple(label)\n",
    "    return tup[0] * 8 + tup[1]\n",
    "\n",
    "def label_to_int(label):\n",
    "    st =  label_to_string(label)\n",
    "    return str_to_int(st)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vis_args = VisualzeBoardArguments()\n",
    "    vis_args.start_pos = 0\n",
    "    vis_args.end_pos = 20\n",
    "    vis_args.layers = 6\n",
    "    vis_args.include_attn_only = False\n",
    "    vis_args.include_mlp_only = False\n",
    "    vis_args.include_layer_norm = True\n",
    "    vis_args.mode = \"flipped\"\n",
    "    vis_args.static_image = True\n",
    "\n",
    "    model = load_model(\"cuda\")\n",
    "    _, focus_games_str = get_focus_games()\n",
    "\n",
    "    clean_input_str = focus_games_str[0][:30]\n",
    "    # visualize_game(clean_input_str, vis_args, model)\n",
    "    '''\n",
    "    print(label_to_int(\"B3\"))\n",
    "    print(label_to_string(\"B3\"))\n",
    "    print(label_to_tuple(\"B3\"))'''\n",
    "\n",
    "# Create a helper function, where I can say, I want to get the thiese activations for the first ... games\n",
    "def get_activation(act_names, num_games, start=0, board_seqs_int=board_seqs_int):\n",
    "    # TODO: If this takes to long or something, Make a filter step!\n",
    "    act_name_results = {act_name : [] for act_name in act_names}\n",
    "    inference_size = 1000\n",
    "    for batch in range(start, start+num_games, inference_size):\n",
    "        with t.inference_mode():\n",
    "            _, cache = model.run_with_cache(\n",
    "                board_seqs_int[batch:batch+inference_size, :-1].to(device),\n",
    "                return_type=None,\n",
    "                names_filter=lambda name: name in act_names\n",
    "                # names_filter=lambda name: name == f\"blocks.{layer}.hook_resid_mid\" or name == f\"blocks.{layer}.mlp.hook_post\"\n",
    "                # names_filter=lambda name: name == f\"blocks.{layer}.hook_resid_pre\" or name == f\"blocks.{layer}.mlp.hook_post\"\n",
    "            )\n",
    "        for act_name in act_names:\n",
    "            act_name_results[act_name] += [cache[act_name]]\n",
    "    for act_name in act_names:\n",
    "        act_name_results[act_name] = t.cat(act_name_results[act_name], dim=0)\n",
    "        act_name_results[act_name] = act_name_results[act_name].detach()[:num_games]\n",
    "    return act_name_results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board_seqs_int_test = t.load(\"data/board_seqs_int_valid.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 59, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "game = 0\n",
    "vis_args = VisualzeBoardArguments()\n",
    "vis_args.start_pos = 14\n",
    "vis_args.end_pos = 20\n",
    "vis_args.layer_start = 1\n",
    "vis_args.layers = 3\n",
    "# vis_args.static_image = True\n",
    "vis_args.include_resid_post = False\n",
    "vis_args.include_layer_norm = True\n",
    "vis_args.horizontal_spacing = 0.07\n",
    "vis_args.static_image = True\n",
    "# visualize_game(to_string(board_seqs_int_test[game]), vis_args, model)\n",
    "visualize_game(board_seqs_string[game], vis_args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_logits, focus_cache = get_focus_logits_and_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_out = focus_cache[\"attn_out\", 0]\n",
    "attn_out.shape\n",
    "import pickle\n",
    "\n",
    "# load data/neurons_in_famility/flipping_neurons.pkl using pickle to a dictionary\n",
    "with open('data/neurons_in_famility/flipping_neurons.pkl', 'rb') as f:\n",
    "    flipping_neurons = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2048, 512]), torch.Size([50, 59, 2048]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_out = focus_cache[\"mlp_post\", layer]\n",
    "W_out = model.W_out\n",
    "W_out.shape, mlp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.b_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OV = model.OV.AB\n",
    "for layer in range(1, 8):\n",
    "    OV_layer = OV[layer]\n",
    "    # resid = focus_cache[f\"blocks.{layer}.ln1.hook_normalized\", layer][0].detach()\n",
    "    probe = get_probe(layer-1, \"linear\", \"post\")[0, :, :, :, MINE]\n",
    "    output = einops.einsum(probe, OV, \"d_model_in rows cols, head d_model_in d_model_out -> rows cols d_model_out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_in(\n",
    "    model: HookedTransformer,\n",
    "    layer: int,\n",
    "    neuron: int,\n",
    "    normalize: bool = False,\n",
    ") -> Float[Tensor, \"d_model\"]:\n",
    "    '''\n",
    "    Returns the input weights for the given neuron.\n",
    "\n",
    "    If normalize is True, the weight is normalized to unit norm.\n",
    "    '''\n",
    "    # SOLUTION\n",
    "    w_in = model.W_in[layer, :, neuron].detach().clone()\n",
    "    if normalize: w_in /= w_in.norm(dim=0, keepdim=True)\n",
    "    return w_in\n",
    "\n",
    "\n",
    "def get_w_out(\n",
    "    model: HookedTransformer,\n",
    "    layer: int,\n",
    "    neuron: int,\n",
    "    normalize: bool = False,\n",
    ") -> Float[Tensor, \"d_model\"]:\n",
    "    '''\n",
    "    Returns the output weights for the given neuron.\n",
    "\n",
    "    If normalize is True, the weight is normalized to unit norm.\n",
    "    '''\n",
    "    # SOLUTION\n",
    "    w_out = model.W_out[layer, neuron, :].detach().clone()\n",
    "    if normalize: w_out /= w_out.norm(dim=0, keepdim=True)\n",
    "    return  w_out\n",
    "\n",
    "def get_similiarity(neuron : Int, layer : Int, tiles : List[Tuple[str, str, str, str]], metric = \"avg\"):\n",
    "    avg_similiarity = 0\n",
    "    direction_all = t.zeros([512]).to(device)\n",
    "    for label, probe_type, feature_str, in_or_out in tiles:\n",
    "        tile_tuple = label_to_tuple(label)\n",
    "        y, x = tile_tuple\n",
    "        feature = get_direction_int(feature_str)\n",
    "        if in_or_out == \"in\":\n",
    "            probe_module = \"mid\"\n",
    "            w = get_w_in(model, layer, neuron, normalize=True)\n",
    "        else:\n",
    "            probe_module = \"post\"\n",
    "            w = get_w_out(model, layer, neuron, normalize=True)\n",
    "        probe = get_probe(layer, probe_type=probe_type, probe_module=probe_module)\n",
    "        direction = probe[0, :, y, x, feature]\n",
    "        direction = direction / direction.norm()\n",
    "        direction_all += direction\n",
    "        similiarity = einops.einsum(direction, w, \"d_model, d_model ->\").item()\n",
    "        if feature_str == \"empty\":\n",
    "            similiarity = similiarity / 3\n",
    "        avg_similiarity += similiarity\n",
    "    direction_all = direction_all / direction_all.norm()\n",
    "    similiarity_all = einops.einsum(direction_all, w, \"d_model, d_model ->\").item()\n",
    "    if metric == \"avg\":\n",
    "        return avg_similiarity / len(tiles)\n",
    "    else:\n",
    "        return similiarity_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c9b748ab3d4eb197236b6e7415099e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5b079cdf424718aaf88227158f3b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd11f389a50402fb6377071fe4b9bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133d705d0082407f8c48b485fe444b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ec63e39ce24e1ebaebcd3ea4675765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c680a02d92424aa2bff29fdb6a4f9250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76e9b7201d043f8b6d6eb5516d384e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = []\n",
    "how_many = 10\n",
    "\n",
    "for row in range(8):\n",
    "    for col in range(8):\n",
    "        labels.append(tuple_to_label((row, col)))\n",
    "\n",
    "flipping_neurons2 = {}\n",
    "\n",
    "for layer in range(1, 8):\n",
    "    print(f\"Layer {layer}\")\n",
    "    similiarities = dict()\n",
    "    flipping_neurons2[layer] = []\n",
    "    for label in tqdm(labels):\n",
    "        tiles_out = [\n",
    "            (label, \"flipped\", \"flipped\", \"out\")\n",
    "        ]\n",
    "\n",
    "        for neuron1 in range(4 * 512):\n",
    "            similiarity_out = get_similiarity(neuron1, layer, tiles_out)\n",
    "            similiarities[neuron1] = similiarity_out\n",
    "\n",
    "        sorted_similiarities = sorted(similiarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        neurons_list = [neuron for neuron, similiarity in sorted_similiarities[:how_many]]\n",
    "        similiarities_list = [round(similiarity, 2) for neuron, similiarity in sorted_similiarities[:how_many]]\n",
    "        flipping_neurons2[layer] += neurons_list\n",
    "        # print(f\"Layer: {layer}, Tile: {label}, Neurons: {neurons}, Similiarities: {similiarities}\")\n",
    "    flipping_neurons2[layer] = list(set(flipping_neurons2[layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "544\n",
      "540\n",
      "454\n",
      "204\n",
      "127\n",
      "258\n",
      "tensor([     0.0000,   5157.6152,   4808.4189,   5780.5054,   3884.3809,\n",
      "         -3282.2759, -12740.1523,   -617.6957])\n",
      "tensor([     0.0000,  -1889.1267,  -1515.0182,  -1069.9028,  -1671.4114,\n",
      "         -1823.5784,  -3657.9995, -12940.5439])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Hier kann ich noch ein bisschen weiter rum spielen ...\n",
    "# TODO: Only look at a set of Flipping Neurons\n",
    "\n",
    "attn_out = focus_cache[\"attn_out\", 0]\n",
    "attn_out.shape\n",
    "import pickle\n",
    "\n",
    "# load data/neurons_in_famility/flipping_neurons.pkl using pickle to a dictionary\n",
    "with open('data/neurons_in_famility/flipping_neurons.pkl', 'rb') as f:\n",
    "    flipping_neurons = pickle.load(f)\n",
    "\n",
    "# flipping_neurons = flipping_neurons2\n",
    "\n",
    "# check how much percent of Flipped Logits come from MLP vs Attention\n",
    "avg_flipped_logit_percent_from_mlp : Float[Tensor, \"layers\"] = t.zeros((8, ))\n",
    "flipped_logit_from_mlp = t.zeros((8, ))\n",
    "flipped_logit_from_attn = t.zeros((8, ))\n",
    "for layer in range(1, 8):\n",
    "    mlp_post = focus_cache[\"mlp_post\", layer]\n",
    "    mlp_post_mean = mlp_post.mean(dim=0)\n",
    "    mlp_post_mean = einops.repeat(mlp_post_mean, \"pos neurons -> game pos neurons\", game=mlp_post.shape[0]).clone()\n",
    "    # get flipping neurons of the layer\n",
    "    neurons = []\n",
    "    # neurons += flipping_neurons[layer]\n",
    "    for rule_name, neuron_list in flipping_neurons[layer].items():\n",
    "        neurons += neuron_list\n",
    "    neurons = list(set(neurons))\n",
    "    neurons.sort()\n",
    "    # neurons = list(range(2048))\n",
    "    # print(neurons[:10])\n",
    "    print(len(neurons))\n",
    "    probe = get_probe(layer, \"flipped\", \"post\")[0]\n",
    "    attn_out = focus_cache[\"attn_out\", layer]\n",
    "    mlp_out = focus_cache[\"mlp_out\", layer]\n",
    "    mlp_post = focus_cache[\"mlp_post\", layer]\n",
    "    W_out = model.W_out[layer].detach()\n",
    "    b_out = model.b_out[layer].detach()\n",
    "    # mlp_post = mlp_post[:, :, neurons]\n",
    "    # mlp_post = t.max(mlp_post, t.zeros_like(mlp_post))\n",
    "    # W_out = W_out[neurons]\n",
    "    # print(mlp_post.shape, mlp_post_mean.shape)\n",
    "    mlp_post_mean[:, :, neurons] = mlp_post[:, :, neurons]\n",
    "    mlp_post = mlp_post_mean\n",
    "    mlp_out = einops.einsum(mlp_post, W_out, \"game pos neurons, neurons d_model -> game pos d_model\") + b_out\n",
    "    # print(mlp_out.shape)\n",
    "    resid_post = focus_cache[\"resid_post\", layer]\n",
    "    logits_from_attn = einops.einsum(attn_out, probe, \"game pos d_model, d_model rows cols options -> game pos rows cols options\")\n",
    "    logits_from_mlp = einops.einsum(mlp_out, probe, \"game pos d_model, d_model rows cols options -> game pos rows cols options\")\n",
    "    logits_from_resid_post = einops.einsum(resid_post, probe, \"game pos d_model, d_model rows cols options -> game pos rows cols options\")\n",
    "    tiles_flipped = logits_from_resid_post[:, :, :, :, 0] > logits_from_resid_post[:, :, :, :, 1]\n",
    "    # tiles_flipped = t.ones_like(tiles_flipped)\n",
    "    logits_from_attn = logits_from_attn[tiles_flipped]\n",
    "    logits_from_mlp = logits_from_mlp[tiles_flipped]\n",
    "    # print(logits_from_attn.shape, logits_from_mlp.shape)\n",
    "    sum_mlp_flipped = logits_from_mlp[:, 0].sum()\n",
    "    sum_attn_flipped = logits_from_attn[:, 0].sum()\n",
    "    # sum_mlp_not_flipped = logits_from_mlp[:, :, :, :, 1].sum()\n",
    "    # sum_attn_not_flipped = logits_from_attn[:, :, :, :, 1].sum()\n",
    "    # print(f\"Layer: {layer}, MLP Flipped: {sum_mlp_flipped}, MLP Not Flipped: {sum_mlp_not_flipped}, Attn Flipped: {sum_attn_flipped}, Attn Not Flipped: {sum_attn_not_flipped}\")\n",
    "    # print(sum_mlp_flipped, sum_attn_flipped)\n",
    "    avg_flipped_logit_percent_from_mlp[layer] = sum_mlp_flipped / (sum_mlp_flipped + sum_attn_flipped)\n",
    "    flipped_logit_from_mlp[layer] = sum_mlp_flipped\n",
    "    flipped_logit_from_attn[layer] = sum_attn_flipped\n",
    "\n",
    "# avg_flipped_logit_percent_from_mlp\n",
    "print(flipped_logit_from_mlp)\n",
    "print(flipped_logit_from_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay just using single Neurons doesen't seem to work ... I think all the negative neurons are relevant maybe. Also I don't have the single flipped Neurons and the very negative flipped to flipped neurons\n",
    "'''\n",
    "REAL: tensor([     0.0000,   6352.9609,   2439.9839,   1518.7620,   -814.8992,\n",
    "         -8579.4238, -17324.6836,   -250.1274])\n",
    "PRED: Top 20 Neurons per flipped : tensor([     0.0000,   4387.9111,   5138.4956,   4367.2275,   3122.8298,\n",
    "         -2957.8474, -13642.4121,  -1687.0056])\n",
    "PRED (Avg + 600 extra Neurons ...) tensor([     0.0000,   5157.6152,   4808.4189,   5780.5054,   3884.3809,\n",
    "         -3282.2759, -12740.1523,   -617.6957])\n",
    "\n",
    "[0.0000,   6352.9609,   2439.9839,   1518.7620,   -814.8992, -8579.4238, -17324.6836,   -250.1274]\n",
    "[0.0000,   4387.9111,   5138.4956,   4367.2275,   3122.8298, -2957.8474, -13642.4121,  -1687.0056]\n",
    "[0.0000,   5157.6152,   4808.4189,   5780.5054,   3884.3809, -3282.2759, -12740.1523,   -617.6957]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ACCESIBLE\\n5, 7, 10, 11, 12, 14, 17, 19, 20, 23]\\n662\\ntorch.Size([50, 59, 512])\\ntensor(473.8499, device='cuda:0') tensor(-1889.1267, device='cuda:0')\\n[10, 12, 13, 18, 23, 29, 30, 31, 33, 36]\\n634\\ntorch.Size([50, 59, 512])\\ntensor(2472.4529, device='cuda:0') tensor(-1515.0182, device='cuda:0')\\n[10, 13, 17, 20, 21, 22, 24, 25, 26, 30]\\n630\\ntorch.Size([50, 59, 512])\\ntensor(4428.3608, device='cuda:0') tensor(-1069.9028, device='cuda:0')\\n[2, 3, 6, 8, 9, 20, 21, 22, 23, 28]\\n536\\ntorch.Size([50, 59, 512])\\ntensor(7608.5562, device='cuda:0') tensor(-1671.4114, device='cuda:0')\\n[2, 3, 8, 12, 19, 22, 34, 37, 55, 67]\\n340\\ntorch.Size([50, 59, 512])\\ntensor(5427.8564, device='cuda:0') tensor(-1823.5784, device='cuda:0')\\n[1, 23, 60, 62, 64, 86, 88, 89, 92, 100]\\n219\\ntorch.Size([50, 59, 512])\\ntensor(4537.5273, device='cuda:0') tensor(-3657.9995, device='cuda:0')\\n[3, 4, 13, 19, 23, 42, 55, 59, 66, 75]\\n284\\ntorch.Size([50, 59, 512])\\ntensor(-2168.6604, device='cuda:0') tensor(-12940.5439, device='cuda:0')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ACCESIBLE\n",
    "5, 7, 10, 11, 12, 14, 17, 19, 20, 23]\n",
    "662\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(473.8499, device='cuda:0') tensor(-1889.1267, device='cuda:0')\n",
    "[10, 12, 13, 18, 23, 29, 30, 31, 33, 36]\n",
    "634\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(2472.4529, device='cuda:0') tensor(-1515.0182, device='cuda:0')\n",
    "[10, 13, 17, 20, 21, 22, 24, 25, 26, 30]\n",
    "630\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(4428.3608, device='cuda:0') tensor(-1069.9028, device='cuda:0')\n",
    "[2, 3, 6, 8, 9, 20, 21, 22, 23, 28]\n",
    "536\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(7608.5562, device='cuda:0') tensor(-1671.4114, device='cuda:0')\n",
    "[2, 3, 8, 12, 19, 22, 34, 37, 55, 67]\n",
    "340\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(5427.8564, device='cuda:0') tensor(-1823.5784, device='cuda:0')\n",
    "[1, 23, 60, 62, 64, 86, 88, 89, 92, 100]\n",
    "219\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(4537.5273, device='cuda:0') tensor(-3657.9995, device='cuda:0')\n",
    "[3, 4, 13, 19, 23, 42, 55, 59, 66, 75]\n",
    "284\n",
    "torch.Size([50, 59, 512])\n",
    "tensor(-2168.6604, device='cuda:0') tensor(-12940.5439, device='cuda:0')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Layer 0 - 4 macht MLP Flipped und Attn Not Flipped. Ab Layer 5 ist es ausgeglichen bzw. anders rum\n",
    "* weirdly: Wenn ich nur die flipped neurons benutze, dann funktioniert es bei Layer 1 nicht dafür aber bei 2 bis 6\n",
    "    * Jezt wo ich das threshold runtergesetzt habe geht es gut ...\n",
    "    * ABER wenn ich alle Neurons hinzu nehme ist es nochmal viel höher beei 1 bis 3 und ab Layer 4 geringer ...\n",
    "\n",
    "* Also Neuronen hinzu zu nehmen kann die Logits verringern ... Liegt glaube ich an negativen Aktivierungen ...\n",
    "    * Nope! Wenn ich negative Aktivierungen hochrunde dann gehen die Logits in den Bach\n",
    "\n",
    "* Ich glaube die MLP layer macht so eine Background Strahlung von ein bischen Positivem Flipped\n",
    "    * Wie kann ich das nutzen ...\n",
    "\n",
    "* Mission ist es eine Art Average Logit zu nehmen plus dann die Logits von einer Handvoll Neurons je nach \n",
    "    * Das geht gut: Ich karkuliere das Average MLP out (ohne bias) und kalkuliere das Average mlp_out von jedem Neuron, dann ziehe ich die Average mlp_out der Neurons ab und nehme stattdessen die richtigen Aktivierungen rein (Dann nehme ich aber die Aktivierungen aus dem nichts ...) Wild wäre vorberechnet, die Average Aktivierung wenn halt eine bestimmte Regel aktiv ist ...\n",
    "\n",
    "* Grober Plan: Gucken welche Regel Aktiv ist, dann Handvoll Neurons raussuchen, dann neue Neuron activations rein pluggen und dann bin ich glücklich ...\n",
    "    * Was ist wenn es Neurons gibt, die Flipped schreiben ohne soeine Regel zu befolgen ... Das würde alles zerstören\n",
    "\n",
    "* Der Average Background sollte auf jeden Fall Position abhängig sein\n",
    "* OKAY! Also ich habe mir eine Menge Neurons angeschaut die Tile Flipped schreiben und es gibt ein Paar Arten\n",
    "    * Normale Flipped\n",
    "        * Unterschied: auch Tile MINE und nächstes Tile Yours sind relevant\n",
    "    * Weird (L1N23)\n",
    "    * Aktiviert wenn von verschiedenen Seiten geflipped\n",
    "    * Aktiviert wenn besonders start nicht geflipped ...\n",
    "* TODO: Ich muss überlegen ob das meinen Plan zerstört ...\n",
    "* Ich glaube aber das meiste sollte schon noch von den Dingern kommen ...\n",
    "\n",
    "* Neurons zu random einsetzen klappt nicht!\n",
    "    * Enweder: Falsche Neurons, Ich sollte die nehmen, die am meisten Tile Flipped schreiben ...\n",
    "    * Oder: Viele Neurons, selbst negative sind relevant ...\n",
    "\n",
    "* Ich habe davor voll die Falschen Schlüsse gezogen, (Average nehmen und dann bestimmte Neurons rein )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
