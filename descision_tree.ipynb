{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from create_dataset import get_filtered_dataset\n",
    "from descision_trees import DecisionTree\n",
    "from create_dataset import save_filtered_dataset_for_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big_dataset = pd.read_csv(f\"data/neuron_datasets/logic_L{layer}.csv\")\\nsave_filtered_dataset_for_neurons(big_dataset=big_dataset,dataset_name=\"logic_small\", layer = layer, neurons = [0, 1, 2, 3, 4, 421])\\nbig_dataset = pd.read_csv(f\"data/neuron_datasets/logic_no_softmax_L{layer}.csv\")\\nsave_filtered_dataset_for_neurons(big_dataset=big_dataset,dataset_name=\"logic_no_softmax_small\", layer = layer, neurons = [0, 1, 2, 3, 4, 421])'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''big_dataset = pd.read_csv(f\"data/neuron_datasets/logic_L{layer}.csv\")\n",
    "save_filtered_dataset_for_neurons(big_dataset=big_dataset,dataset_name=\"logic_small\", layer = layer, neurons = [0, 1, 2, 3, 4, 421])\n",
    "big_dataset = pd.read_csv(f\"data/neuron_datasets/logic_no_softmax_L{layer}.csv\")\n",
    "save_filtered_dataset_for_neurons(big_dataset=big_dataset,dataset_name=\"logic_no_softmax_small\", layer = layer, neurons = [0, 1, 2, 3, 4, 421])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron_predictor import NeuronPredictor\n",
    "\n",
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from utils import probe_directions_list, tuple_to_label\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "def get_variable_names():\n",
    "    variable_names = []\n",
    "    for probe_name in [\"linear\", \"flipped\", \"placed\"]:\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                for option in probe_directions_list[probe_name]:\n",
    "                    label = tuple_to_label((row, col))\n",
    "                    variable_names.append(f\"{label} {option}\")\n",
    "    return variable_names\n",
    "\n",
    "def tree_to_cnf(tree, feature_names=None):\n",
    "    recurse_count = 0\n",
    "    def recurse(node, path):\n",
    "        nonlocal recurse_count\n",
    "        recurse_count += 1\n",
    "        if tree.feature[node] != -2:  # not a leaf\n",
    "            feature = feature_names[tree.feature[node]] if feature_names is not None else f\"feature_{tree.feature[node]}\"\n",
    "\n",
    "            if feature[3:].startswith(\"not\"):\n",
    "                left_feature = feature[:3] + feature[7:]\n",
    "                right_feature = feature\n",
    "            else:\n",
    "                left_feature = feature[:3] + \"not \" + feature[3:]\n",
    "                right_feature = feature\n",
    "            \n",
    "            left_path = path + [f\"{left_feature}\"]\n",
    "            right_path = path + [f\"{right_feature}\"]\n",
    "            \n",
    "            yield from recurse(tree.children_left[node], left_path)\n",
    "            yield from recurse(tree.children_right[node], right_path)\n",
    "        else:  # leaf\n",
    "            predicted_value = tree.value[node][0, 0]\n",
    "            yield f\"({' AND '.join(path)} => {predicted_value:.4f})\"\n",
    "\n",
    "    return list(recurse(0, [])), recurse_count\n",
    "\n",
    "def get_accuracy(y_pred, y_test):\n",
    "    correct = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if (y_pred[i] > 0 and y_test[i] > 0) or (y_pred[i] <= 0 and y_test[i] <= 0):\n",
    "            correct += 1\n",
    "    return correct / len(y_test)\n",
    "\n",
    "def get_f1(y_pred, y_test):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_pred[i] > 0 and y_test[i] > 0:\n",
    "            tp += 1\n",
    "        elif y_pred[i] <= 0 and y_test[i] <= 0:\n",
    "            tn += 1\n",
    "        elif y_pred[i] > 0 and y_test[i] <= 0:\n",
    "            fp += 1\n",
    "        elif y_pred[i] <= 0 and y_test[i] > 0:\n",
    "            fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "EPSILON = 0.000001\n",
    "\n",
    "def scorefunction(depth, accuracy):\n",
    "    return math.log(accuracy + EPSILON, 2) * 2**depth\n",
    "\n",
    "class DecisionTree(NeuronPredictor):\n",
    "    def __init__(self, layer, neuron):\n",
    "        self.column_names_input = get_variable_names()\n",
    "        self.neuron = neuron\n",
    "        self.layer = layer\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.regressor.predict(X)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        top_score = -math.inf\n",
    "        # if max_depth is not None:\n",
    "        #    self.max_depth = max_depth\n",
    "        # else:\n",
    "        #     self.max_depth = 3\n",
    "        self.max_depth = 3\n",
    "        self.regressor = DecisionTreeRegressor(random_state = 0, **kwargs)  \n",
    "        self.regressor.fit(X_train, y_train)\n",
    "        # I think high max_depth will be the main limmiting time factor\n",
    "        \"\"\"for max_depth in range(1, 7):\n",
    "            regressor = DecisionTreeRegressor(random_state = 0, max_depth=max_depth)  \n",
    "            regressor.fit(X_train, y_train)\n",
    "            y_pred = regressor.predict(X_test)\n",
    "            # Get the f1 score using a library function\n",
    "            f1 = f1_score(y_test > 0, y_pred > 0)\n",
    "            # f1 = get_f1(y_pred, y_test)\n",
    "            score = scorefunction(max_depth, f1)\n",
    "            if score > top_score:\n",
    "                top_score = score\n",
    "                self.regressor = regressor\n",
    "                self.max_depth = max_depth\n",
    "                if f1 >= 0.95:\n",
    "                    break\"\"\"\n",
    "\n",
    "    def get_clean_format(self, column_names):\n",
    "        if column_names is None:\n",
    "            column_names = self.column_names_input\n",
    "        cnf_rules = tree_to_cnf(self.regressor.tree_, feature_names=column_names)\n",
    "        return cnf_rules\n",
    "\n",
    "    def get_sparcity(self):\n",
    "        return 1 / (self.max_depth * 2**self.max_depth)\n",
    "\n",
    "    def load(self, layer, neuron):\n",
    "        # Save the decision tree to a file\n",
    "        file_path = f\"neuron_predictors/decision_trees/decision_tree_L{layer}_N{neuron}.joblib\"\n",
    "        if os.path.exists(file_path):\n",
    "            self.regressor = joblib.load(file_path)\n",
    "            self.max_depth = self.regressor.get_depth()\n",
    "            self.layer = layer\n",
    "            self.neuron = neuron\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save(self):\n",
    "        file_path = f\"neuron_predictors/decision_trees/decision_tree_L{self.layer}_N{self.neuron}.joblib\"#\n",
    "        # Save the decision tree to a file\n",
    "        joblib.dump(self.regressor, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1_score(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Calculate the f1 score using vecorized operations.\n",
    "    The inputs are continuous, so we need to threshold them at zero.\n",
    "    - for the true positives and false positves I donâ€™t add 1 to the true / false negatives score but N * C where\n",
    "    - N is the neuron activation\n",
    "    - And C is sum(neuron activations) / count(neuron activations)\n",
    "    \"\"\"\n",
    "    C = np.sum(y_test[y_test > 0]) / len(y_test[y_test > 0])\n",
    "    tp = np.sum(np.logical_and(y_pred > 0, y_test > 0) * (y_test * C))\n",
    "    tn = np.sum(np.logical_and(y_pred <= 0, y_test <= 0))\n",
    "    fp = np.sum(np.logical_and(y_pred > 0, y_test <= 0))\n",
    "    fn = np.sum(np.logical_and(y_pred <= 0, y_test > 0) * (y_test * C))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "neuron = 2\n",
    "'''np.random.seed(42)\n",
    "small_dataset_no_softmax = pd.read_csv(f\"data/neuron_datasets/logic_no_softmax_small_L{layer}.csv\")\n",
    "dataset_no_softmax = get_filtered_dataset(small_dataset_no_softmax, layer, neuron, size = 100000, overfitting_strength=None)\n",
    "small_dataset_softmax = pd.read_csv(f\"data/neuron_datasets/logic_small_L{layer}.csv\")\n",
    "np.random.seed(42)\n",
    "dataset_softmax = get_filtered_dataset(small_dataset_softmax, layer, neuron, size = 100000, overfitting_strength=None)\n",
    "dataset_no_softmax.columns = [f\"{col}_no_softmax\" for col in dataset_no_softmax.columns]'''\n",
    "# merge the two datasets the key is the index or neuron activation\n",
    "small_dataset_softmax = pd.read_csv(f\"data/neuron_datasets/logic_small_L{layer}.csv\")\n",
    "dataset = get_filtered_dataset(small_dataset_softmax, layer, neuron, size = 100000, overfitting_strength=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset = dataset_no_softmax\\n# apply softmax to the dataset\\n# create the input tensor \\nin_tensor = t.stack([t.from_numpy(dataset[\"G4 yours_no_softmax\"].values), t.from_numpy(dataset[\"G4 mine_no_softmax\"].values), t.from_numpy(dataset[\"G4 empty_no_softmax\"].values)], dim=1)\\nin_tensor = t.softmax(in_tensor, dim=1)\\nin_tensor = in_tensor[:, 0]\\nin_tensor = in_tensor.numpy()\\ndataset[\"G4 yours\"] = in_tensor\\n\\ncolumns = dataset.columns.tolist()\\ncolumns = columns[:-2] + [columns[-1]] + [columns[-2]]\\ndataset = dataset[columns]\\n# small_dataset_no_softmax[\"L1_N2\"]\\n# small_dataset_softmax[\"L1_N2\"]\\n# dataset[[\"neuron activation\", \"neuron activation_no_softmax\"]]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = pd.merge(dataset_no_softmax, dataset_softmax, on=\"neuron activation\", suffixes=('_no_softmax', '_softmax'))\n",
    "# dataset = pd.concat([dataset_no_softmax, dataset_softmax], axis=1)\n",
    "'''dataset = dataset_no_softmax\n",
    "# apply softmax to the dataset\n",
    "# create the input tensor \n",
    "in_tensor = t.stack([t.from_numpy(dataset[\"G4 yours_no_softmax\"].values), t.from_numpy(dataset[\"G4 mine_no_softmax\"].values), t.from_numpy(dataset[\"G4 empty_no_softmax\"].values)], dim=1)\n",
    "in_tensor = t.softmax(in_tensor, dim=1)\n",
    "in_tensor = in_tensor[:, 0]\n",
    "in_tensor = in_tensor.numpy()\n",
    "dataset[\"G4 yours\"] = in_tensor\n",
    "\n",
    "columns = dataset.columns.tolist()\n",
    "columns = columns[:-2] + [columns[-1]] + [columns[-2]]\n",
    "dataset = dataset[columns]\n",
    "# small_dataset_no_softmax[\"L1_N2\"]\n",
    "# small_dataset_softmax[\"L1_N2\"]\n",
    "# dataset[[\"neuron activation\", \"neuron activation_no_softmax\"]]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset.loc[dataset[\"neuron activation\"] <= 0, \"neuron activation\"] = -1.0\\ndataset.loc[dataset[\"neuron activation\"] > 0, \"neuron activation\"] = 1.0\\nprint(dataset[dataset[\"neuron activation\"] == 1].shape)\\nprint(dataset[dataset[\"neuron activation\"] == -1].shape)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dataset.loc[dataset[\"neuron activation\"] <= 0, \"neuron activation\"] = -1.0\n",
    "dataset.loc[dataset[\"neuron activation\"] > 0, \"neuron activation\"] = 1.0\n",
    "print(dataset[dataset[\"neuron activation\"] == 1].shape)\n",
    "print(dataset[dataset[\"neuron activation\"] == -1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0 empty</th>\n",
       "      <th>A0 yours</th>\n",
       "      <th>A0 mine</th>\n",
       "      <th>A1 empty</th>\n",
       "      <th>A1 yours</th>\n",
       "      <th>A1 mine</th>\n",
       "      <th>A2 empty</th>\n",
       "      <th>A2 yours</th>\n",
       "      <th>A2 mine</th>\n",
       "      <th>A3 empty</th>\n",
       "      <th>...</th>\n",
       "      <th>H3 not_placed</th>\n",
       "      <th>H4 placed</th>\n",
       "      <th>H4 not_placed</th>\n",
       "      <th>H5 placed</th>\n",
       "      <th>H5 not_placed</th>\n",
       "      <th>H6 placed</th>\n",
       "      <th>H6 not_placed</th>\n",
       "      <th>H7 placed</th>\n",
       "      <th>H7 not_placed</th>\n",
       "      <th>neuron activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>5.515071e-08</td>\n",
       "      <td>4.272128e-07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.077296e-11</td>\n",
       "      <td>8.722876e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.453085e-09</td>\n",
       "      <td>2.429726e-09</td>\n",
       "      <td>6.348039e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.822188e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.686365e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.276853e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.235369e-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.014301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.899494e-09</td>\n",
       "      <td>1.056404e-08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.430894e-08</td>\n",
       "      <td>2.618030e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.120000e-07</td>\n",
       "      <td>6.170320e-07</td>\n",
       "      <td>9.999987e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.972168e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.191337e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.735693e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.145722e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.143468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.638960e-07</td>\n",
       "      <td>3.548654e-08</td>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>3.992198e-05</td>\n",
       "      <td>2.661746e-06</td>\n",
       "      <td>9.999574e-01</td>\n",
       "      <td>5.932582e-09</td>\n",
       "      <td>3.731384e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999715e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.239009e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.198147e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.686210e-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.121551e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.111531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.313376e-11</td>\n",
       "      <td>1.511813e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.278454e-12</td>\n",
       "      <td>1.073964e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.828848e-10</td>\n",
       "      <td>5.434128e-12</td>\n",
       "      <td>1.353134e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.686816e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.903713e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.764745e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.050180e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.009904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>5.878171e-08</td>\n",
       "      <td>7.293071e-08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.777942e-08</td>\n",
       "      <td>3.436203e-08</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.457933e-09</td>\n",
       "      <td>1.016457e-07</td>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.180273e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.863364e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.910162e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.458694e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.144023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.135108e-11</td>\n",
       "      <td>2.602362e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.925223e-12</td>\n",
       "      <td>3.116869e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.928693e-13</td>\n",
       "      <td>1.400643e-11</td>\n",
       "      <td>1.454319e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.411708e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.147491e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.414533e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.116944e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.012884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>9.357086e-07</td>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>9.741315e-07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.258117e-11</td>\n",
       "      <td>3.253899e-10</td>\n",
       "      <td>5.547406e-06</td>\n",
       "      <td>6.420652e-07</td>\n",
       "      <td>9.999938e-01</td>\n",
       "      <td>9.999989e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.752856e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.770287e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.864053e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.121384e-13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.031531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>7.648549e-06</td>\n",
       "      <td>1.416934e-07</td>\n",
       "      <td>9.999923e-01</td>\n",
       "      <td>7.337126e-10</td>\n",
       "      <td>5.717401e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>7.683366e-09</td>\n",
       "      <td>1.156250e-07</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.429070e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.397192e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.869549e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.963614e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.037659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.689933e-12</td>\n",
       "      <td>6.320590e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.955564e-12</td>\n",
       "      <td>5.324504e-11</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.138698e-12</td>\n",
       "      <td>5.479517e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.349378e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.897777e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.787989e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.021123e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1.254816e-08</td>\n",
       "      <td>3.004353e-07</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>5.320626e-11</td>\n",
       "      <td>2.025429e-08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.209692e-08</td>\n",
       "      <td>3.307967e-01</td>\n",
       "      <td>6.692033e-01</td>\n",
       "      <td>3.547873e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.931207e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.069069e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.035926e-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.787232e-11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.131848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 449 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           A0 empty      A0 yours       A0 mine      A1 empty      A1 yours  \\\n",
       "0      9.999995e-01  5.515071e-08  4.272128e-07  1.000000e+00  9.077296e-11   \n",
       "1      2.899494e-09  1.056404e-08  1.000000e+00  4.430894e-08  2.618030e-10   \n",
       "2      5.638960e-07  3.548654e-08  9.999994e-01  3.992198e-05  2.661746e-06   \n",
       "3      1.000000e+00  6.313376e-11  1.511813e-10  1.000000e+00  3.278454e-12   \n",
       "4      9.999999e-01  5.878171e-08  7.293071e-08  1.000000e+00  3.777942e-08   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "99995  1.000000e+00  1.135108e-11  2.602362e-10  1.000000e+00  5.925223e-12   \n",
       "99996  9.357086e-07  9.999981e-01  9.741315e-07  1.000000e+00  1.258117e-11   \n",
       "99997  7.648549e-06  1.416934e-07  9.999923e-01  7.337126e-10  5.717401e-09   \n",
       "99998  1.000000e+00  5.689933e-12  6.320590e-12  1.000000e+00  9.955564e-12   \n",
       "99999  1.254816e-08  3.004353e-07  9.999996e-01  5.320626e-11  2.025429e-08   \n",
       "\n",
       "            A1 mine      A2 empty      A2 yours       A2 mine      A3 empty  \\\n",
       "0      8.722876e-11  1.000000e+00  7.453085e-09  2.429726e-09  6.348039e-08   \n",
       "1      1.000000e+00  7.120000e-07  6.170320e-07  9.999987e-01  9.999999e-01   \n",
       "2      9.999574e-01  5.932582e-09  3.731384e-09  1.000000e+00  9.999715e-01   \n",
       "3      1.073964e-10  1.000000e+00  1.828848e-10  5.434128e-12  1.353134e-08   \n",
       "4      3.436203e-08  9.999999e-01  9.457933e-09  1.016457e-07  9.999994e-01   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "99995  3.116869e-12  1.000000e+00  1.928693e-13  1.400643e-11  1.454319e-09   \n",
       "99996  3.253899e-10  5.547406e-06  6.420652e-07  9.999938e-01  9.999989e-01   \n",
       "99997  1.000000e+00  9.999999e-01  7.683366e-09  1.156250e-07  1.000000e+00   \n",
       "99998  5.324504e-11  1.000000e+00  4.138698e-12  5.479517e-12  1.000000e+00   \n",
       "99999  1.000000e+00  1.209692e-08  3.307967e-01  6.692033e-01  3.547873e-10   \n",
       "\n",
       "       ...  H3 not_placed     H4 placed  H4 not_placed     H5 placed  \\\n",
       "0      ...            1.0  5.822188e-09            1.0  3.686365e-09   \n",
       "1      ...            1.0  7.972168e-11            1.0  3.191337e-12   \n",
       "2      ...            1.0  2.239009e-09            1.0  5.198147e-11   \n",
       "3      ...            1.0  7.686816e-12            1.0  5.903713e-11   \n",
       "4      ...            1.0  9.180273e-10            1.0  3.863364e-10   \n",
       "...    ...            ...           ...            ...           ...   \n",
       "99995  ...            1.0  7.411708e-12            1.0  5.147491e-12   \n",
       "99996  ...            1.0  1.752856e-12            1.0  1.770287e-12   \n",
       "99997  ...            1.0  9.429070e-11            1.0  3.397192e-09   \n",
       "99998  ...            1.0  9.349378e-11            1.0  4.897777e-11   \n",
       "99999  ...            1.0  1.931207e-09            1.0  3.069069e-11   \n",
       "\n",
       "       H5 not_placed     H6 placed  H6 not_placed     H7 placed  \\\n",
       "0                1.0  7.276853e-09            1.0  8.235369e-08   \n",
       "1                1.0  1.735693e-12            1.0  2.145722e-10   \n",
       "2                1.0  1.686210e-08            1.0  6.121551e-11   \n",
       "3                1.0  2.764745e-12            1.0  4.050180e-12   \n",
       "4                1.0  1.910162e-11            1.0  2.458694e-10   \n",
       "...              ...           ...            ...           ...   \n",
       "99995            1.0  3.414533e-11            1.0  1.116944e-10   \n",
       "99996            1.0  3.864053e-12            1.0  8.121384e-13   \n",
       "99997            1.0  4.869549e-09            1.0  1.963614e-09   \n",
       "99998            1.0  2.787989e-10            1.0  4.021123e-12   \n",
       "99999            1.0  4.035926e-10            1.0  4.787232e-11   \n",
       "\n",
       "       H7 not_placed  neuron activation  \n",
       "0                1.0          -0.014301  \n",
       "1                1.0          -0.143468  \n",
       "2                1.0          -0.111531  \n",
       "3                1.0          -0.009904  \n",
       "4                1.0          -0.144023  \n",
       "...              ...                ...  \n",
       "99995            1.0          -0.012884  \n",
       "99996            1.0          -0.031531  \n",
       "99997            1.0          -0.037659  \n",
       "99998            1.0          -0.001246  \n",
       "99999            1.0          -0.131848  \n",
       "\n",
       "[100000 rows x 449 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weighted_f1_score(y, y_pred):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(y)):\n",
    "        score = max(y[i], y_pred[i])\n",
    "        if y_pred[i] > 0 and y[i] > 0:\n",
    "            tp += score\n",
    "        elif y_pred[i] <= 0 and y[i] <= 0:\n",
    "            tn += 1\n",
    "        elif y_pred[i] > 0 and y[i] <= 0:\n",
    "            fp += score\n",
    "        elif y_pred[i] <= 0 and y[i] > 0:\n",
    "            fn += score\n",
    "    precision = tp / (tp + fp + EPSILON)\n",
    "    recall = tp / (tp + fn + EPSILON)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + EPSILON)\n",
    "    return f1.item()\n",
    "\n",
    "# print(new_weighted_f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.08795257930150593, 0.19985438660356752, 0.1221492935810435)\n",
      "0.2556439211104805\n",
      "[[-0.16023673 -1.        ]\n",
      " [-0.1352806  -1.        ]\n",
      " [-0.04884819 -1.        ]\n",
      " [-0.15180685 -1.        ]\n",
      " [-0.16963921 -1.        ]\n",
      " [-0.05052318  0.3       ]\n",
      " [-0.04871121 -1.        ]\n",
      " [-0.11899538  0.3       ]\n",
      " [-0.0341941  -1.        ]\n",
      " [-0.16973707  0.3       ]\n",
      " [-0.16640826 -1.        ]\n",
      " [-0.06518679 -1.        ]\n",
      " [-0.1323253  -1.        ]\n",
      " [-0.10429908 -1.        ]\n",
      " [-0.0113814  -1.        ]\n",
      " [-0.1587878  -1.        ]\n",
      " [-0.10987219 -1.        ]\n",
      " [-0.04079624 -1.        ]\n",
      " [-0.16786101 -1.        ]\n",
      " [-0.10607117 -1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999943"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(y_pred, y, threshold):\n",
    "    y_pred_binary = y_pred > threshold\n",
    "    y_binary = y > threshold\n",
    "    return precision_score(y_binary, y_pred_binary), recall_score(y_binary, y_pred_binary), f1_score(y_binary, y_pred_binary)\n",
    "\n",
    "def pred_manual(formula : list[list[str]], X, threshold = 0.9999839067):\n",
    "    y_pred = np.ones((X.shape[0])) * -1\n",
    "    for i in range(len(y_pred)):\n",
    "        for rule in formula:\n",
    "            if all([X[feature][i] > threshold for feature in rule]):\n",
    "                # print(i)\n",
    "                y_pred[i] = 0.3\n",
    "                break\n",
    "    return y_pred\n",
    "\n",
    "column_names = dataset.columns\n",
    "column_names_input = column_names[:-1]\n",
    "column_names_output = [column_names[-1]]\n",
    "X = dataset[column_names_input].astype(float) \n",
    "y = dataset[column_names_output].astype(float)\n",
    "y = y.to_numpy()\n",
    "y_pred = pred_manual([[\"G4 yours\", \"E6 placed\"],\n",
    "                      [\"G4 yours\", \"H3 placed\"],\n",
    "                      [\"G4 yours\", \"D1 placed\"],\n",
    "                      [\"G4 yours\", \"C0 placed\"],\n",
    "                      [\"G4 yours\", \"E2 placed\"],\n",
    "                      [\"G4 yours\", \"G1 placed\"],\n",
    "                      [\"G4 yours\", \"G2 placed\"],\n",
    "                      [\"G4 yours\", \"G2 placed\"],\n",
    "                      [\"G4 yours\", \"G3 placed\"],\n",
    "                      [\"G4 yours\", \"H4 placed\"],\n",
    "                      [\"G4 yours\", \"A4 placed\"],\n",
    "                      [\"G4 yours\", \"B4 placed\"],\n",
    "                      [\"G4 yours\", \"C4 placed\"],\n",
    "                      [\"G4 yours\", \"D7 placed\"],], X, 0.99)[:, None]\n",
    "\n",
    "'''y_pred = pred_manual([[\"G4 yours\", \"E6 placed\"],\n",
    "                      [\"G4 yours\", \"H3 placed\"],\n",
    "                      [\"G4 yours\", \"D1 placed\"],], X, 0.90)[:, None]'''\n",
    "# y_pred = pred_manual([[\"G4 yours\"]], X)[:, None]\n",
    "print(evaluate(y_pred, y, 0))\n",
    "print(new_weighted_f1_score(y, y_pred))\n",
    "compare = np.concatenate([y, y_pred], axis=1)\n",
    "print(compare[:20])\n",
    "# X[\"G4 yours\"][1]\n",
    "# print(X.iloc[5][X.iloc[5] > 0.9])\n",
    "X[\"G4 yours\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "neuron = 421\n",
    "small_dataset_softmax = pd.read_csv(f\"data/neuron_datasets/logic_small_L{layer}.csv\")\n",
    "dataset = get_filtered_dataset(small_dataset_softmax, layer, neuron, size = 100000, overfitting_strength=None)\n",
    "dataset_columns_new = dataset.columns.tolist()\n",
    "dataset_columns_new = [col_name for col_name in dataset_columns_new if col_name[3:6] != \"not\"]\n",
    "dataset = dataset[dataset_columns_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "descision_tree = DecisionTree(layer, neuron)\n",
    "column_names = dataset.columns\n",
    "column_names_input = column_names[:-1]\n",
    "column_names_output = [column_names[-1]]\n",
    "\n",
    "X = dataset[column_names_input].astype(float)\n",
    "y = dataset[column_names_output].astype(float)\n",
    "# turn into numpy arrays\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# descision_tree.fit(X_train, y_train, min_impurity_decrease=0.0001)\n",
    "descision_tree.fit(X_train, y_train, max_depth=3, min_impurity_decrease=0.001)\n",
    "descision_tree.predict(X)\n",
    "y_pred_train = descision_tree.predict(X_train)\n",
    "y_pred_test = descision_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(B4 not placed AND A4 not placed AND B4 not empty => -0.1152)',\n",
       "  '(B4 not placed AND A4 not placed AND B4 empty => -0.0430)',\n",
       "  '(B4 not placed AND A4 placed AND B4 not flipped => -0.0643)',\n",
       "  '(B4 not placed AND A4 placed AND B4 flipped => 1.7161)',\n",
       "  '(B4 placed AND C4 not empty AND B2 not flipped => 1.9735)',\n",
       "  '(B4 placed AND C4 not empty AND B2 flipped => 1.3467)',\n",
       "  '(B4 placed AND C4 empty => -0.1112)'],\n",
       " 13)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descision_tree.get_clean_format(column_names_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "small_dataset_softmax = pd.read_csv(f\"data/neuron_datasets/logic_small_L{layer}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 2 min_impurity_decrease 0.0001\n",
      "Test f1 weighted: 0.960553424317472\n",
      "Test f1: 0.8818737270875764\n"
     ]
    }
   ],
   "source": [
    "# Create a datafram with columns neuron, min_impurity_decrease, f1_weighted, f1, sparcity and fill it with the results\n",
    "# results = pd.DataFrame(columns=[\"neuron\", \"min_impurity_decrease\", \"f1_weighted\", \"f1\", \"sparcity\"])\n",
    "results = {\n",
    "    \"neuron\": [],\n",
    "    \"min_impurity_decrease\": [],\n",
    "    \"f1_weighted\": [],\n",
    "    \"f1\": [],\n",
    "    \"CNF size\" : [],\n",
    "}\n",
    "\n",
    "def get_data(layer, neuron):\n",
    "    small_dataset_softmax = pd.read_csv(f\"data/neuron_datasets/logic_small_L{layer}.csv\")\n",
    "    dataset = get_filtered_dataset(small_dataset_softmax, layer, neuron, size = 100000, overfitting_strength=None)\n",
    "    dataset_columns_new = dataset.columns.tolist()\n",
    "\n",
    "    dataset_columns_new = [col_name for col_name in dataset_columns_new if col_name[3:6] != \"not\"]\n",
    "    dataset = dataset[dataset_columns_new]\n",
    "\n",
    "    column_names = dataset.columns\n",
    "    column_names_input = column_names[:-1]\n",
    "    column_names_output = [column_names[-1]]\n",
    "\n",
    "    X = dataset[column_names_input].astype(float)\n",
    "    y = dataset[column_names_output].astype(float)\n",
    "    # turn into numpy arrays\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_decision_tree(X_train, y_train, layer, neuron, **kwargs) -> DecisionTree:\n",
    "    descision_tree = DecisionTree(layer, neuron)\n",
    "    descision_tree.fit(X_train, y_train, **kwargs)\n",
    "    return descision_tree\n",
    "\n",
    "def evaluate_decision_tree(descision_tree : DecisionTree, X_test, y_test):\n",
    "    y_pred_test = descision_tree.predict(X_test)\n",
    "    weighted_f1 = new_weighted_f1_score(y_test, y_pred_test)\n",
    "    f1 = f1_score(y_test > 0, y_pred_test > 0)\n",
    "    rules, variable_count = descision_tree.get_clean_format(column_names_input)\n",
    "    return weighted_f1, f1, variable_count, rules\n",
    "\n",
    "def train_and_evaluate_decision_tree(layer : int, neuron : int, **kwargs):\n",
    "    X_train, X_test, y_train, y_test = get_data(layer, neuron)\n",
    "    descision_tree = train_decision_tree(X_train, y_train, layer, neuron, **kwargs)\n",
    "    weighted_f1, f1, variable_count, rules = evaluate_decision_tree(descision_tree, X_test, y_test)\n",
    "    return descision_tree, weighted_f1, f1, variable_count, rules\n",
    "\n",
    "neurons = [0, 1, 2, 3, 4, 421]\n",
    "min_impurity_decreases = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "neurons = [2]\n",
    "min_impurity_decreases = [0.0001]\n",
    "\n",
    "for neuron in neurons:\n",
    "    for min_impurity_decrease in min_impurity_decreases:\n",
    "        print(f\"Neuron {neuron} min_impurity_decrease {min_impurity_decrease}\")\n",
    "        descision_tree, weighted_f1, f1, variable_count, rules = train_and_evaluate_decision_tree(layer, neuron, min_impurity_decrease=min_impurity_decrease)\n",
    "        print(f\"Test f1 weighted: {weighted_f1}\")\n",
    "        print(f\"Test f1: {f1}\")\n",
    "        results[\"neuron\"].append(neuron)\n",
    "        results[\"min_impurity_decrease\"].append(min_impurity_decrease)\n",
    "        results[\"f1_weighted\"].append(weighted_f1)\n",
    "        results[\"f1\"].append(f1)\n",
    "        results[\"CNF size\"].append(variable_count)\n",
    "\n",
    "        if variable_count <= 50:\n",
    "            print(\"Rules:\")\n",
    "            print(rules)\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron</th>\n",
       "      <th>min_impurity_decrease</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>f1</th>\n",
       "      <th>CNF size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.645175</td>\n",
       "      <td>0.390738</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.873408</td>\n",
       "      <td>0.681542</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.815964</td>\n",
       "      <td>0.644571</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.904599</td>\n",
       "      <td>0.764595</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.796542</td>\n",
       "      <td>0.544403</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.853097</td>\n",
       "      <td>0.618812</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.907014</td>\n",
       "      <td>0.729126</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.870067</td>\n",
       "      <td>0.758397</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.888239</td>\n",
       "      <td>0.764114</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.907992</td>\n",
       "      <td>0.770221</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.841624</td>\n",
       "      <td>0.595710</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.892585</td>\n",
       "      <td>0.698908</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>421</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.929453</td>\n",
       "      <td>0.902670</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>421</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.994672</td>\n",
       "      <td>0.979788</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>421</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.998835</td>\n",
       "      <td>0.991469</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>421</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>0.991469</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neuron  min_impurity_decrease  f1_weighted        f1  CNF size\n",
       "0        0                0.01000     0.000000  0.000000         1\n",
       "1        0                0.00100     0.000000  0.000000         1\n",
       "2        0                0.00010     0.645175  0.390738        37\n",
       "3        0                0.00001     0.873408  0.681542       311\n",
       "4        1                0.01000     0.000000  0.000000         1\n",
       "5        1                0.00100     0.000000  0.000000         1\n",
       "6        1                0.00010     0.815964  0.644571        25\n",
       "7        1                0.00001     0.904599  0.764595       179\n",
       "8        2                0.01000     0.000000  0.000000         1\n",
       "9        2                0.00100     0.796542  0.544403         7\n",
       "10       2                0.00010     0.853097  0.618812        79\n",
       "11       2                0.00001     0.907014  0.729126       569\n",
       "12       3                0.01000     0.000000  0.000000         1\n",
       "13       3                0.00100     0.870067  0.758397         3\n",
       "14       3                0.00010     0.888239  0.764114        17\n",
       "15       3                0.00001     0.907992  0.770221       177\n",
       "16       4                0.01000     0.000000  0.000000         1\n",
       "17       4                0.00100     0.000000  0.000000         1\n",
       "18       4                0.00010     0.841624  0.595710        41\n",
       "19       4                0.00001     0.892585  0.698908       219\n",
       "20     421                0.01000     0.929453  0.902670         5\n",
       "21     421                0.00100     0.994672  0.979788        15\n",
       "22     421                0.00010     0.998835  0.991469        39\n",
       "23     421                0.00001     0.998724  0.991469       249"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_f1_weighted = results[\"f1_weighted\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron</th>\n",
       "      <th>min_impurity_decrease</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>f1</th>\n",
       "      <th>CNF size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.68</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.54</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.62</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.73</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.77</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.60</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>421</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>421</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>421</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>421</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neuron  min_impurity_decrease  f1_weighted    f1  CNF size\n",
       "0        0                   0.01         0.00  0.00         1\n",
       "1        0                   0.00         0.00  0.00         1\n",
       "2        0                   0.00         0.65  0.39        37\n",
       "3        0                   0.00         0.87  0.68       311\n",
       "4        1                   0.01         0.00  0.00         1\n",
       "5        1                   0.00         0.00  0.00         1\n",
       "6        1                   0.00         0.82  0.64        25\n",
       "7        1                   0.00         0.90  0.76       179\n",
       "8        2                   0.01         0.00  0.00         1\n",
       "9        2                   0.00         0.80  0.54         7\n",
       "10       2                   0.00         0.85  0.62        79\n",
       "11       2                   0.00         0.91  0.73       569\n",
       "12       3                   0.01         0.00  0.00         1\n",
       "13       3                   0.00         0.87  0.76         3\n",
       "14       3                   0.00         0.89  0.76        17\n",
       "15       3                   0.00         0.91  0.77       177\n",
       "16       4                   0.01         0.00  0.00         1\n",
       "17       4                   0.00         0.00  0.00         1\n",
       "18       4                   0.00         0.84  0.60        41\n",
       "19       4                   0.00         0.89  0.70       219\n",
       "20     421                   0.01         0.93  0.90         5\n",
       "21     421                   0.00         0.99  0.98        15\n",
       "22     421                   0.00         1.00  0.99        39\n",
       "23     421                   0.00         1.00  0.99       249"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round every float to 2 decimals\n",
    "results = results.round(2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neuron 0 fuzzy_sparcity low\n",
    "Train f1 wighted: [0.7021354]\n",
    "Test f1 weighted: [0.64517511]\n",
    "Train f1: 0.4029465930018416\n",
    "Test f1: 0.3907380607814761\n",
    "Train accuracy: 0.9797375\n",
    "Test accuracy: 0.97895\n",
    "Neuron 0 fuzzy_sparcity high\n",
    "Train f1 wighted: [0.]\n",
    "Test f1 weighted: [0.]\n",
    "Train f1: 0.0\n",
    "Test f1: 0.0\n",
    "Train accuracy: 0.9747375\n",
    "Test accuracy: 0.9738\n",
    "['( => -0.0450)']\n",
    "Neuron 1 fuzzy_sparcity low\n",
    "Train f1 wighted: [0.8183083]\n",
    "Test f1 weighted: [0.81596413]\n",
    "Train f1: 0.6449451887941535\n",
    "Test f1: 0.6445714285714286\n",
    "Train accuracy: 0.985425\n",
    "Test accuracy: 0.98445\n",
    "Neuron 1 fuzzy_sparcity high\n",
    "Train f1 wighted: [0.]\n",
    "Test f1 weighted: [0.]\n",
    "Train f1: 0.0\n",
    "Test f1: 0.0\n",
    "Train accuracy: 0.975525\n",
    "Test accuracy: 0.9739\n",
    "['( => -0.0645)']\n",
    "Neuron 2 fuzzy_sparcity low\n",
    "Train f1 wighted: [0.87310313]\n",
    "Test f1 weighted: [0.85309672]\n",
    "Train f1: 0.6448377581120944\n",
    "Test f1: 0.6188118811881188\n",
    "Train accuracy: 0.95485\n",
    "Test accuracy: 0.9538\n",
    "Neuron 2 fuzzy_sparcity high\n",
    "Train f1 wighted: [0.79908459]\n",
    "Test f1 weighted: [0.79654233]\n",
    "Train f1: 0.5402757365805865\n",
    "Test f1: 0.5444026612669945\n",
    "Train accuracy: 0.9178875\n",
    "Test accuracy: 0.92125\n",
    "['(G4 not yours AND G4 not yours => -0.0176)', '(G4 not yours AND G4 yours => -0.1014)', '(G4 yours AND E6 not placed => 0.1332)', '(G4 yours AND E6 placed => 0.7973)']\n",
    "Neuron 3 fuzzy_sparcity low\n",
    "Train f1 wighted: [0.88659247]\n",
    "Test f1 weighted: [0.88823854]\n",
    "Train f1: 0.7624224201071662\n",
    "Test f1: 0.7641135184620079\n",
    "Train accuracy: 0.9229625\n",
    "Test accuracy: 0.9227\n",
    "Neuron 3 fuzzy_sparcity high\n",
    "Train f1 wighted: [0.87005267]\n",
    "Test f1 weighted: [0.87006652]\n",
    "Train f1: 0.7576030373008729\n",
    "Test f1: 0.7583965330444203\n",
    "Train accuracy: 0.9225875\n",
    "Test accuracy: 0.92195\n",
    "['(A0 not yours => -0.0982)', '(A0 yours => 0.1340)']\n",
    "Neuron 4 fuzzy_sparcity low\n",
    "Train f1 wighted: [0.83555208]\n",
    "Test f1 weighted: [0.84162394]\n",
    "Train f1: 0.5831896551724138\n",
    "Test f1: 0.5957095709570958\n",
    "Train accuracy: 0.975825\n",
    "Test accuracy: 0.9755\n",
    "Neuron 4 fuzzy_sparcity high\n",
    "Train f1 wighted: [0.]\n",
    "Test f1 weighted: [0.]\n",
    "Train f1: 0.0\n",
    "Test f1: 0.0\n",
    "Train accuracy: 0.9663125\n",
    "Test accuracy: 0.9649\n",
    "['( => -0.0653)']\n",
    "Neuron 421 fuzzy_sparcity low\n",
    "Train f1 wighted: [0.99963442]\n",
    "Test f1 weighted: [0.9988347]\n",
    "Train f1: 0.9902072420860851\n",
    "Test f1: 0.9914691943127962\n",
    "Train accuracy: 0.9994625\n",
    "Test accuracy: 0.99955\n",
    "Neuron 421 fuzzy_sparcity high\n",
    "Train f1 wighted: [0.9959054]\n",
    "Test f1 weighted: [0.99467223]\n",
    "Train f1: 0.9811580882352942\n",
    "Test f1: 0.9797882579403272\n",
    "Train accuracy: 0.998975\n",
    "Test accuracy: 0.99895\n",
    "['(B4 not placed AND A4 not placed AND B4 not empty => -0.1152)', '(B4 not placed AND A4 not placed AND B4 empty => -0.0430)', '(B4 not placed AND A4 placed AND B4 not flipped => -0.0643)', '(B4 not placed AND A4 placed AND B4 flipped AND D1 not flipped => 2.0546)', '(B4 not placed AND A4 placed AND B4 flipped AND D1 flipped => 1.2594)', '(B4 placed AND C4 not empty AND B2 not flipped => 1.9735)', '(B4 placed AND C4 not empty AND B2 flipped => 1.3467)', '(B4 placed AND C4 empty => -0.1112)']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011169868158954016 0.011069918091494232\n",
      "0.046568370980348155 0.04661455556666943\n",
      "0.6957328385899815 0.7069857697283312\n",
      "0.5572065378900446 0.5927331887201736\n",
      "0.6188118811881188 0.6448377581120944\n"
     ]
    }
   ],
   "source": [
    "# The f1 score from N421 is really good but the f1 score from N2 is not so good ...\n",
    "# I couldn't test softmax and no softmax, rounding doesen't do much\n",
    "threshold = 0\n",
    "y_test_binary = y_test > threshold\n",
    "y_pred_test_binary = y_pred_test > threshold\n",
    "y_train_binary = y_train > threshold\n",
    "y_pred_train_binary = y_pred_train > threshold\n",
    "\n",
    "# get the MSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_squared_error(y_test, y_pred_test), mean_squared_error(y_train, y_pred_train))\n",
    "print(mean_absolute_error(y_test, y_pred_test), mean_absolute_error(y_train, y_pred_train))\n",
    "\n",
    "print(precision_score(y_test_binary, y_pred_test_binary), precision_score(y_train_binary, y_pred_train_binary))\n",
    "print(recall_score(y_test_binary, y_pred_test_binary), recall_score(y_train_binary, y_pred_train_binary))\n",
    "print(f1_score(y_test_binary, y_pred_test_binary), f1_score(y_train_binary, y_pred_train_binary))\n",
    "\n",
    "# evaluate(y_pred_test, y_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(G4 not yours AND G4 not yours => -0.0176)',\n",
       " '(G4 not yours AND G4 yours AND B4 not placed AND G4 not yours => -0.1154)',\n",
       " '(G4 not yours AND G4 yours AND B4 not placed AND G4 yours AND E2 not placed AND D7 not placed AND D1 not placed => -0.0728)',\n",
       " '(G4 not yours AND G4 yours AND B4 not placed AND G4 yours AND E2 not placed AND D7 not placed AND D1 placed => 0.2848)',\n",
       " '(G4 not yours AND G4 yours AND B4 not placed AND G4 yours AND E2 not placed AND D7 placed => 0.2406)',\n",
       " '(G4 not yours AND G4 yours AND B4 not placed AND G4 yours AND E2 placed => 0.5190)',\n",
       " '(G4 not yours AND G4 yours AND B4 placed AND G4 not yours => -0.0403)',\n",
       " '(G4 not yours AND G4 yours AND B4 placed AND G4 yours => 0.2309)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 not placed AND C0 not placed AND H5 not yours => -0.0332)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 not placed AND C0 not placed AND H5 yours AND G4 not flipped AND H5 not placed => 0.0868)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 not placed AND C0 not placed AND H5 yours AND G4 not flipped AND H5 placed AND G4 not mine => 0.5535)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 not placed AND C0 not placed AND H5 yours AND G4 not flipped AND H5 placed AND G4 mine => 0.0426)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 not placed AND C0 not placed AND H5 yours AND G4 flipped => -0.1048)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 not placed AND C0 placed => 0.5363)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 placed AND G4 not flipped => 0.5183)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 not placed AND G3 placed AND G4 flipped => -0.1210)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 not placed AND B4 placed => 0.5200)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 not placed AND C4 placed => 0.4901)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 not placed AND G1 placed => 0.4742)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 not placed AND G2 placed => 0.4092)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 placed AND E6 not flipped => 0.0647)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 not yours AND D7 placed AND E6 flipped => 0.6918)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 not placed AND C0 not placed AND H5 not yours AND G2 not flipped => 0.0421)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 not placed AND C0 not placed AND H5 not yours AND G2 flipped AND H3 not placed => 0.0446)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 not placed AND C0 not placed AND H5 not yours AND G2 flipped AND H3 placed => 1.3333)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 not placed AND C0 not placed AND H5 yours AND G4 not mine => 0.3228)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 not placed AND C0 not placed AND H5 yours AND G4 mine => 0.0354)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 not placed AND C0 placed => 0.6933)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 not placed AND C4 placed => 0.8935)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 not placed AND B4 placed => 0.8753)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 not flipped AND D7 placed => 0.7741)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 flipped AND F4 not flipped AND G4 not mine AND G5 not empty => 0.7168)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 flipped AND F4 not flipped AND G4 not mine AND G5 empty => 1.1946)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 flipped AND F4 not flipped AND G4 mine => 0.3213)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 not placed AND H3 yours AND G3 flipped AND F4 flipped => 0.1228)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 placed AND E2 not flipped => 0.1848)',\n",
       " '(G4 yours AND E6 not placed AND E2 not placed AND D1 placed AND E2 flipped => 0.9364)',\n",
       " '(G4 yours AND E6 not placed AND E2 placed => 0.9366)',\n",
       " '(G4 yours AND E6 placed AND F5 not flipped => 0.6398)',\n",
       " '(G4 yours AND E6 placed AND F5 flipped => 1.2420)']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descision_tree.get_clean_format(column_names = column_names_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damn this operation is fucking slow\n",
    "# I could do it on the gpu ...\n",
    "# weighted_f1_score(y_pred_train > 0, y_train > 0), weighted_f1_score(y_pred_test > 0, y_test > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H7 not_placed'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_text\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming self.regressor is your trained DecisionTreeRegressor\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tree_rules \u001b[38;5;241m=\u001b[39m export_text(descision_tree\u001b[38;5;241m.\u001b[39mregressor, feature_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m), decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree_rules)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "# Assuming self.regressor is your trained DecisionTreeRegressor\n",
    "tree_rules = export_text(descision_tree.regressor, feature_names=list(X.columns), decimals=10)\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(G4 not yours AND G4 not yours => -0.0176)',\n",
       " '(G4 not yours AND G4 yours => -0.1014)',\n",
       " '(G4 yours AND E6 placed => 0.7973)',\n",
       " '(G4 yours AND E6 not_placed => 0.1332)']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descision_tree.get_clean_format(column_names = column_names_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A0 empty_no_softmax', 'A0 yours_no_softmax', 'A0 mine_no_softmax',\n",
       "       'A1 empty_no_softmax', 'A1 yours_no_softmax', 'A1 mine_no_softmax',\n",
       "       'A2 empty_no_softmax', 'A2 yours_no_softmax', 'A2 mine_no_softmax',\n",
       "       'A3 empty_no_softmax',\n",
       "       ...\n",
       "       'H3 not_placed', 'H4 placed', 'H4 not_placed', 'H5 placed',\n",
       "       'H5 not_placed', 'H6 placed', 'H6 not_placed', 'H7 placed',\n",
       "       'H7 not_placed', 'neuron activation'],\n",
       "      dtype='object', length=898)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12.443144\n",
       "1         1.525594\n",
       "2        -6.436412\n",
       "3        -6.848009\n",
       "4        -8.937505\n",
       "           ...    \n",
       "99995    11.221716\n",
       "99996    -8.789615\n",
       "99997    -9.806160\n",
       "99998   -14.874847\n",
       "99999    -8.583365\n",
       "Name: G4 empty_no_softmax, Length: 100000, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(3 * 64 + 4 * 64) + 2\n",
    "dataset[\"G4 empty_no_softmax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAZIT\n",
    "# Theorie das das Neuron groÃŸe aktivierungen von G4 yours detected ist falsch. Runden von neuron outputs macht einen kelinen unterschied.\n",
    "# TODO:\n",
    "# Back to basics (mit focus cache arbeiten)\n",
    "# Nochmal max activating examples anschauen ???\n",
    "# Ist neuron aktiviert, wenn bestimmte andere neurons aktiviert sind ???\n",
    "# Probes trainieren ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G4 yours</th>\n",
       "      <th>G4 placed_no_softmax</th>\n",
       "      <th>G4 yours_no_softmax</th>\n",
       "      <th>G4 empty_no_softmax</th>\n",
       "      <th>G4 mine_no_softmax</th>\n",
       "      <th>neuron activation_no_softmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-13.26</td>\n",
       "      <td>9.13</td>\n",
       "      <td>-6.85</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.47</td>\n",
       "      <td>13.10</td>\n",
       "      <td>-8.44</td>\n",
       "      <td>6.97</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.75</td>\n",
       "      <td>10.84</td>\n",
       "      <td>-4.04</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.94</td>\n",
       "      <td>-12.10</td>\n",
       "      <td>6.97</td>\n",
       "      <td>-7.75</td>\n",
       "      <td>4.13</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.05</td>\n",
       "      <td>10.19</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>3.17</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.93</td>\n",
       "      <td>-13.52</td>\n",
       "      <td>6.88</td>\n",
       "      <td>-12.15</td>\n",
       "      <td>4.34</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.94</td>\n",
       "      <td>-13.66</td>\n",
       "      <td>10.11</td>\n",
       "      <td>-9.67</td>\n",
       "      <td>7.30</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.99</td>\n",
       "      <td>-11.07</td>\n",
       "      <td>10.45</td>\n",
       "      <td>-12.70</td>\n",
       "      <td>5.50</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-12.17</td>\n",
       "      <td>8.90</td>\n",
       "      <td>-9.54</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-13.10</td>\n",
       "      <td>9.32</td>\n",
       "      <td>-5.77</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.55</td>\n",
       "      <td>-11.76</td>\n",
       "      <td>4.63</td>\n",
       "      <td>-12.01</td>\n",
       "      <td>4.44</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.99</td>\n",
       "      <td>-9.20</td>\n",
       "      <td>10.74</td>\n",
       "      <td>-10.16</td>\n",
       "      <td>5.76</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.99</td>\n",
       "      <td>-11.46</td>\n",
       "      <td>11.05</td>\n",
       "      <td>-9.60</td>\n",
       "      <td>6.28</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-12.97</td>\n",
       "      <td>13.70</td>\n",
       "      <td>-7.97</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-11.77</td>\n",
       "      <td>11.07</td>\n",
       "      <td>-10.89</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-11.47</td>\n",
       "      <td>13.51</td>\n",
       "      <td>-7.83</td>\n",
       "      <td>4.20</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.00</td>\n",
       "      <td>11.70</td>\n",
       "      <td>12.58</td>\n",
       "      <td>-5.23</td>\n",
       "      <td>-3.36</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.74</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>9.47</td>\n",
       "      <td>-10.68</td>\n",
       "      <td>8.45</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-9.13</td>\n",
       "      <td>11.28</td>\n",
       "      <td>-10.82</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-11.38</td>\n",
       "      <td>9.63</td>\n",
       "      <td>-7.15</td>\n",
       "      <td>3.09</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>15.74</td>\n",
       "      <td>-13.12</td>\n",
       "      <td>4.80</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-11.37</td>\n",
       "      <td>12.34</td>\n",
       "      <td>-10.69</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.82</td>\n",
       "      <td>-14.38</td>\n",
       "      <td>6.24</td>\n",
       "      <td>-8.09</td>\n",
       "      <td>4.70</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.96</td>\n",
       "      <td>-11.58</td>\n",
       "      <td>6.95</td>\n",
       "      <td>-8.95</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.98</td>\n",
       "      <td>-10.71</td>\n",
       "      <td>8.19</td>\n",
       "      <td>-7.80</td>\n",
       "      <td>4.34</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.52</td>\n",
       "      <td>-10.87</td>\n",
       "      <td>8.99</td>\n",
       "      <td>-8.94</td>\n",
       "      <td>8.90</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.80</td>\n",
       "      <td>-11.96</td>\n",
       "      <td>8.24</td>\n",
       "      <td>-9.81</td>\n",
       "      <td>6.85</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-11.58</td>\n",
       "      <td>5.94</td>\n",
       "      <td>-8.85</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.58</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>7.84</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>7.52</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.94</td>\n",
       "      <td>-12.15</td>\n",
       "      <td>13.16</td>\n",
       "      <td>-11.16</td>\n",
       "      <td>10.44</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-9.53</td>\n",
       "      <td>14.00</td>\n",
       "      <td>-10.87</td>\n",
       "      <td>2.81</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.93</td>\n",
       "      <td>-9.53</td>\n",
       "      <td>12.72</td>\n",
       "      <td>-13.18</td>\n",
       "      <td>10.15</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.82</td>\n",
       "      <td>-10.96</td>\n",
       "      <td>3.84</td>\n",
       "      <td>-5.54</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.97</td>\n",
       "      <td>-11.85</td>\n",
       "      <td>10.72</td>\n",
       "      <td>-8.51</td>\n",
       "      <td>7.33</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.57</td>\n",
       "      <td>-11.91</td>\n",
       "      <td>8.33</td>\n",
       "      <td>-13.13</td>\n",
       "      <td>8.05</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.03</td>\n",
       "      <td>11.95</td>\n",
       "      <td>-12.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.99</td>\n",
       "      <td>-10.62</td>\n",
       "      <td>8.19</td>\n",
       "      <td>-6.96</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-11.96</td>\n",
       "      <td>14.36</td>\n",
       "      <td>-14.73</td>\n",
       "      <td>5.87</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-13.75</td>\n",
       "      <td>9.21</td>\n",
       "      <td>-4.66</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.99</td>\n",
       "      <td>-10.56</td>\n",
       "      <td>7.99</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>3.06</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.00</td>\n",
       "      <td>8.70</td>\n",
       "      <td>18.25</td>\n",
       "      <td>-6.42</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.99</td>\n",
       "      <td>-10.77</td>\n",
       "      <td>12.13</td>\n",
       "      <td>-8.31</td>\n",
       "      <td>7.88</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.83</td>\n",
       "      <td>-11.46</td>\n",
       "      <td>8.75</td>\n",
       "      <td>-10.99</td>\n",
       "      <td>7.17</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.95</td>\n",
       "      <td>15.45</td>\n",
       "      <td>-13.36</td>\n",
       "      <td>5.58</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.89</td>\n",
       "      <td>6.18</td>\n",
       "      <td>-6.68</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.94</td>\n",
       "      <td>-9.55</td>\n",
       "      <td>7.03</td>\n",
       "      <td>-7.44</td>\n",
       "      <td>4.26</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>18.31</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.98</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>10.19</td>\n",
       "      <td>-7.34</td>\n",
       "      <td>6.18</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-10.20</td>\n",
       "      <td>14.51</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>6.82</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-14.26</td>\n",
       "      <td>11.34</td>\n",
       "      <td>-10.42</td>\n",
       "      <td>3.06</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     G4 yours  G4 placed_no_softmax  G4 yours_no_softmax  G4 empty_no_softmax  \\\n",
       "3        1.00                -13.26                 9.13                -6.85   \n",
       "8        1.00                -10.47                13.10                -8.44   \n",
       "9        1.00                -10.75                10.84                -4.04   \n",
       "11       0.94                -12.10                 6.97                -7.75   \n",
       "12       1.00                -10.05                10.19                -8.01   \n",
       "13       0.93                -13.52                 6.88               -12.15   \n",
       "16       0.94                -13.66                10.11                -9.67   \n",
       "20       0.99                -11.07                10.45               -12.70   \n",
       "21       1.00                -12.17                 8.90                -9.54   \n",
       "25       1.00                -13.10                 9.32                -5.77   \n",
       "28       0.55                -11.76                 4.63               -12.01   \n",
       "35       0.99                 -9.20                10.74               -10.16   \n",
       "36       0.99                -11.46                11.05                -9.60   \n",
       "39       1.00                -12.97                13.70                -7.97   \n",
       "49       1.00                -11.77                11.07               -10.89   \n",
       "50       1.00                -11.47                13.51                -7.83   \n",
       "51       1.00                 11.70                12.58                -5.23   \n",
       "53       0.74                -12.00                 9.47               -10.68   \n",
       "59       1.00                 -9.13                11.28               -10.82   \n",
       "63       1.00                -11.38                 9.63                -7.15   \n",
       "66       1.00                 -9.41                15.74               -13.12   \n",
       "67       1.00                -11.37                12.34               -10.69   \n",
       "68       0.82                -14.38                 6.24                -8.09   \n",
       "69       0.96                -11.58                 6.95                -8.95   \n",
       "78       0.98                -10.71                 8.19                -7.80   \n",
       "79       0.52                -10.87                 8.99                -8.94   \n",
       "80       0.80                -11.96                 8.24                -9.81   \n",
       "81       1.00                -11.58                 5.94                -8.85   \n",
       "85       0.58                 -9.28                 7.84                -5.31   \n",
       "86       0.94                -12.15                13.16               -11.16   \n",
       "87       1.00                 -9.53                14.00               -10.87   \n",
       "90       0.93                 -9.53                12.72               -13.18   \n",
       "94       0.82                -10.96                 3.84                -5.54   \n",
       "97       0.97                -11.85                10.72                -8.51   \n",
       "102      0.57                -11.91                 8.33               -13.13   \n",
       "106      1.00                -10.03                11.95               -12.12   \n",
       "114      0.99                -10.62                 8.19                -6.96   \n",
       "115      1.00                -11.96                14.36               -14.73   \n",
       "118      1.00                -13.75                 9.21                -4.66   \n",
       "119      0.99                -10.56                 7.99                -8.71   \n",
       "122      1.00                  8.70                18.25                -6.42   \n",
       "123      0.99                -10.77                12.13                -8.31   \n",
       "126      0.83                -11.46                 8.75               -10.99   \n",
       "140      1.00                -10.95                15.45               -13.36   \n",
       "141      1.00                -10.89                 6.18                -6.68   \n",
       "142      0.94                 -9.55                 7.03                -7.44   \n",
       "145      1.00                  9.89                18.31                -7.99   \n",
       "152      0.98                 -8.40                10.19                -7.34   \n",
       "157      1.00                -10.20                14.51               -12.40   \n",
       "159      1.00                -14.26                11.34               -10.42   \n",
       "\n",
       "     G4 mine_no_softmax  neuron activation_no_softmax  \n",
       "3                  0.46                         -0.16  \n",
       "8                  6.97                         -0.14  \n",
       "9                 -1.96                         -0.07  \n",
       "11                 4.13                         -0.17  \n",
       "12                 3.17                         -0.10  \n",
       "13                 4.34                         -0.17  \n",
       "16                 7.30                         -0.16  \n",
       "20                 5.50                         -0.14  \n",
       "21                 0.72                         -0.17  \n",
       "25                 1.94                         -0.16  \n",
       "28                 4.44                         -0.13  \n",
       "35                 5.76                         -0.16  \n",
       "36                 6.28                         -0.16  \n",
       "39                 0.14                         -0.07  \n",
       "49                 1.60                         -0.17  \n",
       "50                 4.20                         -0.03  \n",
       "51                -3.36                         -0.03  \n",
       "53                 8.45                         -0.13  \n",
       "59                 3.89                         -0.12  \n",
       "63                 3.09                         -0.17  \n",
       "66                 4.80                         -0.08  \n",
       "67                 0.96                         -0.13  \n",
       "68                 4.70                         -0.12  \n",
       "69                 3.75                         -0.17  \n",
       "78                 4.34                         -0.17  \n",
       "79                 8.90                         -0.17  \n",
       "80                 6.85                         -0.17  \n",
       "81                -0.11                         -0.17  \n",
       "85                 7.52                         -0.12  \n",
       "86                10.44                         -0.16  \n",
       "87                 2.81                         -0.11  \n",
       "90                10.15                         -0.17  \n",
       "94                 2.33                         -0.10  \n",
       "97                 7.33                         -0.15  \n",
       "102                8.05                         -0.11  \n",
       "106               -0.12                         -0.17  \n",
       "114                4.00                         -0.16  \n",
       "115                5.87                         -0.17  \n",
       "118                0.86                         -0.16  \n",
       "119                3.06                         -0.16  \n",
       "122                1.43                         -0.06  \n",
       "123                7.88                         -0.02  \n",
       "126                7.17                         -0.17  \n",
       "140                5.58                         -0.16  \n",
       "141                0.73                         -0.16  \n",
       "142                4.26                         -0.17  \n",
       "145               -0.17                         -0.11  \n",
       "152                6.18                         -0.17  \n",
       "157                6.82                         -0.08  \n",
       "159                3.06                         -0.03  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N2 looks very hard on the visualization like it detects G4 Yours\n",
    "# When the neuron activates highly, G4 is always yours\n",
    "# sometimes when G4 is Yours the neuron doesen't activate\n",
    "# apperantly also it can be sligthly activated when G4 is there This is more rare though and the activation is small\n",
    "# I could imagine that we are missing inputs such as previous neurons activating, positions and other probes ...\n",
    "# I could also imagine that this get's distorted by softmax (doesen't really make sense to use this nonlinearity, when the pre neuron activation is linear)\n",
    "# It probably activates for large dot product with G4 yours, put the probe can also detect small cosine similiarities\n",
    "# => Try out using the pre softmax activations\n",
    "dataset_round = dataset.round(2)\n",
    "# dataset_round = dataset_round[[\"G4 yours\", \"G4 yours_no_softmax\", \"G4 empty\", \"G4 mine\", \"G4 placed\", \"neuron activation\"]]\n",
    "dataset_round = dataset_round[[\"G4 yours\", \"G4 placed_no_softmax\", \"G4 yours_no_softmax\", \"G4 empty_no_softmax\", \"G4 mine_no_softmax\", \"neuron activation_no_softmax\"]]\n",
    "# dataset_round = dataset_round[dataset_round[\"neuron activation\"] > 0]\n",
    "dataset_round = dataset_round[dataset_round[\"G4 yours\"] > 0.5]\n",
    "dataset_round = dataset_round[dataset_round[\"neuron activation_no_softmax\"] < 0]\n",
    "# dataset_round = dataset_round[(dataset_round[\"neuron activation\"] > 0) & (dataset_round[\"G4 yours\"] < 0.5)]\n",
    "dataset_round[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>-0.028999</td>\n",
       "      <td>-0.057998</td>\n",
       "      <td>-0.115997</td>\n",
       "      <td>-0.231993</td>\n",
       "      <td>-0.463986</td>\n",
       "      <td>-0.927972</td>\n",
       "      <td>-1.855945</td>\n",
       "      <td>-3.711890</td>\n",
       "      <td>-7.423780</td>\n",
       "      <td>-14.847559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>-0.148001</td>\n",
       "      <td>-0.296002</td>\n",
       "      <td>-0.592005</td>\n",
       "      <td>-1.184009</td>\n",
       "      <td>-2.368019</td>\n",
       "      <td>-4.736037</td>\n",
       "      <td>-9.472074</td>\n",
       "      <td>-18.944149</td>\n",
       "      <td>-37.888298</td>\n",
       "      <td>-75.776595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>-0.304006</td>\n",
       "      <td>-0.608012</td>\n",
       "      <td>-1.216025</td>\n",
       "      <td>-2.432049</td>\n",
       "      <td>-4.864099</td>\n",
       "      <td>-9.728198</td>\n",
       "      <td>-19.456396</td>\n",
       "      <td>-38.912792</td>\n",
       "      <td>-77.825584</td>\n",
       "      <td>-155.651168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>-0.468931</td>\n",
       "      <td>-0.937861</td>\n",
       "      <td>-1.875722</td>\n",
       "      <td>-3.751444</td>\n",
       "      <td>-7.502888</td>\n",
       "      <td>-15.005776</td>\n",
       "      <td>-30.011552</td>\n",
       "      <td>-60.023105</td>\n",
       "      <td>-120.046210</td>\n",
       "      <td>-240.092420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>-0.643856</td>\n",
       "      <td>-1.287712</td>\n",
       "      <td>-2.575425</td>\n",
       "      <td>-5.150850</td>\n",
       "      <td>-10.301699</td>\n",
       "      <td>-20.603398</td>\n",
       "      <td>-41.206796</td>\n",
       "      <td>-82.413592</td>\n",
       "      <td>-164.827185</td>\n",
       "      <td>-329.654369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>-0.830075</td>\n",
       "      <td>-1.660150</td>\n",
       "      <td>-3.320300</td>\n",
       "      <td>-6.640600</td>\n",
       "      <td>-13.281200</td>\n",
       "      <td>-26.562400</td>\n",
       "      <td>-53.124800</td>\n",
       "      <td>-106.249600</td>\n",
       "      <td>-212.499200</td>\n",
       "      <td>-424.998399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>-32.000000</td>\n",
       "      <td>-64.000000</td>\n",
       "      <td>-128.000000</td>\n",
       "      <td>-256.000000</td>\n",
       "      <td>-512.000000</td>\n",
       "      <td>-1024.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         2         3          4          5          6   \\\n",
       "0.99 -0.028999 -0.057998 -0.115997  -0.231993  -0.463986  -0.927972   \n",
       "0.95 -0.148001 -0.296002 -0.592005  -1.184009  -2.368019  -4.736037   \n",
       "0.90 -0.304006 -0.608012 -1.216025  -2.432049  -4.864099  -9.728198   \n",
       "0.85 -0.468931 -0.937861 -1.875722  -3.751444  -7.502888 -15.005776   \n",
       "0.80 -0.643856 -1.287712 -2.575425  -5.150850 -10.301699 -20.603398   \n",
       "0.75 -0.830075 -1.660150 -3.320300  -6.640600 -13.281200 -26.562400   \n",
       "0.50 -2.000000 -4.000000 -8.000000 -16.000000 -32.000000 -64.000000   \n",
       "\n",
       "              7           8           9            10  \n",
       "0.99   -1.855945   -3.711890   -7.423780   -14.847559  \n",
       "0.95   -9.472074  -18.944149  -37.888298   -75.776595  \n",
       "0.90  -19.456396  -38.912792  -77.825584  -155.651168  \n",
       "0.85  -30.011552  -60.023105 -120.046210  -240.092420  \n",
       "0.80  -41.206796  -82.413592 -164.827185  -329.654369  \n",
       "0.75  -53.124800 -106.249600 -212.499200  -424.998399  \n",
       "0.50 -128.000000 -256.000000 -512.000000 -1024.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "accuracies.reverse()\n",
    "depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "def scorefunction(depth, accuracy):\n",
    "    return math.log(accuracy, 2) * 2**depth\n",
    "\n",
    "scores = []\n",
    "for accuracy in accuracies:\n",
    "    scores += [[scorefunction(depth, accuracy) for depth in depths]]\n",
    "\n",
    "# Turn this into a nice table\n",
    "pd.DataFrame(scores, index=accuracies, columns=depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print the Tree Rules\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_text\n\u001b[0;32m----> 3\u001b[0m r \u001b[38;5;241m=\u001b[39m export_text(\u001b[43mregressor\u001b[49m, feature_names\u001b[38;5;241m=\u001b[39mcolumn_names_input)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(r)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# (C4 Not Flipped AND A4 Placed) OR (C4 Flipped AND B4 Yours)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# (C4 Not Flipped AND A4 Placed) => High Activation, (C4 Flipped AND B4 Yours) => very High Activation, (C4 Flipped AND B4 Not Yours) => very Low Activation (This form is really nice)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regressor' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the Tree Rules\n",
    "from sklearn.tree import export_text\n",
    "r = export_text(regressor, feature_names=column_names_input)\n",
    "print(r)\n",
    "# (C4 Not Flipped AND A4 Placed) OR (C4 Flipped AND B4 Yours)\n",
    "# (C4 Not Flipped AND A4 Placed) => High Activation, (C4 Flipped AND B4 Yours) => very High Activation, (C4 Flipped AND B4 Not Yours) => very Low Activation (This form is really nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_input = column_names_input.tolist()\n",
    "\n",
    "A4_placed = column_names_input.index(\"A4 placed\")\n",
    "B4_flipped = column_names_input.index(\"B4 yours\")\n",
    "B4_placed = column_names_input.index(\"B4 placed\")\n",
    "C4_flipped = column_names_input.index(\"C4 yours\")\n",
    "# First Check Manually\n",
    "# Create y_pred, such that it is 1 if (A4 Placed > 0.5 AND B4 Flipped > 0.5) OR (B4 Placed > 0.5 AND C4 Flipped > 0.5) otherwise -1\n",
    "y_pred = np.zeros(y_test.shape)\n",
    "for i in range(len(y_test)):\n",
    "    if (X_test[i][A4_placed] > 0.5 and X_test[i][B4_flipped] > 0.5) or (X_test[i][B4_placed] > 0.5 and X_test[i][C4_flipped] > 0.5):\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A4 placed</th>\n",
       "      <th>B4 yours</th>\n",
       "      <th>B4 placed</th>\n",
       "      <th>C4 yours</th>\n",
       "      <th>neuron activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A4 placed  B4 yours  B4 placed  C4 yours  neuron activation\n",
       "0         0.0      0.02        0.0      1.00              -0.16\n",
       "1         0.0      0.85        0.0      1.00              -0.14\n",
       "2         0.0      0.00        0.0      0.00              -0.05\n",
       "3         0.0      0.97        0.0      0.01              -0.15\n",
       "4         0.0      0.00        0.0      0.00              -0.17\n",
       "5         0.0      0.00        0.0      0.39              -0.05\n",
       "6         0.0      0.00        0.0      0.01              -0.05\n",
       "7         0.0      0.99        0.0      0.54              -0.12\n",
       "8         0.0      0.00        0.0      0.00              -0.03\n",
       "9         0.0      0.00        0.0      0.00              -0.17\n",
       "10        0.0      0.05        0.0      0.03              -0.17\n",
       "11        0.0      1.00        0.0      0.92              -0.07\n",
       "12        0.0      0.00        0.0      0.00              -0.13\n",
       "13        0.0      1.00        0.0      1.00              -0.10\n",
       "14        0.0      0.00        0.0      0.00              -0.01\n",
       "15        0.0      1.00        0.0      0.00              -0.16\n",
       "16        0.0      1.00        0.0      0.36              -0.11\n",
       "17        0.0      0.00        0.0      1.00              -0.04\n",
       "18        0.0      0.00        0.0      0.77              -0.17\n",
       "19        0.0      0.37        0.0      0.25              -0.11\n",
       "20        0.0      0.00        0.0      0.00              -0.14\n",
       "21        0.0      1.00        0.0      0.00              -0.17\n",
       "22        0.0      0.00        0.0      0.00              -0.16\n",
       "23        0.0      0.00        0.0      0.00              -0.02\n",
       "24        0.0      0.00        0.0      1.00              -0.13\n",
       "25        0.0      0.00        0.0      0.00              -0.02\n",
       "26        0.0      1.00        0.0      0.99              -0.07\n",
       "27        0.0      0.00        0.0      0.02              -0.12\n",
       "28        0.0      0.00        0.0      1.00              -0.03\n",
       "29        0.0      1.00        0.0      0.00              -0.11\n",
       "30        0.0      0.00        0.0      0.00              -0.02\n",
       "31        0.0      0.92        0.0      0.29              -0.08\n",
       "32        0.0      0.00        0.0      0.00              -0.02\n",
       "33        0.0      0.00        0.0      0.00              -0.04\n",
       "34        0.0      0.50        0.0      0.95              -0.11\n",
       "35        0.0      0.00        0.0      0.39              -0.15\n",
       "36        0.0      0.03        0.0      0.98              -0.12\n",
       "37        0.0      0.91        0.0      1.00              -0.09\n",
       "38        0.0      0.00        0.0      0.00              -0.00\n",
       "39        0.0      0.00        0.0      0.00              -0.01\n",
       "40        0.0      0.99        0.0      0.00              -0.13\n",
       "41        0.0      0.00        0.0      0.62              -0.11\n",
       "42        0.0      0.35        0.0      0.31              -0.13\n",
       "43        0.0      0.01        0.0      0.95              -0.16\n",
       "44        0.0      0.95        0.0      0.06              -0.12\n",
       "45        0.0      0.92        0.0      0.02              -0.15\n",
       "46        0.0      0.98        0.0      0.38              -0.10\n",
       "47        0.0      1.00        0.0      1.00              -0.11\n",
       "48        0.0      0.16        0.0      1.00              -0.07\n",
       "49        1.0      1.00        0.0      0.93               2.36"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'ts very weird, that B4 seems to be flipped and placed at the same time. This should not be possible.\n",
    "# round the dataset to 2 decimal places\n",
    "dataset = dataset.round(2)\n",
    "dataset[[\"A4 placed\", \"B4 yours\", \"B4 placed\", \"C4 yours\", \"neuron activation\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 empty: -0.33139568567276\n",
      "A0 yours: 1.8200362920761108\n",
      "A1 empty: 0.7885934710502625\n",
      "A1 yours: -1.0049554109573364\n",
      "B0 empty: -0.7153743505477905\n",
      "B0 yours: -1.7870670557022095\n",
      "B1 empty: -0.987899124622345\n",
      "B1 yours: -1.0271559953689575\n",
      "A0 flipped: 0.02292773686349392\n",
      "A0 not_flipped: 1.3866825103759766\n",
      "A1 flipped: 0.20812645554542542\n",
      "A1 not_flipped: 1.0746756792068481\n",
      "B0 flipped: -2.5803966522216797\n",
      "B0 not_flipped: -0.8226088881492615\n",
      "B1 flipped: -1.7223740816116333\n",
      "B1 not_flipped: 1.0704987049102783\n",
      "A0 placed: -2.397033452987671\n",
      "A0 not_placed: 1.4333208799362183\n",
      "A1 placed: 1.495888590812683\n",
      "A1 not_placed: 1.9834816455841064\n",
      "B0 placed: 0.6923328042030334\n",
      "B0 not_placed: 0.0492352731525898\n",
      "B1 placed: -0.09268636256456375\n",
      "B1 not_placed: 0.23773331940174103\n"
     ]
    }
   ],
   "source": [
    "rows = 2\n",
    "cols = 2\n",
    "options = 2\n",
    "batch = 1\n",
    "\n",
    "probe_results_all = []\n",
    "for probe_name in [\"linear\", \"flipped\", \"placed\"]:\n",
    "    probe_result = t.randn((1, 2, 2, 2))\n",
    "    for row in range(2):\n",
    "        for col in range(2):\n",
    "            for option in range(2):\n",
    "                print(f\"{tuple_to_label((row, col))} {get_direction_str(probe_name, option)}: {probe_result[0, row, col, option]}\")\n",
    "    probe_result = einops.rearrange(probe_result, \"batch rows cols options -> batch (rows cols options)\")\n",
    "    probe_results_all.append(probe_result)\n",
    "probe_results_all = t.cat(probe_results_all, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3314,  1.8200,  0.7886, -1.0050, -0.7154, -1.7871, -0.9879, -1.0272,\n",
       "          0.0229,  1.3867,  0.2081,  1.0747, -2.5804, -0.8226, -1.7224,  1.0705,\n",
       "         -2.3970,  1.4333,  1.4959,  1.9835,  0.6923,  0.0492, -0.0927,  0.2377]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3314,  1.8200,  0.7886, -1.0050, -0.7154, -1.7871, -0.9879, -1.0272,\n",
       "          0.0229,  1.3867,  0.2081,  1.0747, -2.5804, -0.8226, -1.7224,  1.0705,\n",
       "         -2.3970,  1.4333,  1.4959,  1.9835,  0.6923,  0.0492, -0.0927,  0.2377]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
